{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Quickstart","text":"<p>Adala is an Autonomous DAta (Labeling) Agent framework.</p> <p>Adala offers a robust framework for implementing agents specialized in data processing, with an emphasis on diverse data labeling tasks. These agents are autonomous, meaning they can independently acquire one or more skills through iterative learning. This learning process is influenced by their operating environment, observations, and reflections. Users define the environment by providing a ground truth dataset. Every agent learns and applies its skills in what we refer to as a \"runtime\", synonymous with LLM.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>Install Adala:</p> <pre><code>pip install adala\n</code></pre>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Set OPENAI_API_KEY (see instructions here)</p>"},{"location":"#quickstart_1","title":"\ud83c\udfac Quickstart","text":"<p>In this example we will use Adala as a standalone library directly inside Python notebook.</p> <p>Click here to see an extended quickstart example. </p> <pre><code>import pandas as pd\n\nfrom adala.agents import Agent\nfrom adala.environments import StaticEnvironment\nfrom adala.skills import ClassificationSkill\nfrom adala.runtimes import OpenAIChatRuntime\nfrom rich import print\n\n# Train dataset\ntrain_df = pd.DataFrame([\n    [\"It was the negative first impressions, and then it started working.\", \"Positive\"],\n    [\"Not loud enough and doesn't turn on like it should.\", \"Negative\"],\n    [\"I don't know what to say.\", \"Neutral\"],\n    [\"Manager was rude, but the most important that mic shows very flat frequency response.\", \"Positive\"],\n    [\"The phone doesn't seem to accept anything except CBR mp3s.\", \"Negative\"],\n    [\"I tried it before, I bought this device for my son.\", \"Neutral\"],\n], columns=[\"text\", \"sentiment\"])\n\n# Test dataset\ntest_df = pd.DataFrame([\n    \"All three broke within two months of use.\",\n    \"The device worked for a long time, can't say anything bad.\",\n    \"Just a random line of text.\"\n], columns=[\"text\"])\n\nagent = Agent(\n    # connect to a dataset\n    environment=StaticEnvironment(df=train_df),\n\n    # define a skill\n    skills=ClassificationSkill(\n        name='sentiment',\n        instructions=\"Label text as positive, negative or neutral.\",\n        labels={'sentiment': [\"Positive\", \"Negative\", \"Neutral\"]},\n        input_template=\"Text: {text}\",\n        output_template=\"Sentiment: {sentiment}\"\n    ),\n\n    # define all the different runtimes your skills may use\n    runtimes = {\n        # You can specify your OPENAI API KEY here via `OpenAIRuntime(..., api_key='your-api-key')`\n        'openai': OpenAIChatRuntime(model='gpt-3.5-turbo'),\n    },\n    default_runtime='openai',\n\n    # NOTE! If you have access to GPT-4, you can uncomment the lines bellow for better results\n#     default_teacher_runtime='openai-gpt4',\n#     teacher_runtimes = {\n#       'openai-gpt4': OpenAIRuntime(model='gpt-4')\n#     }\n)\n\nprint(agent)\nprint(agent.skills)\n\nagent.learn(learning_iterations=3, accuracy_threshold=0.95)\n\nprint('\\n=&gt; Run tests ...')\npredictions = agent.run(test_df)\nprint('\\n =&gt; Test results:')\nprint(predictions)\n</code></pre>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Agents - main interface for  interacting with environment</li> <li>Datasets - data inputs for agents</li> <li>Environments - environments for agents, where it collects ground truth signal</li> <li>Memories - agent's memory for storing and retrieving data</li> <li>Runtimes - agent's execution runtime (e.g. LLMs providers)</li> <li>Skills - agent skills for data labeling</li> </ul>"},{"location":"agents/","title":"Agents","text":""},{"location":"agents/#adala.agents.base.Agent","title":"<code>Agent</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Represents a customizable agent that can interact with environments, employ skills, and leverage memory and runtimes.</p> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <p>The environment with which the agent interacts.</p> <code>skills</code> <code>SkillSet</code> <p>The skills possessed by the agent.</p> <code>memory</code> <code>LongTermMemory</code> <p>The agent's long-term memory. Defaults to None.</p> <code>runtimes</code> <code>Dict[str, Runtime]</code> <p>The runtimes available to the agent. Defaults to predefined runtimes.</p> <code>default_runtime</code> <code>str</code> <p>The default runtime used by the agent. Defaults to 'openai'.</p> <code>teacher_runtimes</code> <code>Dict[str, Runtime]</code> <p>The runtimes available to the agent's teacher. Defaults to predefined runtimes.</p> <code>default_teacher_runtime</code> <code>str</code> <p>The default runtime used by the agent's teacher. Defaults to 'openai-gpt3'.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from adala.environments import StaticEnvironment\n&gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill\n&gt;&gt;&gt; from adala.agents import Agent\n&gt;&gt;&gt; agent = Agent(skills=LinearSkillSet(skills=[TransformSkill()]), environment=StaticEnvironment())\n&gt;&gt;&gt; agent.learn()  # starts the learning process\n&gt;&gt;&gt; predictions = agent.run()  # runs the agent and returns the predictions\n</code></pre> Source code in <code>adala/agents/base.py</code> <pre><code>class Agent(BaseModel, ABC):\n    \"\"\"\n    Represents a customizable agent that can interact with environments,\n    employ skills, and leverage memory and runtimes.\n\n    Attributes:\n        environment (Environment): The environment with which the agent interacts.\n        skills (SkillSet): The skills possessed by the agent.\n        memory (LongTermMemory, optional): The agent's long-term memory. Defaults to None.\n        runtimes (Dict[str, Runtime], optional): The runtimes available to the agent. Defaults to predefined runtimes.\n        default_runtime (str): The default runtime used by the agent. Defaults to 'openai'.\n        teacher_runtimes (Dict[str, Runtime], optional): The runtimes available to the agent's teacher. Defaults to predefined runtimes.\n        default_teacher_runtime (str): The default runtime used by the agent's teacher. Defaults to 'openai-gpt3'.\n\n    Examples:\n        &gt;&gt;&gt; from adala.environments import StaticEnvironment\n        &gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill\n        &gt;&gt;&gt; from adala.agents import Agent\n        &gt;&gt;&gt; agent = Agent(skills=LinearSkillSet(skills=[TransformSkill()]), environment=StaticEnvironment())\n        &gt;&gt;&gt; agent.learn()  # starts the learning process\n        &gt;&gt;&gt; predictions = agent.run()  # runs the agent and returns the predictions\n    \"\"\"\n\n    environment: Optional[SerializeAsAny[Union[Environment, AsyncEnvironment]]] = None\n    skills: SerializeAsAny[SkillSet]\n\n    memory: Memory = Field(default=None)\n    runtimes: Dict[str, SerializeAsAny[Union[Runtime, AsyncRuntime]]] = Field(\n        default_factory=lambda: {\"default\": OpenAIChatRuntime(model=\"gpt-3.5-turbo\")}\n    )\n    default_runtime: str = \"default\"\n    teacher_runtimes: Dict[str, SerializeAsAny[Union[Runtime, AsyncRuntime]]] = Field(\n        default_factory=lambda: {\"default\": None}\n    )\n    default_teacher_runtime: str = \"default\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __rich__(self) -&gt; str:\n        \"\"\"\n        Returns a colorized and formatted representation of the Agent instance.\n\n        Returns:\n            str: A rich-formatted representation of the agent.\n        \"\"\"\n\n        skill_names = \", \".join([skill.name for skill in self.skills.skills.values()])\n        runtime_names = \", \".join(self.runtimes.keys())\n\n        return (\n            f\"[bold blue]Agent Instance[/bold blue]\\n\\n\"\n            f\"Environment: {self.environment.__class__.__name__}\\n\"\n            f\"Skills: {skill_names}\\n\"\n            f\"Runtimes: {runtime_names}\\n\"\n            f\"Default Runtime: {self.default_runtime}\\n\"\n            f\"Default Teacher Runtime: {self.default_teacher_runtime}\"\n        )\n\n    @field_validator(\"environment\", mode=\"before\")\n    def environment_validator(cls, v) -&gt; Environment:\n        \"\"\"\n        Validates and possibly transforms the environment attribute:\n        if the environment is an InternalDataFrame, it is transformed into a StaticEnvironment.\n        \"\"\"\n        logger.debug(f\"Validating environment attribute: {v}\")\n        if isinstance(v, InternalDataFrame):\n            v = StaticEnvironment(df=v)\n        elif isinstance(v, dict) and \"type\" in v:\n            v = Environment.create_from_registry(v.pop(\"type\"), **v)\n        return v\n\n    @field_validator(\"skills\", mode=\"before\")\n    def skills_validator(cls, v) -&gt; SkillSet:\n        \"\"\"\n        Validates and possibly transforms the skills attribute.\n        \"\"\"\n        if isinstance(v, SkillSet):\n            return v\n        elif isinstance(v, Skill):\n            return LinearSkillSet(skills=[v])\n        elif isinstance(v, list):\n            return LinearSkillSet(skills=v)\n        else:\n            raise ValueError(\n                f\"skills must be of type SkillSet or Skill, but received type {type(v)}\"\n            )\n\n    @field_validator(\"runtimes\", \"teacher_runtimes\", mode=\"before\")\n    def runtimes_validator(cls, v) -&gt; Dict[str, Union[Runtime, AsyncRuntime]]:\n        \"\"\"\n        Validates and creates runtimes\n        \"\"\"\n        out = {}\n        for runtime_name, runtime_value in v.items():\n            if isinstance(runtime_value, dict):\n                if \"type\" not in runtime_value:\n                    raise ValueError(\n                        f\"Runtime {runtime_name} must have a 'type' field to specify the runtime type.\"\n                    )\n                type_name = runtime_value.pop(\"type\")\n                runtime_value = Runtime.create_from_registry(\n                    type=type_name, **runtime_value\n                )\n            out[runtime_name] = runtime_value\n        return out\n\n    @model_validator(mode=\"after\")\n    def verify_input_parameters(self):\n        \"\"\"\n        Verifies that the input parameters are valid.\"\"\"\n\n        def _raise_default_runtime_error(val, runtime, runtimes, default_value):\n            print_error(\n                f\"The Agent.{runtime} is set to {val}, \"\n                f\"but this runtime is not available in the list: {list(runtimes)}. \"\n                f\"Please choose one of the available runtimes and initialize the agent again, for example:\\n\\n\"\n                f\"agent = Agent(..., {runtime}='{default_value}')\\n\\n\"\n                f\"Make sure the default runtime is available in the list of runtimes. For example:\\n\\n\"\n                f\"agent = Agent(..., runtimes={{'{default_value}': OpenAIRuntime(model='gpt-4')}})\\n\\n\"\n            )\n            raise ValueError(f\"default runtime {val} not found in provided runtimes.\")\n\n        if self.default_runtime not in self.runtimes:\n            _raise_default_runtime_error(\n                self.default_runtime, \"default_runtime\", self.runtimes, \"openai\"\n            )\n        if self.default_teacher_runtime not in self.teacher_runtimes:\n            _raise_default_runtime_error(\n                self.default_teacher_runtime,\n                \"default_teacher_runtime\",\n                self.teacher_runtimes,\n                \"openai-gpt4\",\n            )\n        return self\n\n    def get_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n        \"\"\"\n        Retrieves the specified runtime or the default runtime if none is specified.\n\n        Args:\n            runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n        Returns:\n            Runtime: The requested runtime.\n\n        Raises:\n            ValueError: If the specified runtime is not found.\n        \"\"\"\n\n        if runtime is None:\n            runtime = self.default_runtime\n        if runtime not in self.runtimes:\n            raise ValueError(f'Runtime \"{runtime}\" not found.')\n        return self.runtimes[runtime]\n\n    def get_teacher_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n        \"\"\"\n        Retrieves the specified teacher runtime or the default runtime if none is specified.\n\n        Args:\n            runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n        Returns:\n            Runtime: The requested runtime.\n\n        Raises:\n            ValueError: If the specified runtime is not found.\n        \"\"\"\n\n        if runtime is None:\n            runtime = self.default_teacher_runtime\n        if runtime not in self.teacher_runtimes:\n            raise ValueError(f'Teacher Runtime \"{runtime}\" not found.')\n        runtime = self.teacher_runtimes[runtime]\n        if not runtime:\n            raise ValueError(\n                f\"Teacher Runtime is requested, but it was not set.\"\n                f\"Please provide a teacher runtime in the agent's constructor explicitly:\"\n                f\"agent = Agent(..., teacher_runtimes={{'default': OpenAIChatRuntime(model='gpt-4')}})\"\n            )\n        return runtime\n\n    def run(\n        self, input: InternalDataFrame = None, runtime: Optional[str] = None, **kwargs\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Runs the agent on the specified dataset.\n\n        Args:\n            input (InternalDataFrame): The dataset to run the agent on.\n            runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n            kwargs: Additional keyword arguments to pass to the runtime.\n        Returns:\n            InternalDataFrame: The dataset with the agent's predictions.\n        \"\"\"\n        if input is None:\n            if self.environment is None:\n                raise ValueError(\"input is None and no environment is set.\")\n            input = self.environment.get_data_batch(None)\n        runtime = self.get_runtime(runtime=runtime)\n        predictions = self.skills.apply(input, runtime=runtime)\n        return predictions\n\n    async def arun(\n        self, input: InternalDataFrame = None, runtime: Optional[str] = None\n    ) -&gt; Optional[InternalDataFrame]:\n        \"\"\"\n        Runs the agent on the specified input asynchronously.\n        If no input is specified, the agent will run on the environment until it is exhausted.\n        If input is specified, the agent will run on the input, ignoring the connected genvironment.\n\n        Args:\n            input (InternalDataFrame): The dataset to run the agent on.\n            runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n\n        Returns:\n            InternalDataFrame: The dataset with the agent's predictions.\n        \"\"\"\n\n        runtime = self.get_runtime(runtime=runtime)\n        if not isinstance(runtime, AsyncRuntime):\n            raise ValueError(\n                \"When using asynchronous run with `agent.arun()`, the runtime must be an AsyncRuntime.\"\n            )\n        else:\n            logger.info(\"Using runtime %s\", type(runtime))\n\n        if input is None:\n            if self.environment is None:\n                raise ValueError(\"input is None and no environment is set.\")\n            if not isinstance(self.environment, AsyncEnvironment):\n                raise ValueError(\n                    \"When using asynchronous run with `agent.arun()` and noe input, the environment must be an AsyncEnvironment.\"\n                )\n            # run on the environment until it is exhausted\n            while True:\n                try:\n                    data_batch = await self.environment.get_data_batch(\n                        batch_size=runtime.batch_size\n                    )\n                    if data_batch.empty:\n                        logger.info(\"No more data in the environment. Exiting.\")\n                        break\n                except Exception as e:\n                    # TODO: environment should raise a specific exception + log error\n                    print_error(f\"Error getting data batch from environment: {e}\")\n                    break\n                predictions = await self.skills.aapply(data_batch, runtime=runtime)\n                await self.environment.set_predictions(predictions)\n\n        else:\n            # single run on the input data\n            predictions = await self.skills.aapply(input, runtime=runtime)\n            return predictions\n\n    def select_skill_to_train(\n        self, feedback: EnvironmentFeedback, accuracy_threshold: float\n    ) -&gt; Tuple[str, str, float]:\n        \"\"\"\n        Selects the skill to train based on the feedback signal.\n\n        Args:\n            feedback (Feedback): The feedback signal.\n            accuracy_threshold (float): The accuracy threshold to use for selecting the skill to train.\n\n        Returns:\n            str: The name of the skill to train.\n            str: The name of the skill output to train.\n            float: The accuracy score of the skill to train.\n\n        \"\"\"\n\n        # Use ground truth signal to find the skill to improve\n        # TODO: what if it is not possible to estimate accuracy per skill?\n        accuracy = feedback.get_accuracy()\n        train_skill_name, train_skill_output, acc_score = \"\", \"\", None\n        for skill_output, skill_name in self.skills.get_skill_outputs().items():\n            if skill_output in accuracy and accuracy[skill_output] &lt; accuracy_threshold:\n                train_skill_name, train_skill_output = skill_name, skill_output\n                acc_score = accuracy[skill_output]\n                break\n\n        return train_skill_name, train_skill_output, acc_score\n\n    def learn(\n        self,\n        learning_iterations: int = 3,\n        accuracy_threshold: float = 0.9,\n        update_memory: bool = True,\n        batch_size: Optional[int] = None,\n        num_feedbacks: Optional[int] = None,\n        runtime: Optional[str] = None,\n        teacher_runtime: Optional[str] = None,\n    ):\n        \"\"\"\n        Enables the agent to learn and improve its skills based on interactions with its environment.\n\n        Args:\n            learning_iterations (int, optional): The number of iterations for learning. Defaults to 3.\n            accuracy_threshold (float, optional): The desired accuracy threshold to reach. Defaults to 0.9.\n            update_memory (bool, optional): Flag to determine if memory should be updated after learning. Defaults to True.\n            num_feedbacks (int, optional): The number of predictions to request feedback for. Defaults to None.\n            runtime (str, optional): The runtime to be used for the learning process. Defaults to None.\n            teacher_runtime (str, optional): The teacher runtime to be used for the learning process. Defaults to None.\n        \"\"\"\n\n        runtime = self.get_runtime(runtime=runtime)\n        teacher_runtime = self.get_teacher_runtime(runtime=teacher_runtime)\n\n        for iteration in range(learning_iterations):\n            print_text(\n                f\"\\n\\n=&gt; Iteration #{iteration}: Getting feedback, analyzing and improving ...\"\n            )\n\n            inputs = self.environment.get_data_batch(batch_size=batch_size)\n            predictions = self.skills.apply(inputs, runtime=runtime)\n            feedback = self.environment.get_feedback(\n                self.skills, predictions, num_feedbacks=num_feedbacks\n            )\n            # TODO: this is just pretty printing - remove later for efficiency\n            print(\"Predictions and feedback:\")\n            print_dataframe(\n                feedback.feedback.rename(\n                    columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n                ).merge(predictions, left_index=True, right_index=True)\n            )\n            # -----------------------------\n            skill_mismatch = feedback.match.fillna(True) == False\n            has_errors = skill_mismatch.any(axis=1).any()\n            if not has_errors:\n                print_text(\"No errors found!\")\n                continue\n            first_skill_with_errors = skill_mismatch.any(axis=0).idxmax()\n\n            accuracy = feedback.get_accuracy()\n            # TODO: iterating over skill can be more complex, and we should take order into account\n            for skill_output, skill_name in self.skills.get_skill_outputs().items():\n                skill = self.skills[skill_name]\n                if skill.frozen:\n                    continue\n\n                print_text(\n                    f'Skill output to improve: \"{skill_output}\" (Skill=\"{skill_name}\")\\n'\n                    f\"Accuracy = {accuracy[skill_output] * 100:0.2f}%\",\n                    style=\"bold red\",\n                )\n\n                old_instructions = skill.instructions\n                skill.improve(\n                    predictions, skill_output, feedback, runtime=teacher_runtime\n                )\n\n                if is_running_in_jupyter():\n                    highlight_differences(old_instructions, skill.instructions)\n                else:\n                    print_text(skill.instructions, style=\"bold green\")\n\n                if skill_name == first_skill_with_errors:\n                    break\n\n        print_text(\"Train is done!\")\n\n    async def arefine_skill(\n        self,\n        skill_name: str,\n        input_variables: List[str],\n        data: Optional[List[Dict]] = None,\n        reapply: bool = False,\n        instructions: Optional[str] = None,\n    ) -&gt; ImprovedPromptResponse:\n        \"\"\"\n        beta v2 of Agent.learn() that is:\n        - compatible with the newer LiteLLM runtimes\n        - compatible with the newer response_model output formats for skills\n        - returns chain of thought reasoning in a legible format\n\n        Limitations so far:\n        - single skill at a time\n        - only returns the improved input_template, doesn't modify the skill in place\n        - no iterations/variable cost\n        \"\"\"\n\n        skill = self.skills[skill_name]\n        if not isinstance(skill, TransformSkill):\n            raise ValueError(f\"Skill {skill_name} is not a TransformSkill\")\n\n        # get default runtimes\n        runtime = self.get_runtime()\n        teacher_runtime = self.get_teacher_runtime()\n\n        # get inputs\n        # TODO: replace it with async environment.get_data_batch()\n        if data is None:\n            predictions = None\n        else:\n            inputs = InternalDataFrame.from_records(data or [])\n            if reapply:\n                predictions = await self.skills.aapply(inputs, runtime=runtime)\n            else:\n                predictions = inputs\n\n        response = await skill.aimprove(\n            predictions=predictions,\n            teacher_runtime=teacher_runtime,\n            target_input_variables=input_variables,\n            instructions=instructions,\n        )\n        return response\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.__rich__","title":"<code>__rich__()</code>","text":"<p>Returns a colorized and formatted representation of the Agent instance.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A rich-formatted representation of the agent.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def __rich__(self) -&gt; str:\n    \"\"\"\n    Returns a colorized and formatted representation of the Agent instance.\n\n    Returns:\n        str: A rich-formatted representation of the agent.\n    \"\"\"\n\n    skill_names = \", \".join([skill.name for skill in self.skills.skills.values()])\n    runtime_names = \", \".join(self.runtimes.keys())\n\n    return (\n        f\"[bold blue]Agent Instance[/bold blue]\\n\\n\"\n        f\"Environment: {self.environment.__class__.__name__}\\n\"\n        f\"Skills: {skill_names}\\n\"\n        f\"Runtimes: {runtime_names}\\n\"\n        f\"Default Runtime: {self.default_runtime}\\n\"\n        f\"Default Teacher Runtime: {self.default_teacher_runtime}\"\n    )\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.arefine_skill","title":"<code>arefine_skill(skill_name, input_variables, data=None, reapply=False, instructions=None)</code>  <code>async</code>","text":"<p>beta v2 of Agent.learn() that is: - compatible with the newer LiteLLM runtimes - compatible with the newer response_model output formats for skills - returns chain of thought reasoning in a legible format</p> <p>Limitations so far: - single skill at a time - only returns the improved input_template, doesn't modify the skill in place - no iterations/variable cost</p> Source code in <code>adala/agents/base.py</code> <pre><code>async def arefine_skill(\n    self,\n    skill_name: str,\n    input_variables: List[str],\n    data: Optional[List[Dict]] = None,\n    reapply: bool = False,\n    instructions: Optional[str] = None,\n) -&gt; ImprovedPromptResponse:\n    \"\"\"\n    beta v2 of Agent.learn() that is:\n    - compatible with the newer LiteLLM runtimes\n    - compatible with the newer response_model output formats for skills\n    - returns chain of thought reasoning in a legible format\n\n    Limitations so far:\n    - single skill at a time\n    - only returns the improved input_template, doesn't modify the skill in place\n    - no iterations/variable cost\n    \"\"\"\n\n    skill = self.skills[skill_name]\n    if not isinstance(skill, TransformSkill):\n        raise ValueError(f\"Skill {skill_name} is not a TransformSkill\")\n\n    # get default runtimes\n    runtime = self.get_runtime()\n    teacher_runtime = self.get_teacher_runtime()\n\n    # get inputs\n    # TODO: replace it with async environment.get_data_batch()\n    if data is None:\n        predictions = None\n    else:\n        inputs = InternalDataFrame.from_records(data or [])\n        if reapply:\n            predictions = await self.skills.aapply(inputs, runtime=runtime)\n        else:\n            predictions = inputs\n\n    response = await skill.aimprove(\n        predictions=predictions,\n        teacher_runtime=teacher_runtime,\n        target_input_variables=input_variables,\n        instructions=instructions,\n    )\n    return response\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.arun","title":"<code>arun(input=None, runtime=None)</code>  <code>async</code>","text":"<p>Runs the agent on the specified input asynchronously. If no input is specified, the agent will run on the environment until it is exhausted. If input is specified, the agent will run on the input, ignoring the connected genvironment.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The dataset to run the agent on.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>The name of the runtime to use. Defaults to None, use the default runtime.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>Optional[InternalDataFrame]</code> <p>The dataset with the agent's predictions.</p> Source code in <code>adala/agents/base.py</code> <pre><code>async def arun(\n    self, input: InternalDataFrame = None, runtime: Optional[str] = None\n) -&gt; Optional[InternalDataFrame]:\n    \"\"\"\n    Runs the agent on the specified input asynchronously.\n    If no input is specified, the agent will run on the environment until it is exhausted.\n    If input is specified, the agent will run on the input, ignoring the connected genvironment.\n\n    Args:\n        input (InternalDataFrame): The dataset to run the agent on.\n        runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n\n    Returns:\n        InternalDataFrame: The dataset with the agent's predictions.\n    \"\"\"\n\n    runtime = self.get_runtime(runtime=runtime)\n    if not isinstance(runtime, AsyncRuntime):\n        raise ValueError(\n            \"When using asynchronous run with `agent.arun()`, the runtime must be an AsyncRuntime.\"\n        )\n    else:\n        logger.info(\"Using runtime %s\", type(runtime))\n\n    if input is None:\n        if self.environment is None:\n            raise ValueError(\"input is None and no environment is set.\")\n        if not isinstance(self.environment, AsyncEnvironment):\n            raise ValueError(\n                \"When using asynchronous run with `agent.arun()` and noe input, the environment must be an AsyncEnvironment.\"\n            )\n        # run on the environment until it is exhausted\n        while True:\n            try:\n                data_batch = await self.environment.get_data_batch(\n                    batch_size=runtime.batch_size\n                )\n                if data_batch.empty:\n                    logger.info(\"No more data in the environment. Exiting.\")\n                    break\n            except Exception as e:\n                # TODO: environment should raise a specific exception + log error\n                print_error(f\"Error getting data batch from environment: {e}\")\n                break\n            predictions = await self.skills.aapply(data_batch, runtime=runtime)\n            await self.environment.set_predictions(predictions)\n\n    else:\n        # single run on the input data\n        predictions = await self.skills.aapply(input, runtime=runtime)\n        return predictions\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.environment_validator","title":"<code>environment_validator(v)</code>","text":"<p>Validates and possibly transforms the environment attribute: if the environment is an InternalDataFrame, it is transformed into a StaticEnvironment.</p> Source code in <code>adala/agents/base.py</code> <pre><code>@field_validator(\"environment\", mode=\"before\")\ndef environment_validator(cls, v) -&gt; Environment:\n    \"\"\"\n    Validates and possibly transforms the environment attribute:\n    if the environment is an InternalDataFrame, it is transformed into a StaticEnvironment.\n    \"\"\"\n    logger.debug(f\"Validating environment attribute: {v}\")\n    if isinstance(v, InternalDataFrame):\n        v = StaticEnvironment(df=v)\n    elif isinstance(v, dict) and \"type\" in v:\n        v = Environment.create_from_registry(v.pop(\"type\"), **v)\n    return v\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.get_runtime","title":"<code>get_runtime(runtime=None)</code>","text":"<p>Retrieves the specified runtime or the default runtime if none is specified.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>str</code> <p>The name of the runtime to retrieve. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The requested runtime.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified runtime is not found.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def get_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n    \"\"\"\n    Retrieves the specified runtime or the default runtime if none is specified.\n\n    Args:\n        runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n    Returns:\n        Runtime: The requested runtime.\n\n    Raises:\n        ValueError: If the specified runtime is not found.\n    \"\"\"\n\n    if runtime is None:\n        runtime = self.default_runtime\n    if runtime not in self.runtimes:\n        raise ValueError(f'Runtime \"{runtime}\" not found.')\n    return self.runtimes[runtime]\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.get_teacher_runtime","title":"<code>get_teacher_runtime(runtime=None)</code>","text":"<p>Retrieves the specified teacher runtime or the default runtime if none is specified.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>str</code> <p>The name of the runtime to retrieve. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The requested runtime.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified runtime is not found.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def get_teacher_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n    \"\"\"\n    Retrieves the specified teacher runtime or the default runtime if none is specified.\n\n    Args:\n        runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n    Returns:\n        Runtime: The requested runtime.\n\n    Raises:\n        ValueError: If the specified runtime is not found.\n    \"\"\"\n\n    if runtime is None:\n        runtime = self.default_teacher_runtime\n    if runtime not in self.teacher_runtimes:\n        raise ValueError(f'Teacher Runtime \"{runtime}\" not found.')\n    runtime = self.teacher_runtimes[runtime]\n    if not runtime:\n        raise ValueError(\n            f\"Teacher Runtime is requested, but it was not set.\"\n            f\"Please provide a teacher runtime in the agent's constructor explicitly:\"\n            f\"agent = Agent(..., teacher_runtimes={{'default': OpenAIChatRuntime(model='gpt-4')}})\"\n        )\n    return runtime\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.learn","title":"<code>learn(learning_iterations=3, accuracy_threshold=0.9, update_memory=True, batch_size=None, num_feedbacks=None, runtime=None, teacher_runtime=None)</code>","text":"<p>Enables the agent to learn and improve its skills based on interactions with its environment.</p> <p>Parameters:</p> Name Type Description Default <code>learning_iterations</code> <code>int</code> <p>The number of iterations for learning. Defaults to 3.</p> <code>3</code> <code>accuracy_threshold</code> <code>float</code> <p>The desired accuracy threshold to reach. Defaults to 0.9.</p> <code>0.9</code> <code>update_memory</code> <code>bool</code> <p>Flag to determine if memory should be updated after learning. Defaults to True.</p> <code>True</code> <code>num_feedbacks</code> <code>int</code> <p>The number of predictions to request feedback for. Defaults to None.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>The runtime to be used for the learning process. Defaults to None.</p> <code>None</code> <code>teacher_runtime</code> <code>str</code> <p>The teacher runtime to be used for the learning process. Defaults to None.</p> <code>None</code> Source code in <code>adala/agents/base.py</code> <pre><code>def learn(\n    self,\n    learning_iterations: int = 3,\n    accuracy_threshold: float = 0.9,\n    update_memory: bool = True,\n    batch_size: Optional[int] = None,\n    num_feedbacks: Optional[int] = None,\n    runtime: Optional[str] = None,\n    teacher_runtime: Optional[str] = None,\n):\n    \"\"\"\n    Enables the agent to learn and improve its skills based on interactions with its environment.\n\n    Args:\n        learning_iterations (int, optional): The number of iterations for learning. Defaults to 3.\n        accuracy_threshold (float, optional): The desired accuracy threshold to reach. Defaults to 0.9.\n        update_memory (bool, optional): Flag to determine if memory should be updated after learning. Defaults to True.\n        num_feedbacks (int, optional): The number of predictions to request feedback for. Defaults to None.\n        runtime (str, optional): The runtime to be used for the learning process. Defaults to None.\n        teacher_runtime (str, optional): The teacher runtime to be used for the learning process. Defaults to None.\n    \"\"\"\n\n    runtime = self.get_runtime(runtime=runtime)\n    teacher_runtime = self.get_teacher_runtime(runtime=teacher_runtime)\n\n    for iteration in range(learning_iterations):\n        print_text(\n            f\"\\n\\n=&gt; Iteration #{iteration}: Getting feedback, analyzing and improving ...\"\n        )\n\n        inputs = self.environment.get_data_batch(batch_size=batch_size)\n        predictions = self.skills.apply(inputs, runtime=runtime)\n        feedback = self.environment.get_feedback(\n            self.skills, predictions, num_feedbacks=num_feedbacks\n        )\n        # TODO: this is just pretty printing - remove later for efficiency\n        print(\"Predictions and feedback:\")\n        print_dataframe(\n            feedback.feedback.rename(\n                columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n            ).merge(predictions, left_index=True, right_index=True)\n        )\n        # -----------------------------\n        skill_mismatch = feedback.match.fillna(True) == False\n        has_errors = skill_mismatch.any(axis=1).any()\n        if not has_errors:\n            print_text(\"No errors found!\")\n            continue\n        first_skill_with_errors = skill_mismatch.any(axis=0).idxmax()\n\n        accuracy = feedback.get_accuracy()\n        # TODO: iterating over skill can be more complex, and we should take order into account\n        for skill_output, skill_name in self.skills.get_skill_outputs().items():\n            skill = self.skills[skill_name]\n            if skill.frozen:\n                continue\n\n            print_text(\n                f'Skill output to improve: \"{skill_output}\" (Skill=\"{skill_name}\")\\n'\n                f\"Accuracy = {accuracy[skill_output] * 100:0.2f}%\",\n                style=\"bold red\",\n            )\n\n            old_instructions = skill.instructions\n            skill.improve(\n                predictions, skill_output, feedback, runtime=teacher_runtime\n            )\n\n            if is_running_in_jupyter():\n                highlight_differences(old_instructions, skill.instructions)\n            else:\n                print_text(skill.instructions, style=\"bold green\")\n\n            if skill_name == first_skill_with_errors:\n                break\n\n    print_text(\"Train is done!\")\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.run","title":"<code>run(input=None, runtime=None, **kwargs)</code>","text":"<p>Runs the agent on the specified dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The dataset to run the agent on.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>The name of the runtime to use. Defaults to None, use the default runtime.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the runtime.</p> <code>{}</code> <p>Returns:     InternalDataFrame: The dataset with the agent's predictions.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def run(\n    self, input: InternalDataFrame = None, runtime: Optional[str] = None, **kwargs\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Runs the agent on the specified dataset.\n\n    Args:\n        input (InternalDataFrame): The dataset to run the agent on.\n        runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n        kwargs: Additional keyword arguments to pass to the runtime.\n    Returns:\n        InternalDataFrame: The dataset with the agent's predictions.\n    \"\"\"\n    if input is None:\n        if self.environment is None:\n            raise ValueError(\"input is None and no environment is set.\")\n        input = self.environment.get_data_batch(None)\n    runtime = self.get_runtime(runtime=runtime)\n    predictions = self.skills.apply(input, runtime=runtime)\n    return predictions\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.runtimes_validator","title":"<code>runtimes_validator(v)</code>","text":"<p>Validates and creates runtimes</p> Source code in <code>adala/agents/base.py</code> <pre><code>@field_validator(\"runtimes\", \"teacher_runtimes\", mode=\"before\")\ndef runtimes_validator(cls, v) -&gt; Dict[str, Union[Runtime, AsyncRuntime]]:\n    \"\"\"\n    Validates and creates runtimes\n    \"\"\"\n    out = {}\n    for runtime_name, runtime_value in v.items():\n        if isinstance(runtime_value, dict):\n            if \"type\" not in runtime_value:\n                raise ValueError(\n                    f\"Runtime {runtime_name} must have a 'type' field to specify the runtime type.\"\n                )\n            type_name = runtime_value.pop(\"type\")\n            runtime_value = Runtime.create_from_registry(\n                type=type_name, **runtime_value\n            )\n        out[runtime_name] = runtime_value\n    return out\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.select_skill_to_train","title":"<code>select_skill_to_train(feedback, accuracy_threshold)</code>","text":"<p>Selects the skill to train based on the feedback signal.</p> <p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>Feedback</code> <p>The feedback signal.</p> required <code>accuracy_threshold</code> <code>float</code> <p>The accuracy threshold to use for selecting the skill to train.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the skill to train.</p> <code>str</code> <code>str</code> <p>The name of the skill output to train.</p> <code>float</code> <code>float</code> <p>The accuracy score of the skill to train.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def select_skill_to_train(\n    self, feedback: EnvironmentFeedback, accuracy_threshold: float\n) -&gt; Tuple[str, str, float]:\n    \"\"\"\n    Selects the skill to train based on the feedback signal.\n\n    Args:\n        feedback (Feedback): The feedback signal.\n        accuracy_threshold (float): The accuracy threshold to use for selecting the skill to train.\n\n    Returns:\n        str: The name of the skill to train.\n        str: The name of the skill output to train.\n        float: The accuracy score of the skill to train.\n\n    \"\"\"\n\n    # Use ground truth signal to find the skill to improve\n    # TODO: what if it is not possible to estimate accuracy per skill?\n    accuracy = feedback.get_accuracy()\n    train_skill_name, train_skill_output, acc_score = \"\", \"\", None\n    for skill_output, skill_name in self.skills.get_skill_outputs().items():\n        if skill_output in accuracy and accuracy[skill_output] &lt; accuracy_threshold:\n            train_skill_name, train_skill_output = skill_name, skill_output\n            acc_score = accuracy[skill_output]\n            break\n\n    return train_skill_name, train_skill_output, acc_score\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.skills_validator","title":"<code>skills_validator(v)</code>","text":"<p>Validates and possibly transforms the skills attribute.</p> Source code in <code>adala/agents/base.py</code> <pre><code>@field_validator(\"skills\", mode=\"before\")\ndef skills_validator(cls, v) -&gt; SkillSet:\n    \"\"\"\n    Validates and possibly transforms the skills attribute.\n    \"\"\"\n    if isinstance(v, SkillSet):\n        return v\n    elif isinstance(v, Skill):\n        return LinearSkillSet(skills=[v])\n    elif isinstance(v, list):\n        return LinearSkillSet(skills=v)\n    else:\n        raise ValueError(\n            f\"skills must be of type SkillSet or Skill, but received type {type(v)}\"\n        )\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.verify_input_parameters","title":"<code>verify_input_parameters()</code>","text":"<p>Verifies that the input parameters are valid.</p> Source code in <code>adala/agents/base.py</code> <pre><code>@model_validator(mode=\"after\")\ndef verify_input_parameters(self):\n    \"\"\"\n    Verifies that the input parameters are valid.\"\"\"\n\n    def _raise_default_runtime_error(val, runtime, runtimes, default_value):\n        print_error(\n            f\"The Agent.{runtime} is set to {val}, \"\n            f\"but this runtime is not available in the list: {list(runtimes)}. \"\n            f\"Please choose one of the available runtimes and initialize the agent again, for example:\\n\\n\"\n            f\"agent = Agent(..., {runtime}='{default_value}')\\n\\n\"\n            f\"Make sure the default runtime is available in the list of runtimes. For example:\\n\\n\"\n            f\"agent = Agent(..., runtimes={{'{default_value}': OpenAIRuntime(model='gpt-4')}})\\n\\n\"\n        )\n        raise ValueError(f\"default runtime {val} not found in provided runtimes.\")\n\n    if self.default_runtime not in self.runtimes:\n        _raise_default_runtime_error(\n            self.default_runtime, \"default_runtime\", self.runtimes, \"openai\"\n        )\n    if self.default_teacher_runtime not in self.teacher_runtimes:\n        _raise_default_runtime_error(\n            self.default_teacher_runtime,\n            \"default_teacher_runtime\",\n            self.teacher_runtimes,\n            \"openai-gpt4\",\n        )\n    return self\n</code></pre>"},{"location":"agents/#adala.agents.base.create_agent_from_dict","title":"<code>create_agent_from_dict(json_dict)</code>","text":"<p>Creates an agent from a JSON dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>json_dict</code> <code>Dict</code> <p>The JSON dictionary to create the agent from.</p> required <p>Returns:</p> Name Type Description <code>Agent</code> <p>The created agent.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def create_agent_from_dict(json_dict: Dict):\n    \"\"\"\n    Creates an agent from a JSON dictionary.\n\n    Args:\n        json_dict (Dict): The JSON dictionary to create the agent from.\n\n    Returns:\n        Agent: The created agent.\n    \"\"\"\n\n    agent = Agent(**json_dict)\n    return agent\n</code></pre>"},{"location":"agents/#adala.agents.base.create_agent_from_file","title":"<code>create_agent_from_file(file_path)</code>","text":"<p>Creates an agent from a YAML file: 1. Define agent reasoning workflow in <code>workflow.yml</code>:</p> <pre><code>- name: reasoning\n  type: sample_transform\n  sample_size: 10\n  instructions: \"Think step-by-step.\"\n  input_template: \"Question: {question}\"\n  output_template: \"{reasoning}\"\n\n- name: numeric_answer\n  type: transform\n  instructions: &gt;\n    Given math question and reasoning, provide only numeric answer after `Answer: `, for example:\n    Question: &lt;math question&gt;\n    Reasoning: &lt;reasoning&gt;\n    Answer: &lt;your numerical answer&gt;\n  input_template: &gt;\n    Question: {question}\n    Reasoning: {reasoning}\n  output_template: &gt;\n    Answer: {answer}\n</code></pre> <ol> <li>Run adala math reasoning workflow on the <code>gsm8k</code> dataset:</li> </ol> <pre><code>adala run --input gsm8k --dataset-config main --dataset-split test --workflow workflow.yml\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the YAML file to create the agent from.</p> required <p>Returns:</p> Name Type Description <code>Agent</code> <p>The created agent.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def create_agent_from_file(file_path: str):\n    \"\"\"\n    Creates an agent from a YAML file:\n    1. Define agent reasoning workflow in `workflow.yml`:\n\n    ```yaml\n    - name: reasoning\n      type: sample_transform\n      sample_size: 10\n      instructions: \"Think step-by-step.\"\n      input_template: \"Question: {question}\"\n      output_template: \"{reasoning}\"\n\n    - name: numeric_answer\n      type: transform\n      instructions: &gt;\n        Given math question and reasoning, provide only numeric answer after `Answer: `, for example:\n        Question: &lt;math question&gt;\n        Reasoning: &lt;reasoning&gt;\n        Answer: &lt;your numerical answer&gt;\n      input_template: &gt;\n        Question: {question}\n        Reasoning: {reasoning}\n      output_template: &gt;\n        Answer: {answer}\n    ```\n\n    2. Run adala math reasoning workflow on the `gsm8k` dataset:\n\n    ```sh\n    adala run --input gsm8k --dataset-config main --dataset-split test --workflow workflow.yml\n    ```\n\n    Args:\n        file_path (str): The path to the YAML file to create the agent from.\n\n    Returns:\n        Agent: The created agent.\n    \"\"\"\n\n    with open(file_path, \"r\") as file:\n        json_dict = yaml.safe_load(file)\n    if isinstance(json_dict, list):\n        json_dict = {\"skills\": json_dict}\n    return create_agent_from_dict(json_dict)\n</code></pre>"},{"location":"environments/","title":"Environments","text":""},{"location":"environments/#adala.environments.base.AsyncEnvironment","title":"<code>AsyncEnvironment</code>","text":"<p>               Bases: <code>Environment</code>, <code>ABC</code></p> Source code in <code>adala/environments/base.py</code> <pre><code>class AsyncEnvironment(Environment, ABC):\n    @abstractmethod\n    async def initialize(self):\n        \"\"\"\n        Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    async def finalize(self):\n        \"\"\"\n        Finalize the environment, e.g by closing a database connection, closing a file or stopping a stream.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    async def get_data_batch(self, batch_size: Optional[int]) -&gt; InternalDataFrame:\n        \"\"\"\n        Get a batch of data from data stream to be processed by the skill set.\n\n        Args:\n            batch_size (Optional[int], optional): The size of the batch. Defaults to None\n\n        Returns:\n            InternalDataFrame: The data batch.\n        \"\"\"\n\n    @abstractmethod\n    async def get_feedback(\n        self,\n        skills: SkillSet,\n        predictions: InternalDataFrame,\n        num_feedbacks: Optional[int] = None,\n    ) -&gt; EnvironmentFeedback:\n        \"\"\"\n        Request feedback for the predictions.\n\n        Args:\n            skills (SkillSet): The set of skills/models whose predictions are being evaluated.\n            predictions (InternalDataFrame): The predictions to compare with the ground truth.\n            num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n        Returns:\n            EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n        \"\"\"\n\n    @abstractmethod\n    async def set_predictions(self, predictions: InternalDataFrame):\n        \"\"\"\n        Push predictions back to the environment.\n\n        Args:\n            predictions (InternalDataFrame): The predictions to push to the environment.\n        \"\"\"\n\n    @abstractmethod\n    async def save(self):\n        \"\"\"\n        Save the current state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    async def restore(self):\n        \"\"\"\n        Restore the state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.finalize","title":"<code>finalize()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Finalize the environment, e.g by closing a database connection, closing a file or stopping a stream.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def finalize(self):\n    \"\"\"\n    Finalize the environment, e.g by closing a database connection, closing a file or stopping a stream.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.get_data_batch","title":"<code>get_data_batch(batch_size)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Get a batch of data from data stream to be processed by the skill set.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>Optional[int]</code> <p>The size of the batch. Defaults to None</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The data batch.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def get_data_batch(self, batch_size: Optional[int]) -&gt; InternalDataFrame:\n    \"\"\"\n    Get a batch of data from data stream to be processed by the skill set.\n\n    Args:\n        batch_size (Optional[int], optional): The size of the batch. Defaults to None\n\n    Returns:\n        InternalDataFrame: The data batch.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.get_feedback","title":"<code>get_feedback(skills, predictions, num_feedbacks=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Request feedback for the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>skills</code> <code>SkillSet</code> <p>The set of skills/models whose predictions are being evaluated.</p> required <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions to compare with the ground truth.</p> required <code>num_feedbacks</code> <code>Optional[int]</code> <p>The number of feedbacks to request. Defaults to all predictions</p> <code>None</code> <p>Returns:     EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def get_feedback(\n    self,\n    skills: SkillSet,\n    predictions: InternalDataFrame,\n    num_feedbacks: Optional[int] = None,\n) -&gt; EnvironmentFeedback:\n    \"\"\"\n    Request feedback for the predictions.\n\n    Args:\n        skills (SkillSet): The set of skills/models whose predictions are being evaluated.\n        predictions (InternalDataFrame): The predictions to compare with the ground truth.\n        num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n    Returns:\n        EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.initialize","title":"<code>initialize()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def initialize(self):\n    \"\"\"\n    Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.restore","title":"<code>restore()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Restore the state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def restore(self):\n    \"\"\"\n    Restore the state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.save","title":"<code>save()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Save the current state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def save(self):\n    \"\"\"\n    Save the current state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.set_predictions","title":"<code>set_predictions(predictions)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Push predictions back to the environment.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions to push to the environment.</p> required Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def set_predictions(self, predictions: InternalDataFrame):\n    \"\"\"\n    Push predictions back to the environment.\n\n    Args:\n        predictions (InternalDataFrame): The predictions to push to the environment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment","title":"<code>Environment</code>","text":"<p>               Bases: <code>BaseModelInRegistry</code></p> <p>An abstract base class that defines the structure and required methods for an environment in which machine learning models operate and are evaluated against ground truth data.</p> <p>Subclasses should implement methods to handle feedback requests, comparison to ground truth, dataset conversion, and state persistence.</p> Source code in <code>adala/environments/base.py</code> <pre><code>class Environment(BaseModelInRegistry):\n    \"\"\"\n    An abstract base class that defines the structure and required methods for an environment\n    in which machine learning models operate and are evaluated against ground truth data.\n\n    Subclasses should implement methods to handle feedback requests, comparison to ground truth,\n    dataset conversion, and state persistence.\n    \"\"\"\n\n    @abstractmethod\n    def initialize(self):\n        \"\"\"\n        Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    def finalize(self):\n        \"\"\"\n        Finalize the environment, e.g by closing a database connection, writing memory to file or stopping a stream.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    def get_data_batch(self, batch_size: Optional[int]) -&gt; InternalDataFrame:\n        \"\"\"\n        Get a batch of data from data stream to be processed by the skill set.\n\n        Args:\n            batch_size (Optional[int], optional): The size of the batch. Defaults to None\n\n        Returns:\n            InternalDataFrame: The data batch.\n        \"\"\"\n\n    @abstractmethod\n    def get_feedback(\n        self,\n        skills: SkillSet,\n        predictions: InternalDataFrame,\n        num_feedbacks: Optional[int] = None,\n    ) -&gt; EnvironmentFeedback:\n        \"\"\"\n        Request feedback for the predictions.\n\n        Args:\n            skills (SkillSet): The set of skills/models whose predictions are being evaluated.\n            predictions (InternalDataFrame): The predictions to compare with the ground truth.\n            num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n        Returns:\n            EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n        \"\"\"\n\n    @abstractmethod\n    def save(self):\n        \"\"\"\n        Save the current state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    def restore(self):\n        \"\"\"\n        Restore the state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.finalize","title":"<code>finalize()</code>  <code>abstractmethod</code>","text":"<p>Finalize the environment, e.g by closing a database connection, writing memory to file or stopping a stream.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef finalize(self):\n    \"\"\"\n    Finalize the environment, e.g by closing a database connection, writing memory to file or stopping a stream.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.get_data_batch","title":"<code>get_data_batch(batch_size)</code>  <code>abstractmethod</code>","text":"<p>Get a batch of data from data stream to be processed by the skill set.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>Optional[int]</code> <p>The size of the batch. Defaults to None</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The data batch.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef get_data_batch(self, batch_size: Optional[int]) -&gt; InternalDataFrame:\n    \"\"\"\n    Get a batch of data from data stream to be processed by the skill set.\n\n    Args:\n        batch_size (Optional[int], optional): The size of the batch. Defaults to None\n\n    Returns:\n        InternalDataFrame: The data batch.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.get_feedback","title":"<code>get_feedback(skills, predictions, num_feedbacks=None)</code>  <code>abstractmethod</code>","text":"<p>Request feedback for the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>skills</code> <code>SkillSet</code> <p>The set of skills/models whose predictions are being evaluated.</p> required <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions to compare with the ground truth.</p> required <code>num_feedbacks</code> <code>Optional[int]</code> <p>The number of feedbacks to request. Defaults to all predictions</p> <code>None</code> <p>Returns:     EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef get_feedback(\n    self,\n    skills: SkillSet,\n    predictions: InternalDataFrame,\n    num_feedbacks: Optional[int] = None,\n) -&gt; EnvironmentFeedback:\n    \"\"\"\n    Request feedback for the predictions.\n\n    Args:\n        skills (SkillSet): The set of skills/models whose predictions are being evaluated.\n        predictions (InternalDataFrame): The predictions to compare with the ground truth.\n        num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n    Returns:\n        EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.initialize","title":"<code>initialize()</code>  <code>abstractmethod</code>","text":"<p>Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef initialize(self):\n    \"\"\"\n    Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.restore","title":"<code>restore()</code>  <code>abstractmethod</code>","text":"<p>Restore the state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef restore(self):\n    \"\"\"\n    Restore the state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.save","title":"<code>save()</code>  <code>abstractmethod</code>","text":"<p>Save the current state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef save(self):\n    \"\"\"\n    Save the current state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.EnvironmentFeedback","title":"<code>EnvironmentFeedback</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class that represents the feedback received from an environment, along with the calculated correctness of predictions.</p> <p>Attributes:</p> Name Type Description <code>match</code> <code>InternalDataFrame</code> <p>A DataFrame indicating the correctness of predictions.                        Each row corresponds to a prediction, and each column is a boolean indicating if skill matches ground truth.                        Columns are named after the skill names.                        Indices correspond to prediction indices.                        Example:                            <code>| index | skill_1 | skill_2 | skill_3 |                             |-------|---------|---------|---------|                             | 0     | True    | True    | False   |                             | 1     | False   | False   | False   |                             | 2     | True    | True    | True    |</code></p> <code>feedback</code> <code>InternalDataFrame</code> <p>A DataFrame that contains ground truth feedback per each skill output</p> Source code in <code>adala/environments/base.py</code> <pre><code>class EnvironmentFeedback(BaseModel):\n    \"\"\"\n    A class that represents the feedback received from an environment,\n    along with the calculated correctness of predictions.\n\n    Attributes:\n        match (InternalDataFrame): A DataFrame indicating the correctness of predictions.\n                                   Each row corresponds to a prediction, and each column is a boolean indicating if skill matches ground truth.\n                                   Columns are named after the skill names.\n                                   Indices correspond to prediction indices.\n                                   Example:\n                                       ```\n                                        | index | skill_1 | skill_2 | skill_3 |\n                                        |-------|---------|---------|---------|\n                                        | 0     | True    | True    | False   |\n                                        | 1     | False   | False   | False   |\n                                        | 2     | True    | True    | True    |\n                                        ```\n        feedback (InternalDataFrame): A DataFrame that contains ground truth feedback per each skill output\n    \"\"\"\n\n    match: InternalDataFrame\n    feedback: InternalDataFrame\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def get_accuracy(self) -&gt; InternalSeries:\n        \"\"\"\n        Calculate the accuracy of predictions as the mean of matches.\n\n        Returns:\n            InternalSeries: A series representing the accuracy of predictions.\n        \"\"\"\n        return self.match.mean()\n\n    def __rich__(self):\n        text = \"[bold blue]Environment Feedback:[/bold blue]\\n\\n\"\n        text += f\"\\n[bold]Match[/bold]\\n{self.match}\"\n        if self.feedback is not None:\n            text += f\"\\n[bold]Feedback[/bold]\\n{self.feedback}\"\n        return text\n</code></pre>"},{"location":"environments/#adala.environments.base.EnvironmentFeedback.get_accuracy","title":"<code>get_accuracy()</code>","text":"<p>Calculate the accuracy of predictions as the mean of matches.</p> <p>Returns:</p> Name Type Description <code>InternalSeries</code> <code>InternalSeries</code> <p>A series representing the accuracy of predictions.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def get_accuracy(self) -&gt; InternalSeries:\n    \"\"\"\n    Calculate the accuracy of predictions as the mean of matches.\n\n    Returns:\n        InternalSeries: A series representing the accuracy of predictions.\n    \"\"\"\n    return self.match.mean()\n</code></pre>"},{"location":"memories/","title":"Memories","text":""},{"location":"memories/#adala.memories.base.Memory","title":"<code>Memory</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base class for memories.</p> Source code in <code>adala/memories/base.py</code> <pre><code>class Memory(BaseModel, ABC):\n    \"\"\"\n    Base class for memories.\n    \"\"\"\n\n    @abstractmethod\n    def remember(self, observation: str, data: Dict):\n        \"\"\"\n        Base method for remembering experiences in long term memory.\n        \"\"\"\n\n    def remember_many(self, observations: List[str], data: List[Dict]):\n        \"\"\"\n        Base method for remembering experiences in long term memory.\n        \"\"\"\n        for observation, d in zip(observations, data):\n            self.remember(observation, d)\n\n    @abstractmethod\n    def retrieve(self, observation: str, num_results: int = 1) -&gt; Any:\n        \"\"\"\n        Base method for retrieving past experiences from long term memory, based on current observations\n\n        Args:\n            observation: the current observation\n            num_results: the number of results to return\n        \"\"\"\n\n    def retrieve_many(self, observations: List[str], num_results: int = 1) -&gt; List[Any]:\n        \"\"\"\n        Base method for retrieving past experiences from long term memory, based on current observations\n\n        Args:\n            observation: the current observation\n            num_results: the number of results to return\n        \"\"\"\n        return [self.retrieve(observation) for observation in observations]\n\n    @abstractmethod\n    def clear(self):\n        \"\"\"\n        Base method for clearing memory.\n        \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Base method for clearing memory.</p> Source code in <code>adala/memories/base.py</code> <pre><code>@abstractmethod\ndef clear(self):\n    \"\"\"\n    Base method for clearing memory.\n    \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.remember","title":"<code>remember(observation, data)</code>  <code>abstractmethod</code>","text":"<p>Base method for remembering experiences in long term memory.</p> Source code in <code>adala/memories/base.py</code> <pre><code>@abstractmethod\ndef remember(self, observation: str, data: Dict):\n    \"\"\"\n    Base method for remembering experiences in long term memory.\n    \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.remember_many","title":"<code>remember_many(observations, data)</code>","text":"<p>Base method for remembering experiences in long term memory.</p> Source code in <code>adala/memories/base.py</code> <pre><code>def remember_many(self, observations: List[str], data: List[Dict]):\n    \"\"\"\n    Base method for remembering experiences in long term memory.\n    \"\"\"\n    for observation, d in zip(observations, data):\n        self.remember(observation, d)\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.retrieve","title":"<code>retrieve(observation, num_results=1)</code>  <code>abstractmethod</code>","text":"<p>Base method for retrieving past experiences from long term memory, based on current observations</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>str</code> <p>the current observation</p> required <code>num_results</code> <code>int</code> <p>the number of results to return</p> <code>1</code> Source code in <code>adala/memories/base.py</code> <pre><code>@abstractmethod\ndef retrieve(self, observation: str, num_results: int = 1) -&gt; Any:\n    \"\"\"\n    Base method for retrieving past experiences from long term memory, based on current observations\n\n    Args:\n        observation: the current observation\n        num_results: the number of results to return\n    \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.retrieve_many","title":"<code>retrieve_many(observations, num_results=1)</code>","text":"<p>Base method for retrieving past experiences from long term memory, based on current observations</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <p>the current observation</p> required <code>num_results</code> <code>int</code> <p>the number of results to return</p> <code>1</code> Source code in <code>adala/memories/base.py</code> <pre><code>def retrieve_many(self, observations: List[str], num_results: int = 1) -&gt; List[Any]:\n    \"\"\"\n    Base method for retrieving past experiences from long term memory, based on current observations\n\n    Args:\n        observation: the current observation\n        num_results: the number of results to return\n    \"\"\"\n    return [self.retrieve(observation) for observation in observations]\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory","title":"<code>FileMemory</code>","text":"<p>               Bases: <code>Memory</code></p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>class FileMemory(Memory):\n    filepath: str\n\n    def remember(self, observation: str, experience: Any):\n        \"\"\"\n        Serialize experience in JSON and append to file\n        \"\"\"\n        with open(self.filepath) as f:\n            memory = json.load(f)\n        memory[observation] = experience\n        with open(self.filepath, \"w\") as f:\n            json.dump(memory, f, indent=2)\n\n    def retrieve(self, observation: str) -&gt; Any:\n        \"\"\"\n        Retrieve experience from file\n        \"\"\"\n        with open(self.filepath) as f:\n            memory = json.load(f)\n        return memory[observation]\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory.remember","title":"<code>remember(observation, experience)</code>","text":"<p>Serialize experience in JSON and append to file</p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>def remember(self, observation: str, experience: Any):\n    \"\"\"\n    Serialize experience in JSON and append to file\n    \"\"\"\n    with open(self.filepath) as f:\n        memory = json.load(f)\n    memory[observation] = experience\n    with open(self.filepath, \"w\") as f:\n        json.dump(memory, f, indent=2)\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory.retrieve","title":"<code>retrieve(observation)</code>","text":"<p>Retrieve experience from file</p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>def retrieve(self, observation: str) -&gt; Any:\n    \"\"\"\n    Retrieve experience from file\n    \"\"\"\n    with open(self.filepath) as f:\n        memory = json.load(f)\n    return memory[observation]\n</code></pre>"},{"location":"runtimes/","title":"Runtimes","text":""},{"location":"runtimes/#adala.runtimes.base.AsyncRuntime","title":"<code>AsyncRuntime</code>","text":"<p>               Bases: <code>Runtime</code></p> <p>Async version of runtime that uses asyncio to process batch of records.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>class AsyncRuntime(Runtime):\n    \"\"\"Async version of runtime that uses asyncio to process batch of records.\"\"\"\n\n    @abstractmethod\n    async def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, Any]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n        response_model: Optional[Type[BaseModel]] = None,\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Processes a record.\n\n        Args:\n            record (Dict[str, str]): The record to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            output_template (str): The output template.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n            response_model (Optional[Type[BaseModel]]): The response model to use for processing records. Defaults to None.\n                                                        If set, the response will be generated according to this model and `output_template` and `field_schema` fields will be ignored.\n                                                        Note, explicitly providing ResponseModel will be the default behavior for all runtimes in the future.\n\n        Returns:\n            Dict[str, str]: The processed record.\n        \"\"\"\n\n    @abstractmethod\n    async def batch_to_batch(\n        self,\n        batch: InternalDataFrame,\n        input_template: str,\n        instructions_template: str,\n        response_model: Type[BaseModel],\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n        output_template: Optional[\n            str\n        ] = None,  # TODO: deprecated in favor of response_model, can be removed\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Processes a record.\n\n        Args:\n            batch (InternalDataFrame): The batch to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            response_model (Optional[Type[BaseModel]]): The response model to use for processing records.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n            output_template (str): The output template. Deprecated.\n\n        Returns:\n            InternalDataFrame: The processed batch.\n        \"\"\"\n        output = batch.progress_apply(\n            self.record_to_record,\n            axis=1,\n            result_type=\"expand\",\n            input_template=input_template,\n            instructions_template=instructions_template,\n            output_template=output_template,\n            extra_fields=extra_fields,\n            field_schema=field_schema,\n            instructions_first=instructions_first,\n            response_model=response_model,\n        )\n        return output\n\n    async def get_cost_estimate_async(\n        self, prompt: str, substitutions: List[Dict], output_fields: Optional[List[str]]\n    ) -&gt; CostEstimate:\n        raise NotImplementedError(\"This runtime does not support cost estimates\")\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.AsyncRuntime.batch_to_batch","title":"<code>batch_to_batch(batch, input_template, instructions_template, response_model, extra_fields=None, field_schema=None, instructions_first=True, output_template=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Processes a record.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>InternalDataFrame</code> <p>The batch to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>response_model</code> <code>Optional[Type[BaseModel]]</code> <p>The response model to use for processing records.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <code>output_template</code> <code>str</code> <p>The output template. Deprecated.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The processed batch.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>@abstractmethod\nasync def batch_to_batch(\n    self,\n    batch: InternalDataFrame,\n    input_template: str,\n    instructions_template: str,\n    response_model: Type[BaseModel],\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n    output_template: Optional[\n        str\n    ] = None,  # TODO: deprecated in favor of response_model, can be removed\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Processes a record.\n\n    Args:\n        batch (InternalDataFrame): The batch to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        response_model (Optional[Type[BaseModel]]): The response model to use for processing records.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n        output_template (str): The output template. Deprecated.\n\n    Returns:\n        InternalDataFrame: The processed batch.\n    \"\"\"\n    output = batch.progress_apply(\n        self.record_to_record,\n        axis=1,\n        result_type=\"expand\",\n        input_template=input_template,\n        instructions_template=instructions_template,\n        output_template=output_template,\n        extra_fields=extra_fields,\n        field_schema=field_schema,\n        instructions_first=instructions_first,\n        response_model=response_model,\n    )\n    return output\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.AsyncRuntime.record_to_record","title":"<code>record_to_record(record, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=True, response_model=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Processes a record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>The record to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>output_template</code> <code>str</code> <p>The output template.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <code>response_model</code> <code>Optional[Type[BaseModel]]</code> <p>The response model to use for processing records. Defaults to None.                                         If set, the response will be generated according to this model and <code>output_template</code> and <code>field_schema</code> fields will be ignored.                                         Note, explicitly providing ResponseModel will be the default behavior for all runtimes in the future.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: The processed record.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>@abstractmethod\nasync def record_to_record(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, Any]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n    response_model: Optional[Type[BaseModel]] = None,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Processes a record.\n\n    Args:\n        record (Dict[str, str]): The record to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        output_template (str): The output template.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n        response_model (Optional[Type[BaseModel]]): The response model to use for processing records. Defaults to None.\n                                                    If set, the response will be generated according to this model and `output_template` and `field_schema` fields will be ignored.\n                                                    Note, explicitly providing ResponseModel will be the default behavior for all runtimes in the future.\n\n    Returns:\n        Dict[str, str]: The processed record.\n    \"\"\"\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime","title":"<code>Runtime</code>","text":"<p>               Bases: <code>BaseModelInRegistry</code></p> <p>Base class representing a generic runtime environment.</p> <p>Attributes:</p> Name Type Description <code>verbose</code> <code>bool</code> <p>Flag indicating if runtime outputs should be verbose. Defaults to False.</p> <code>batch_size</code> <code>Optional[int]</code> <p>The batch size to use for processing records. Defaults to None.</p> <code>concurrency</code> <code>Optional[int]</code> <p>The number of parallel processes to use for processing records. Defaults to 1.                         Note that when parallel processing is used, the memory footprint will be doubled compared to sequential processing.</p> <code>response_model</code> <code>Optional[Type[BaseModel]]</code> <p>The response model to use for processing records. Defaults to None.                                         If set, the response will be generated according to this model and <code>output_template</code> and <code>field_schema</code> fields will be ignored.                                         Note, explicitly providing ResponseModel will be the default behavior for all runtimes in the future.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>class Runtime(BaseModelInRegistry):\n    \"\"\"\n    Base class representing a generic runtime environment.\n\n    Attributes:\n        verbose (bool): Flag indicating if runtime outputs should be verbose. Defaults to False.\n        batch_size (Optional[int]): The batch size to use for processing records. Defaults to None.\n        concurrency (Optional[int]): The number of parallel processes to use for processing records. Defaults to 1.\n                                    Note that when parallel processing is used, the memory footprint will be doubled compared to sequential processing.\n        response_model (Optional[Type[BaseModel]]): The response model to use for processing records. Defaults to None.\n                                                    If set, the response will be generated according to this model and `output_template` and `field_schema` fields will be ignored.\n                                                    Note, explicitly providing ResponseModel will be the default behavior for all runtimes in the future.\n    \"\"\"\n\n    verbose: bool = False\n    batch_size: Optional[int] = None\n    concurrency: Optional[int] = Field(default=1, alias=\"concurrent_clients\")\n\n    @model_validator(mode=\"after\")\n    def init_runtime(self) -&gt; \"Runtime\":\n        \"\"\"Initializes the runtime.\n\n        This method should be used to validate and potentially initialize the runtime instance.\n\n        Returns:\n            Runtime: The initialized runtime instance.\n        \"\"\"\n        return self\n\n    @abstractmethod\n    def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        response_model: Type[BaseModel],\n        extra_fields: Optional[Dict[str, Any]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n        output_template: Optional[\n            str\n        ] = None,  # TODO: deprecated in favor of response_model, can be removed\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Processes a record.\n\n        Args:\n            record (Dict[str, str]): The record to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            response_model (Type[BaseModel]): The response model to use for processing records.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n            output_template (str): The output template. Deprecated.\n\n        Returns:\n            Dict[str, str]: The processed record.\n        \"\"\"\n\n    def batch_to_batch(\n        self,\n        batch: InternalDataFrame,\n        input_template: str,\n        instructions_template: str,\n        response_model: Type[BaseModel],\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n        output_template: Optional[\n            str\n        ] = None,  # TODO: deprecated in favor of response_model, can be removed\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Processes a record.\n        It supports parallel processing of the batch:\n         - when the `concurrency` is set to -1 (using all available CPUs),\n         - when the `concurrency` is set to 1 (sequential processing),\n         - when the `concurrency` is set to a fixed number of CPUs.\n        Please note that parallel processing doubles the memory footprint compared to sequential processing.\n\n        Args:\n            batch (InternalDataFrame): The batch to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions' template.\n            response_model (Type[BaseModel]): The response model to use for processing records.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n            output_template (str): The output template. Deprecated.\n        Returns:\n            InternalDataFrame: The processed batch.\n        \"\"\"\n        if self.concurrency == -1:\n            # run batch processing each row in a parallel way, using all available CPUs\n            logger.info(\"Running batch processing in parallel using all available CPUs\")\n            pandarallel.initialize(progress_bar=self.verbose)\n            apply_func = batch.parallel_apply\n        elif self.concurrency == 1:\n            # run batch processing each row in a sequential way\n            logger.info(\"Running batch processing sequentially\")\n            if self.verbose:\n                apply_func = batch.progress_apply\n            else:\n                apply_func = batch.apply\n        elif self.concurrency &gt; 1:\n            # run batch processing each row in a parallel way, using a fixed number of CPUs\n            logger.info(\n                f\"Running batch processing in parallel using {self.concurrency} CPUs\"\n            )\n            # Warning: parallel processing doubles the memory footprint compared to sequential processing\n            # read more about https://nalepae.github.io/pandarallel/\n            pandarallel.initialize(\n                nb_workers=self.concurrency, progress_bar=self.verbose\n            )\n            apply_func = batch.parallel_apply\n        else:\n            raise ValueError(f\"Invalid concurrency value: {self.concurrency}\")\n\n        output = apply_func(\n            self.record_to_record,\n            axis=1,\n            result_type=\"expand\",\n            input_template=input_template,\n            instructions_template=instructions_template,\n            output_template=output_template,\n            extra_fields=extra_fields,\n            field_schema=field_schema,\n            instructions_first=instructions_first,\n            response_model=response_model,\n        )\n        return output\n\n    def record_to_batch(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        response_model: Type[BaseModel],\n        output_batch_size: int = 1,\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n        output_template: Optional[\n            str\n        ] = None,  # TODO: deprecated in favor of response_model, can be removed\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Processes a record and return a batch.\n\n        Args:\n            record (Dict[str, str]): The record to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            response_model (Optional[Type[BaseModel]]): The response model to use for processing records. Defaults to None.\n            output_batch_size (int): The batch size for the output. Defaults to 1.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n            output_template (str): The output template. Deprecated.\n\n        Returns:\n            InternalDataFrame: The processed batch.\n        \"\"\"\n        batch = InternalDataFrame([record] * output_batch_size)\n        return self.batch_to_batch(\n            batch=batch,\n            input_template=input_template,\n            instructions_template=instructions_template,\n            output_template=output_template,\n            extra_fields=extra_fields,\n            field_schema=field_schema,\n            instructions_first=instructions_first,\n            response_model=response_model,\n        )\n\n    def get_cost_estimate(\n        self, prompt: str, substitutions: List[Dict], output_fields: Optional[List[str]]\n    ) -&gt; CostEstimate:\n        raise NotImplementedError(\"This runtime does not support cost estimates\")\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.batch_to_batch","title":"<code>batch_to_batch(batch, input_template, instructions_template, response_model, extra_fields=None, field_schema=None, instructions_first=True, output_template=None)</code>","text":"<p>Processes a record. It supports parallel processing of the batch:  - when the <code>concurrency</code> is set to -1 (using all available CPUs),  - when the <code>concurrency</code> is set to 1 (sequential processing),  - when the <code>concurrency</code> is set to a fixed number of CPUs. Please note that parallel processing doubles the memory footprint compared to sequential processing.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>InternalDataFrame</code> <p>The batch to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions' template.</p> required <code>response_model</code> <code>Type[BaseModel]</code> <p>The response model to use for processing records.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <code>output_template</code> <code>str</code> <p>The output template. Deprecated.</p> <code>None</code> <p>Returns:     InternalDataFrame: The processed batch.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def batch_to_batch(\n    self,\n    batch: InternalDataFrame,\n    input_template: str,\n    instructions_template: str,\n    response_model: Type[BaseModel],\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n    output_template: Optional[\n        str\n    ] = None,  # TODO: deprecated in favor of response_model, can be removed\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Processes a record.\n    It supports parallel processing of the batch:\n     - when the `concurrency` is set to -1 (using all available CPUs),\n     - when the `concurrency` is set to 1 (sequential processing),\n     - when the `concurrency` is set to a fixed number of CPUs.\n    Please note that parallel processing doubles the memory footprint compared to sequential processing.\n\n    Args:\n        batch (InternalDataFrame): The batch to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions' template.\n        response_model (Type[BaseModel]): The response model to use for processing records.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n        output_template (str): The output template. Deprecated.\n    Returns:\n        InternalDataFrame: The processed batch.\n    \"\"\"\n    if self.concurrency == -1:\n        # run batch processing each row in a parallel way, using all available CPUs\n        logger.info(\"Running batch processing in parallel using all available CPUs\")\n        pandarallel.initialize(progress_bar=self.verbose)\n        apply_func = batch.parallel_apply\n    elif self.concurrency == 1:\n        # run batch processing each row in a sequential way\n        logger.info(\"Running batch processing sequentially\")\n        if self.verbose:\n            apply_func = batch.progress_apply\n        else:\n            apply_func = batch.apply\n    elif self.concurrency &gt; 1:\n        # run batch processing each row in a parallel way, using a fixed number of CPUs\n        logger.info(\n            f\"Running batch processing in parallel using {self.concurrency} CPUs\"\n        )\n        # Warning: parallel processing doubles the memory footprint compared to sequential processing\n        # read more about https://nalepae.github.io/pandarallel/\n        pandarallel.initialize(\n            nb_workers=self.concurrency, progress_bar=self.verbose\n        )\n        apply_func = batch.parallel_apply\n    else:\n        raise ValueError(f\"Invalid concurrency value: {self.concurrency}\")\n\n    output = apply_func(\n        self.record_to_record,\n        axis=1,\n        result_type=\"expand\",\n        input_template=input_template,\n        instructions_template=instructions_template,\n        output_template=output_template,\n        extra_fields=extra_fields,\n        field_schema=field_schema,\n        instructions_first=instructions_first,\n        response_model=response_model,\n    )\n    return output\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.init_runtime","title":"<code>init_runtime()</code>","text":"<p>Initializes the runtime.</p> <p>This method should be used to validate and potentially initialize the runtime instance.</p> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The initialized runtime instance.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>@model_validator(mode=\"after\")\ndef init_runtime(self) -&gt; \"Runtime\":\n    \"\"\"Initializes the runtime.\n\n    This method should be used to validate and potentially initialize the runtime instance.\n\n    Returns:\n        Runtime: The initialized runtime instance.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.record_to_batch","title":"<code>record_to_batch(record, input_template, instructions_template, response_model, output_batch_size=1, extra_fields=None, field_schema=None, instructions_first=True, output_template=None)</code>","text":"<p>Processes a record and return a batch.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>The record to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>response_model</code> <code>Optional[Type[BaseModel]]</code> <p>The response model to use for processing records. Defaults to None.</p> required <code>output_batch_size</code> <code>int</code> <p>The batch size for the output. Defaults to 1.</p> <code>1</code> <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <code>output_template</code> <code>str</code> <p>The output template. Deprecated.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The processed batch.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def record_to_batch(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    response_model: Type[BaseModel],\n    output_batch_size: int = 1,\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n    output_template: Optional[\n        str\n    ] = None,  # TODO: deprecated in favor of response_model, can be removed\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Processes a record and return a batch.\n\n    Args:\n        record (Dict[str, str]): The record to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        response_model (Optional[Type[BaseModel]]): The response model to use for processing records. Defaults to None.\n        output_batch_size (int): The batch size for the output. Defaults to 1.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n        output_template (str): The output template. Deprecated.\n\n    Returns:\n        InternalDataFrame: The processed batch.\n    \"\"\"\n    batch = InternalDataFrame([record] * output_batch_size)\n    return self.batch_to_batch(\n        batch=batch,\n        input_template=input_template,\n        instructions_template=instructions_template,\n        output_template=output_template,\n        extra_fields=extra_fields,\n        field_schema=field_schema,\n        instructions_first=instructions_first,\n        response_model=response_model,\n    )\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.record_to_record","title":"<code>record_to_record(record, input_template, instructions_template, response_model, extra_fields=None, field_schema=None, instructions_first=True, output_template=None)</code>  <code>abstractmethod</code>","text":"<p>Processes a record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>The record to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>response_model</code> <code>Type[BaseModel]</code> <p>The response model to use for processing records.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <code>output_template</code> <code>str</code> <p>The output template. Deprecated.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: The processed record.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>@abstractmethod\ndef record_to_record(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    response_model: Type[BaseModel],\n    extra_fields: Optional[Dict[str, Any]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n    output_template: Optional[\n        str\n    ] = None,  # TODO: deprecated in favor of response_model, can be removed\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Processes a record.\n\n    Args:\n        record (Dict[str, str]): The record to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        response_model (Type[BaseModel]): The response model to use for processing records.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n        output_template (str): The output template. Deprecated.\n\n    Returns:\n        Dict[str, str]: The processed record.\n    \"\"\"\n</code></pre>"},{"location":"skills/","title":"Skills","text":""},{"location":"skills/#adala.skills._base.AnalysisSkill","title":"<code>AnalysisSkill</code>","text":"<p>               Bases: <code>Skill</code></p> <p>Analysis skill that analyzes a dataframe and returns a record (e.g. for data analysis purposes). See base class Skill for more information about the attributes.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class AnalysisSkill(Skill):\n    \"\"\"\n    Analysis skill that analyzes a dataframe and returns a record (e.g. for data analysis purposes).\n    See base class Skill for more information about the attributes.\n    \"\"\"\n\n    input_prefix: str = \"\"\n    input_separator: str = \"\\n\"\n    chunk_size: Optional[int] = None\n\n    def _iter_over_chunks(\n        self, input: InternalDataFrame, chunk_size: Optional[int] = None\n    ):\n        \"\"\"\n        Iterates over chunks of the input dataframe.\n        Returns a generator of strings that are the concatenation of the rows of the chunk with `input_separator`\n        interpolated with the `input_template` and `extra_fields`.\n        \"\"\"\n\n        if input.empty:\n            yield \"\"\n            return\n\n        if isinstance(input, InternalSeries):\n            input = input.to_frame()\n        elif isinstance(input, dict):\n            input = InternalDataFrame([input])\n\n        extra_fields = self._get_extra_fields()\n\n        # if chunk_size is specified, split the input into chunks and process each chunk separately\n        if self.chunk_size is not None:\n            chunks = (\n                input.iloc[i : i + self.chunk_size]\n                for i in range(0, len(input), self.chunk_size)\n            )\n        else:\n            chunks = [input]\n\n        # define the row preprocessing function\n        def row_preprocessing(row):\n            return partial_str_format(\n                self.input_template, **row, **extra_fields, i=int(row.name) + 1\n            )\n\n        total = input.shape[0] // self.chunk_size if self.chunk_size is not None else 1\n        for chunk in tqdm(chunks, desc=\"Processing chunks\", total=total):\n            # interpolate every row with input_template and concatenate them with input_separator to produce a single string\n            agg_chunk = (\n                chunk.reset_index()\n                .apply(row_preprocessing, axis=1)\n                .str.cat(sep=self.input_separator)\n            )\n            yield agg_chunk\n\n    def apply(\n        self,\n        input: Union[InternalDataFrame, InternalSeries, Dict],\n        runtime: Runtime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a dataframe and returns a record.\n\n        Args:\n            input (InternalDataFrame): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalSeries: The record containing the analysis results.\n        \"\"\"\n        outputs = []\n        for agg_chunk in self._iter_over_chunks(input):\n            output = runtime.record_to_record(\n                {\"input\": f\"{self.input_prefix}{agg_chunk}\"},\n                input_template=\"{input}\",\n                output_template=self.output_template,\n                instructions_template=self.instructions,\n                instructions_first=self.instructions_first,\n                response_model=self.response_model,\n            )\n            outputs.append(InternalSeries(output))\n        output = InternalDataFrame(outputs)\n\n        return output\n\n    async def aapply(\n        self,\n        input: Union[InternalDataFrame, InternalSeries, Dict],\n        runtime: AsyncRuntime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a dataframe and returns a record.\n        \"\"\"\n        outputs = []\n        for agg_chunk in self._iter_over_chunks(input):\n            output = await runtime.record_to_record(\n                {\"input\": f\"{self.input_prefix}{agg_chunk}\"},\n                input_template=\"{input}\",\n                output_template=self.output_template,\n                instructions_template=self.instructions,\n                instructions_first=self.instructions_first,\n                response_model=self.response_model,\n            )\n            outputs.append(InternalSeries(output))\n        output = InternalDataFrame(outputs)\n\n        return output\n\n    def improve(self, **kwargs):\n        \"\"\"\n        Improves the skill.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.AnalysisSkill.aapply","title":"<code>aapply(input, runtime)</code>  <code>async</code>","text":"<p>Applies the skill to a dataframe and returns a record.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>async def aapply(\n    self,\n    input: Union[InternalDataFrame, InternalSeries, Dict],\n    runtime: AsyncRuntime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a dataframe and returns a record.\n    \"\"\"\n    outputs = []\n    for agg_chunk in self._iter_over_chunks(input):\n        output = await runtime.record_to_record(\n            {\"input\": f\"{self.input_prefix}{agg_chunk}\"},\n            input_template=\"{input}\",\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            instructions_first=self.instructions_first,\n            response_model=self.response_model,\n        )\n        outputs.append(InternalSeries(output))\n    output = InternalDataFrame(outputs)\n\n    return output\n</code></pre>"},{"location":"skills/#adala.skills._base.AnalysisSkill.apply","title":"<code>apply(input, runtime)</code>","text":"<p>Applies the skill to a dataframe and returns a record.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalSeries</code> <code>InternalDataFrame</code> <p>The record containing the analysis results.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def apply(\n    self,\n    input: Union[InternalDataFrame, InternalSeries, Dict],\n    runtime: Runtime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a dataframe and returns a record.\n\n    Args:\n        input (InternalDataFrame): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalSeries: The record containing the analysis results.\n    \"\"\"\n    outputs = []\n    for agg_chunk in self._iter_over_chunks(input):\n        output = runtime.record_to_record(\n            {\"input\": f\"{self.input_prefix}{agg_chunk}\"},\n            input_template=\"{input}\",\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            instructions_first=self.instructions_first,\n            response_model=self.response_model,\n        )\n        outputs.append(InternalSeries(output))\n    output = InternalDataFrame(outputs)\n\n    return output\n</code></pre>"},{"location":"skills/#adala.skills._base.AnalysisSkill.improve","title":"<code>improve(**kwargs)</code>","text":"<p>Improves the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def improve(self, **kwargs):\n    \"\"\"\n    Improves the skill.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.SampleTransformSkill","title":"<code>SampleTransformSkill</code>","text":"<p>               Bases: <code>TransformSkill</code></p> Source code in <code>adala/skills/_base.py</code> <pre><code>class SampleTransformSkill(TransformSkill):\n    sample_size: int\n\n    def apply(\n        self,\n        input: InternalDataFrame,\n        runtime: Runtime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a dataframe and returns a dataframe.\n\n        Args:\n            input (InternalDataFrame): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalDataFrame: The processed data.\n        \"\"\"\n        return super(SampleTransformSkill, self).apply(\n            input.sample(self.sample_size), runtime\n        )\n</code></pre>"},{"location":"skills/#adala.skills._base.SampleTransformSkill.apply","title":"<code>apply(input, runtime)</code>","text":"<p>Applies the skill to a dataframe and returns a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The processed data.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def apply(\n    self,\n    input: InternalDataFrame,\n    runtime: Runtime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a dataframe and returns a dataframe.\n\n    Args:\n        input (InternalDataFrame): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalDataFrame: The processed data.\n    \"\"\"\n    return super(SampleTransformSkill, self).apply(\n        input.sample(self.sample_size), runtime\n    )\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill","title":"<code>Skill</code>","text":"<p>               Bases: <code>BaseModelInRegistry</code></p> <p>Abstract base class representing a skill.</p> <p>Provides methods to interact with and obtain information about skills.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name of the skill.</p> <code>instructions</code> <code>str</code> <p>Instructs agent what to do with the input data.</p> <code>input_template</code> <code>str</code> <p>Template for the input data.</p> <code>output_template</code> <code>str</code> <p>Template for the output data.</p> <code>description</code> <code>Optional[str]</code> <p>Description of the skill.</p> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>instructions_first</code> <code>bool</code> <p>Flag indicating if instructions should be executed before input. Defaults to True.</p> <code>verbose</code> <code>bool</code> <p>Flag indicating if runtime outputs should be verbose. Defaults to False.</p> <code>frozen</code> <code>bool</code> <p>Flag indicating if the skill is frozen. Defaults to False.</p> <code>response_model</code> <code>Optional[Type[BaseModel]]</code> <p>Pydantic-based response model for the skill. If used, <code>output_template</code> and <code>field_schema</code> are ignored. Note that using <code>response_model</code> will become the default in the future.</p> <code>type</code> <code>ClassVar[str]</code> <p>Type of the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class Skill(BaseModelInRegistry):\n    \"\"\"\n    Abstract base class representing a skill.\n\n    Provides methods to interact with and obtain information about skills.\n\n    Attributes:\n        name (str): Unique name of the skill.\n        instructions (str): Instructs agent what to do with the input data.\n        input_template (str): Template for the input data.\n        output_template (str): Template for the output data.\n        description (Optional[str]): Description of the skill.\n        field_schema (Optional[Dict]): Field [JSON schema](https://json-schema.org/) to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        instructions_first (bool): Flag indicating if instructions should be executed before input. Defaults to True.\n        verbose (bool): Flag indicating if runtime outputs should be verbose. Defaults to False.\n        frozen (bool): Flag indicating if the skill is frozen. Defaults to False.\n        response_model (Optional[Type[BaseModel]]): Pydantic-based response model for the skill. If used, `output_template` and `field_schema` are ignored. Note that using `response_model` will become the default in the future.\n        type (ClassVar[str]): Type of the skill.\n    \"\"\"\n\n    name: str = Field(\n        title=\"Skill name\",\n        description=\"Unique name of the skill\",\n        examples=[\"labeling\", \"classification\", \"text-generation\"],\n    )\n    instructions: str = Field(\n        title=\"Skill instructions\",\n        description=\"Instructs agent what to do with the input data. \"\n        \"Can use templating to refer to input fields.\",\n        examples=[\"Label the input text with the following labels: {labels}\"],\n        # TODO: instructions can be deprecated in favor of using `input_template` to specify the instructions\n        default=\"\",\n    )\n    input_template: str = Field(\n        title=\"Input template\",\n        description=\"Template for the input data. \"\n        \"Can use templating to refer to input parameters and perform data transformations.\",\n        examples=[\"Input: {input}\", \"Input: {input}\\nLabels: {labels}\\nOutput: \"],\n    )\n    output_template: str = Field(\n        title=\"Output template\",\n        description=\"Template for the output data. \"\n        \"Can use templating to refer to input parameters and perform data transformations\",\n        examples=[\"Output: {output}\", \"{predictions}\"],\n        # TODO: output_template can be deprecated in favor of using `response_model` to specify the output\n        default=\"\",\n    )\n    description: Optional[str] = Field(\n        default=\"\",\n        title=\"Skill description\",\n        description=\"Description of the skill. Can be used to retrieve skill from the library.\",\n        examples=[\"The skill to perform sentiment analysis on the input text.\"],\n    )\n    field_schema: Optional[Dict[str, Any]] = Field(\n        default=None,\n        title=\"Field schema\",\n        description=\"JSON schema for the fields of the input and output data.\",\n        examples=[\n            {\n                \"input\": {\"type\": \"string\"},\n                \"output\": {\"type\": \"string\"},\n                \"labels\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"positive\", \"negative\", \"neutral\"],\n                    },\n                },\n            }\n        ],\n    )\n    instructions_first: bool = Field(\n        default=True,\n        title=\"Instructions first\",\n        description=\"Flag indicating if instructions should be shown before the input data.\",\n        examples=[True, False],\n    )\n\n    frozen: bool = Field(\n        default=False,\n        title=\"Frozen\",\n        description=\"Flag indicating if the skill is frozen.\",\n        examples=[True, False],\n    )\n\n    response_model: Type[BaseModel] = Field(\n        default=None,\n        title=\"Response model\",\n        description=\"Pydantic-based response model for the skill. If used, `output_template` and `field_schema` are ignored.\",\n    )\n\n    def _get_extra_fields(self):\n        \"\"\"\n        Retrieves fields that are not categorized as system fields.\n\n        Returns:\n            dict: A dictionary containing fields that are not system fields.\n        \"\"\"\n\n        # TODO: more robust way to exclude system fields\n        system_fields = {\n            \"name\",\n            \"description\",\n            \"input_template\",\n            \"output_template\",\n            \"instructions\",\n            \"field_schema\",\n            \"extra_fields\",\n            \"instructions_first\",\n            \"verbose\",\n            \"frozen\",\n            \"response_model\",\n            \"type\",\n        }\n        extra_fields = self.model_dump(exclude=system_fields)\n        return extra_fields\n\n    def get_input_fields(self):\n        \"\"\"\n        Retrieves input fields.\n\n        Returns:\n            List[str]: A list of input fields.\n        \"\"\"\n        extra_fields = self._get_extra_fields()\n        input_fields = parse_template(\n            partial_str_format(self.input_template, **extra_fields),\n            include_texts=False,\n        )\n        return [f[\"text\"] for f in input_fields]\n\n    def get_output_fields(self):\n        \"\"\"\n        Retrieves output fields.\n\n        Returns:\n            List[str]: A list of output fields.\n        \"\"\"\n        if self.response_model:\n            return list(self.response_model.__fields__.keys())\n        if self.field_schema:\n            return list(self.field_schema.keys())\n\n        extra_fields = self._get_extra_fields()\n        # TODO: input fields are not considered - shall we disallow input fields in output template?\n        output_fields = parse_template(\n            partial_str_format(self.output_template, **extra_fields),\n            include_texts=False,\n        )\n        return [f[\"text\"] for f in output_fields]\n\n    def _create_response_model_from_field_schema(self):\n        assert self.field_schema, \"field_schema is required to create a response model\"\n        if self.response_model:\n            return\n        self.response_model = field_schema_to_pydantic_class(\n            self.field_schema, self.name, self.description\n        )\n\n    @model_validator(mode=\"after\")\n    def validate_response_model(self):\n        if self.response_model:\n            # if response_model, we use it right away\n            return self\n\n        if not self.field_schema:\n            # if field_schema is not provided, extract it from `output_template`\n            logger.info(\n                f\"Parsing output_template to generate the response model: {self.output_template}\"\n            )\n            self.field_schema = {}\n            chunks = parse_template(self.output_template)\n\n            previous_text = \"\"\n            for chunk in chunks:\n                if chunk[\"type\"] == \"text\":\n                    previous_text = chunk[\"text\"]\n                if chunk[\"type\"] == \"var\":\n                    field_name = chunk[\"text\"]\n                    # by default, all fields are strings\n                    field_type = \"string\"\n\n                    # if description is not provided, use the text before the field,\n                    # otherwise use the field name with underscores replaced by spaces\n                    field_description = previous_text or field_name.replace(\"_\", \" \")\n                    field_description = field_description.strip(\n                        string.punctuation\n                    ).strip()\n                    previous_text = \"\"\n\n                    # create default JSON schema entry for the field\n                    self.field_schema[field_name] = {\n                        \"type\": field_type,\n                        \"description\": field_description,\n                    }\n\n        self._create_response_model_from_field_schema()\n        return self\n\n    # When serializing the agent, ensure `response_model` is excluded.\n    # It will be restored from `field_schema` during deserialization.\n    @field_serializer(\"response_model\")\n    def serialize_response_model(self, value):\n        return None\n\n    # remove `response_model` from the pickle serialization\n    def __getstate__(self):\n        state = super().__getstate__()\n        state[\"__dict__\"][\"response_model\"] = None\n        return state\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        # ensure response_model is restored from field_schema, if not already set\n        self._create_response_model_from_field_schema()\n\n    @abstractmethod\n    def apply(self, input, runtime):\n        \"\"\"\n        Base method for applying the skill.\n        \"\"\"\n\n    @abstractmethod\n    def improve(self, predictions, train_skill_output, feedback, runtime):\n        \"\"\"\n        Base method for improving the skill.\n        \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill.apply","title":"<code>apply(input, runtime)</code>  <code>abstractmethod</code>","text":"<p>Base method for applying the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>@abstractmethod\ndef apply(self, input, runtime):\n    \"\"\"\n    Base method for applying the skill.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill.get_input_fields","title":"<code>get_input_fields()</code>","text":"<p>Retrieves input fields.</p> <p>Returns:</p> Type Description <p>List[str]: A list of input fields.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def get_input_fields(self):\n    \"\"\"\n    Retrieves input fields.\n\n    Returns:\n        List[str]: A list of input fields.\n    \"\"\"\n    extra_fields = self._get_extra_fields()\n    input_fields = parse_template(\n        partial_str_format(self.input_template, **extra_fields),\n        include_texts=False,\n    )\n    return [f[\"text\"] for f in input_fields]\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill.get_output_fields","title":"<code>get_output_fields()</code>","text":"<p>Retrieves output fields.</p> <p>Returns:</p> Type Description <p>List[str]: A list of output fields.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def get_output_fields(self):\n    \"\"\"\n    Retrieves output fields.\n\n    Returns:\n        List[str]: A list of output fields.\n    \"\"\"\n    if self.response_model:\n        return list(self.response_model.__fields__.keys())\n    if self.field_schema:\n        return list(self.field_schema.keys())\n\n    extra_fields = self._get_extra_fields()\n    # TODO: input fields are not considered - shall we disallow input fields in output template?\n    output_fields = parse_template(\n        partial_str_format(self.output_template, **extra_fields),\n        include_texts=False,\n    )\n    return [f[\"text\"] for f in output_fields]\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill.improve","title":"<code>improve(predictions, train_skill_output, feedback, runtime)</code>  <code>abstractmethod</code>","text":"<p>Base method for improving the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>@abstractmethod\ndef improve(self, predictions, train_skill_output, feedback, runtime):\n    \"\"\"\n    Base method for improving the skill.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills._base.SynthesisSkill","title":"<code>SynthesisSkill</code>","text":"<p>               Bases: <code>Skill</code></p> <p>Synthesis skill that synthesize a dataframe from a record (e.g. for dataset generation purposes). See base class Skill for more information about the attributes.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class SynthesisSkill(Skill):\n    \"\"\"\n    Synthesis skill that synthesize a dataframe from a record (e.g. for dataset generation purposes).\n    See base class Skill for more information about the attributes.\n    \"\"\"\n\n    def apply(\n        self,\n        input: Union[Dict, InternalSeries],\n        runtime: Runtime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a record and returns a dataframe.\n\n        Args:\n            input (InternalSeries): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalDataFrame: The synthesized data.\n        \"\"\"\n        if isinstance(input, InternalSeries):\n            input = input.to_dict()\n        return runtime.record_to_batch(\n            input,\n            input_template=self.input_template,\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            field_schema=self.field_schema,\n            extra_fields=self._get_extra_fields(),\n            instructions_first=self.instructions_first,\n            response_model=self.response_model,\n        )\n\n    def improve(self, **kwargs):\n        \"\"\"\n        Improves the skill.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.SynthesisSkill.apply","title":"<code>apply(input, runtime)</code>","text":"<p>Applies the skill to a record and returns a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalSeries</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The synthesized data.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def apply(\n    self,\n    input: Union[Dict, InternalSeries],\n    runtime: Runtime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a record and returns a dataframe.\n\n    Args:\n        input (InternalSeries): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalDataFrame: The synthesized data.\n    \"\"\"\n    if isinstance(input, InternalSeries):\n        input = input.to_dict()\n    return runtime.record_to_batch(\n        input,\n        input_template=self.input_template,\n        output_template=self.output_template,\n        instructions_template=self.instructions,\n        field_schema=self.field_schema,\n        extra_fields=self._get_extra_fields(),\n        instructions_first=self.instructions_first,\n        response_model=self.response_model,\n    )\n</code></pre>"},{"location":"skills/#adala.skills._base.SynthesisSkill.improve","title":"<code>improve(**kwargs)</code>","text":"<p>Improves the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def improve(self, **kwargs):\n    \"\"\"\n    Improves the skill.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill","title":"<code>TransformSkill</code>","text":"<p>               Bases: <code>Skill</code></p> <p>Transform skill that transforms a dataframe to another dataframe (e.g. for data annotation purposes). See base class Skill for more information about the attributes.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class TransformSkill(Skill):\n    \"\"\"\n    Transform skill that transforms a dataframe to another dataframe (e.g. for data annotation purposes).\n    See base class Skill for more information about the attributes.\n    \"\"\"\n\n    def apply(\n        self,\n        input: InternalDataFrame,\n        runtime: Runtime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a dataframe and returns another dataframe.\n\n        Args:\n            input (InternalDataFrame): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalDataFrame: The transformed data.\n        \"\"\"\n\n        return runtime.batch_to_batch(\n            input,\n            input_template=self.input_template,\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            field_schema=self.field_schema,\n            extra_fields=self._get_extra_fields(),\n            instructions_first=self.instructions_first,\n            response_model=self.response_model,\n        )\n\n    async def aapply(\n        self,\n        input: InternalDataFrame,\n        runtime: AsyncRuntime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a dataframe and returns another dataframe.\n\n        Args:\n            input (InternalDataFrame): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalDataFrame: The transformed data.\n        \"\"\"\n\n        return await runtime.batch_to_batch(\n            input,\n            input_template=self.input_template,\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            field_schema=self.field_schema,\n            extra_fields=self._get_extra_fields(),\n            instructions_first=self.instructions_first,\n            response_model=self.response_model,\n        )\n\n    def improve(\n        self,\n        predictions: InternalDataFrame,\n        train_skill_output: str,\n        feedback,\n        runtime: Runtime,\n        add_cot: bool = False,\n    ):\n        \"\"\"\n        Improves the skill.\n\n        Args:\n            predictions (InternalDataFrame): The predictions made by the skill.\n            train_skill_output (str): The name of the output field of the skill.\n            feedback (InternalDataFrame): The feedback provided by the user.\n            runtime (Runtime): The runtime instance to be used for processing (CURRENTLY SUPPORTS ONLY `OpenAIChatRuntime`).\n            add_cot (bool): Flag indicating if the skill should be used the Chain-of-Thought strategy. Defaults to False.\n        \"\"\"\n        if feedback.feedback[train_skill_output].isna().all():\n            # No feedback left - nothing to improve\n            return\n\n        if feedback.match[train_skill_output].all():\n            # all feedback is \"correct\" - nothing to improve\n            return\n\n        fb = feedback.feedback.rename(\n            columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n        )\n        analyzed_df = fb.merge(predictions, left_index=True, right_index=True)\n\n        examples = []\n\n        for i, row in enumerate(analyzed_df.to_dict(orient=\"records\")):\n            # if fb marked as NaN, skip\n            if not row[f\"{train_skill_output}__fb\"]:\n                continue\n\n            # TODO: self.output_template can be missed or incompatible with the field_schema\n            # we need to redefine how we create examples for learn()\n            if not self.output_template:\n                raise ValueError(\n                    \"`output_template` is required for improve() method and must contain \"\n                    \"the output fields from `field_schema`\"\n                )\n            examples.append(\n                f\"### Example #{i}\\n\\n\"\n                f\"{partial_str_format(self.input_template, **row)}\\n\\n\"\n                f\"{partial_str_format(self.output_template, **row)}\\n\\n\"\n                f'User feedback: {row[f\"{train_skill_output}__fb\"]}\\n\\n'\n            )\n\n        examples = \"\\n\".join(examples)\n\n        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n\n        # full template\n        if self.instructions_first:\n            full_template = f\"\"\"\n{{prompt}}\n{self.input_template}\n{self.output_template}\"\"\"\n        else:\n            full_template = f\"\"\"\n{self.input_template}\n{{prompt}}\n{self.output_template}\"\"\"\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nA prompt is a text paragraph that outlines the expected actions and instructs the large language model (LLM) to \\\ngenerate a specific output. This prompt is concatenated with the input text, and the \\\nmodel then creates the required output.\nThis describes the full template how the prompt is concatenated with the input to produce the output:\n\n```\n{full_template}\n```\n\nHere:\n- \"{self.input_template}\" is input template,\n- \"{{prompt}}\" is the LLM prompt,\n- \"{self.output_template}\" is the output template.\n\nModel can produce erroneous output if a prompt is not well defined. \\\nIn our collaboration, we\u2019ll work together to refine a prompt. The process consists of two main steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction examples. Each example contains the input text, the final prediction produced by the model, and the user feedback. \\\nUser feedback indicates whether the model prediction is correct or not. \\\nYour task is to analyze the examples and user feedback, determining whether the \\\nexisting instruction is describing the task reflected by these examples precisely, and suggests changes to the prompt to address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review your reasoning in step 1, integrate the insights to refine the prompt, \\\nand provide me with the new prompt that improves the model\u2019s performance.\"\"\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Sure, I\u2019d be happy to help you with this prompt engineering problem. \"\n                \"Please provide me with the current prompt and the examples with user feedback.\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n## Current prompt\n{self.instructions}\n\n## Examples\n{examples}\n\nSummarize your analysis about incorrect predictions and suggest changes to the prompt.\"\"\",\n            }\n        ]\n        reasoning = runtime.get_llm_response(messages)\n\n        messages += [\n            {\"role\": \"assistant\", \"content\": reasoning},\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nNow please carefully review your reasoning in Step 1 and help with Step 2: refining the prompt.\n\n## Current prompt\n{self.instructions}\n\n## Follow this guidance to refine the prompt:\n\n1. The new prompt should should describe the task precisely, and address the points raised in the user feedback.\n\n2. The new prompt should be similar to the current prompt, and only differ in the parts that address the issues you identified in Step 1.\n    Example:\n    - Current prompt: \"Generate a summary of the input text.\"\n    - New prompt: \"Generate a summary of the input text. Pay attention to the original style.\"\n\n3. Reply only with the new prompt. Do not include input and output templates in the prompt.\n\"\"\",\n            },\n        ]\n\n        if add_cot:\n            cot_instructions = \"\"\"\n\n4. In the new prompt, you should ask the model to perform step-by-step reasoning, and provide rationale or explanations for its prediction before giving the final answer. \\\nInstruct the model to give the final answer at the end of the prompt, using the following template: \"Final answer: &lt;answer&gt;\".\n    Example:\n    - Current prompt: \"Generate a summary of the input text.\"\n    - New prompt: \"Generate a summary of the input text. Explain your reasoning step-by-step. Use the following template to give the final answer at the end of the prompt: \"Final answer: &lt;answer&gt;\".\"\"\"\n            messages[-1][\"content\"] += cot_instructions\n        # display dialogue:\n        for message in messages:\n            print(f'\"{{{message[\"role\"]}}}\":\\n{message[\"content\"]}')\n        new_prompt = runtime.get_llm_response(messages)\n        self.instructions = new_prompt\n\n    async def aimprove(\n        self,\n        teacher_runtime: AsyncRuntime,\n        target_input_variables: List[str],\n        predictions: Optional[InternalDataFrame] = None,\n        instructions: Optional[str] = None,\n    ):\n        \"\"\"\n        Improves the skill.\n        \"\"\"\n\n        from adala.skills.collection.prompt_improvement import (\n            PromptImprovementSkill,\n            ImprovedPromptResponse,\n            ErrorResponseModel,\n            PromptImprovementSkillResponseModel,\n        )\n\n        response_dct = {}\n        try:\n            prompt_improvement_skill = PromptImprovementSkill(\n                skill_to_improve=self,\n                input_variables=target_input_variables,\n                instructions=instructions,\n            )\n            if predictions is None:\n                input_df = InternalDataFrame()\n            else:\n                input_df = predictions\n            response_df = await prompt_improvement_skill.aapply(\n                input=input_df,\n                runtime=teacher_runtime,\n            )\n\n            # awkward to go from response model -&gt; dict -&gt; df -&gt; dict -&gt; response model\n            response_dct = response_df.iloc[0].to_dict()\n\n            # unflatten the response\n            if response_dct.pop(\"_adala_error\", False):\n                output = ErrorResponseModel(**response_dct)\n            else:\n                output = PromptImprovementSkillResponseModel(**response_dct)\n\n        except Exception as e:\n            logger.error(\n                f\"Error improving skill: {e}. Traceback: {traceback.format_exc()}\"\n            )\n            output = ErrorResponseModel(\n                _adala_message=str(e),\n                _adala_details=traceback.format_exc(),\n            )\n\n        # get tokens and token cost\n        resp = ImprovedPromptResponse(output=output, **response_dct)\n        logger.debug(f\"resp: {resp}\")\n\n        return resp\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill.aapply","title":"<code>aapply(input, runtime)</code>  <code>async</code>","text":"<p>Applies the skill to a dataframe and returns another dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The transformed data.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>async def aapply(\n    self,\n    input: InternalDataFrame,\n    runtime: AsyncRuntime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a dataframe and returns another dataframe.\n\n    Args:\n        input (InternalDataFrame): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalDataFrame: The transformed data.\n    \"\"\"\n\n    return await runtime.batch_to_batch(\n        input,\n        input_template=self.input_template,\n        output_template=self.output_template,\n        instructions_template=self.instructions,\n        field_schema=self.field_schema,\n        extra_fields=self._get_extra_fields(),\n        instructions_first=self.instructions_first,\n        response_model=self.response_model,\n    )\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill.aimprove","title":"<code>aimprove(teacher_runtime, target_input_variables, predictions=None, instructions=None)</code>  <code>async</code>","text":"<p>Improves the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>async def aimprove(\n    self,\n    teacher_runtime: AsyncRuntime,\n    target_input_variables: List[str],\n    predictions: Optional[InternalDataFrame] = None,\n    instructions: Optional[str] = None,\n):\n    \"\"\"\n    Improves the skill.\n    \"\"\"\n\n    from adala.skills.collection.prompt_improvement import (\n        PromptImprovementSkill,\n        ImprovedPromptResponse,\n        ErrorResponseModel,\n        PromptImprovementSkillResponseModel,\n    )\n\n    response_dct = {}\n    try:\n        prompt_improvement_skill = PromptImprovementSkill(\n            skill_to_improve=self,\n            input_variables=target_input_variables,\n            instructions=instructions,\n        )\n        if predictions is None:\n            input_df = InternalDataFrame()\n        else:\n            input_df = predictions\n        response_df = await prompt_improvement_skill.aapply(\n            input=input_df,\n            runtime=teacher_runtime,\n        )\n\n        # awkward to go from response model -&gt; dict -&gt; df -&gt; dict -&gt; response model\n        response_dct = response_df.iloc[0].to_dict()\n\n        # unflatten the response\n        if response_dct.pop(\"_adala_error\", False):\n            output = ErrorResponseModel(**response_dct)\n        else:\n            output = PromptImprovementSkillResponseModel(**response_dct)\n\n    except Exception as e:\n        logger.error(\n            f\"Error improving skill: {e}. Traceback: {traceback.format_exc()}\"\n        )\n        output = ErrorResponseModel(\n            _adala_message=str(e),\n            _adala_details=traceback.format_exc(),\n        )\n\n    # get tokens and token cost\n    resp = ImprovedPromptResponse(output=output, **response_dct)\n    logger.debug(f\"resp: {resp}\")\n\n    return resp\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill.apply","title":"<code>apply(input, runtime)</code>","text":"<p>Applies the skill to a dataframe and returns another dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The transformed data.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def apply(\n    self,\n    input: InternalDataFrame,\n    runtime: Runtime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a dataframe and returns another dataframe.\n\n    Args:\n        input (InternalDataFrame): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalDataFrame: The transformed data.\n    \"\"\"\n\n    return runtime.batch_to_batch(\n        input,\n        input_template=self.input_template,\n        output_template=self.output_template,\n        instructions_template=self.instructions,\n        field_schema=self.field_schema,\n        extra_fields=self._get_extra_fields(),\n        instructions_first=self.instructions_first,\n        response_model=self.response_model,\n    )\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill.improve","title":"<code>improve(predictions, train_skill_output, feedback, runtime, add_cot=False)</code>","text":"<p>Improves the skill.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions made by the skill.</p> required <code>train_skill_output</code> <code>str</code> <p>The name of the output field of the skill.</p> required <code>feedback</code> <code>InternalDataFrame</code> <p>The feedback provided by the user.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing (CURRENTLY SUPPORTS ONLY <code>OpenAIChatRuntime</code>).</p> required <code>add_cot</code> <code>bool</code> <p>Flag indicating if the skill should be used the Chain-of-Thought strategy. Defaults to False.</p> <code>False</code> Source code in <code>adala/skills/_base.py</code> <pre><code>    def improve(\n        self,\n        predictions: InternalDataFrame,\n        train_skill_output: str,\n        feedback,\n        runtime: Runtime,\n        add_cot: bool = False,\n    ):\n        \"\"\"\n        Improves the skill.\n\n        Args:\n            predictions (InternalDataFrame): The predictions made by the skill.\n            train_skill_output (str): The name of the output field of the skill.\n            feedback (InternalDataFrame): The feedback provided by the user.\n            runtime (Runtime): The runtime instance to be used for processing (CURRENTLY SUPPORTS ONLY `OpenAIChatRuntime`).\n            add_cot (bool): Flag indicating if the skill should be used the Chain-of-Thought strategy. Defaults to False.\n        \"\"\"\n        if feedback.feedback[train_skill_output].isna().all():\n            # No feedback left - nothing to improve\n            return\n\n        if feedback.match[train_skill_output].all():\n            # all feedback is \"correct\" - nothing to improve\n            return\n\n        fb = feedback.feedback.rename(\n            columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n        )\n        analyzed_df = fb.merge(predictions, left_index=True, right_index=True)\n\n        examples = []\n\n        for i, row in enumerate(analyzed_df.to_dict(orient=\"records\")):\n            # if fb marked as NaN, skip\n            if not row[f\"{train_skill_output}__fb\"]:\n                continue\n\n            # TODO: self.output_template can be missed or incompatible with the field_schema\n            # we need to redefine how we create examples for learn()\n            if not self.output_template:\n                raise ValueError(\n                    \"`output_template` is required for improve() method and must contain \"\n                    \"the output fields from `field_schema`\"\n                )\n            examples.append(\n                f\"### Example #{i}\\n\\n\"\n                f\"{partial_str_format(self.input_template, **row)}\\n\\n\"\n                f\"{partial_str_format(self.output_template, **row)}\\n\\n\"\n                f'User feedback: {row[f\"{train_skill_output}__fb\"]}\\n\\n'\n            )\n\n        examples = \"\\n\".join(examples)\n\n        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n\n        # full template\n        if self.instructions_first:\n            full_template = f\"\"\"\n{{prompt}}\n{self.input_template}\n{self.output_template}\"\"\"\n        else:\n            full_template = f\"\"\"\n{self.input_template}\n{{prompt}}\n{self.output_template}\"\"\"\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nA prompt is a text paragraph that outlines the expected actions and instructs the large language model (LLM) to \\\ngenerate a specific output. This prompt is concatenated with the input text, and the \\\nmodel then creates the required output.\nThis describes the full template how the prompt is concatenated with the input to produce the output:\n\n```\n{full_template}\n```\n\nHere:\n- \"{self.input_template}\" is input template,\n- \"{{prompt}}\" is the LLM prompt,\n- \"{self.output_template}\" is the output template.\n\nModel can produce erroneous output if a prompt is not well defined. \\\nIn our collaboration, we\u2019ll work together to refine a prompt. The process consists of two main steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction examples. Each example contains the input text, the final prediction produced by the model, and the user feedback. \\\nUser feedback indicates whether the model prediction is correct or not. \\\nYour task is to analyze the examples and user feedback, determining whether the \\\nexisting instruction is describing the task reflected by these examples precisely, and suggests changes to the prompt to address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review your reasoning in step 1, integrate the insights to refine the prompt, \\\nand provide me with the new prompt that improves the model\u2019s performance.\"\"\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Sure, I\u2019d be happy to help you with this prompt engineering problem. \"\n                \"Please provide me with the current prompt and the examples with user feedback.\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n## Current prompt\n{self.instructions}\n\n## Examples\n{examples}\n\nSummarize your analysis about incorrect predictions and suggest changes to the prompt.\"\"\",\n            }\n        ]\n        reasoning = runtime.get_llm_response(messages)\n\n        messages += [\n            {\"role\": \"assistant\", \"content\": reasoning},\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nNow please carefully review your reasoning in Step 1 and help with Step 2: refining the prompt.\n\n## Current prompt\n{self.instructions}\n\n## Follow this guidance to refine the prompt:\n\n1. The new prompt should should describe the task precisely, and address the points raised in the user feedback.\n\n2. The new prompt should be similar to the current prompt, and only differ in the parts that address the issues you identified in Step 1.\n    Example:\n    - Current prompt: \"Generate a summary of the input text.\"\n    - New prompt: \"Generate a summary of the input text. Pay attention to the original style.\"\n\n3. Reply only with the new prompt. Do not include input and output templates in the prompt.\n\"\"\",\n            },\n        ]\n\n        if add_cot:\n            cot_instructions = \"\"\"\n\n4. In the new prompt, you should ask the model to perform step-by-step reasoning, and provide rationale or explanations for its prediction before giving the final answer. \\\nInstruct the model to give the final answer at the end of the prompt, using the following template: \"Final answer: &lt;answer&gt;\".\n    Example:\n    - Current prompt: \"Generate a summary of the input text.\"\n    - New prompt: \"Generate a summary of the input text. Explain your reasoning step-by-step. Use the following template to give the final answer at the end of the prompt: \"Final answer: &lt;answer&gt;\".\"\"\"\n            messages[-1][\"content\"] += cot_instructions\n        # display dialogue:\n        for message in messages:\n            print(f'\"{{{message[\"role\"]}}}\":\\n{message[\"content\"]}')\n        new_prompt = runtime.get_llm_response(messages)\n        self.instructions = new_prompt\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet","title":"<code>LinearSkillSet</code>","text":"<p>               Bases: <code>SkillSet</code></p> <p>Represents a sequence of skills that are acquired in a specific order to achieve a goal.</p> <p>LinearSkillSet ensures that skills are applied in a sequential manner.</p> <p>Attributes:</p> Name Type Description <code>skills</code> <code>Union[List[Skill], Dict[str, Skill]]</code> <p>Provided skills</p> <code>skill_sequence</code> <code>List[str]</code> <p>Ordered list of skill names indicating the order                                   in which they should be acquired.</p> <p>Examples:</p> <pre><code>Create a LinearSkillSet with a list of skills specified as BaseSkill instances:\n&gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill, AnalysisSkill, ClassificationSkill\n&gt;&gt;&gt; skillset = LinearSkillSet(skills=[TransformSkill(), ClassificationSkill(), AnalysisSkill()])\n</code></pre> Source code in <code>adala/skills/skillset.py</code> <pre><code>class LinearSkillSet(SkillSet):\n    \"\"\"\n    Represents a sequence of skills that are acquired in a specific order to achieve a goal.\n\n    LinearSkillSet ensures that skills are applied in a sequential manner.\n\n    Attributes:\n        skills (Union[List[Skill], Dict[str, Skill]]): Provided skills\n        skill_sequence (List[str], optional): Ordered list of skill names indicating the order\n                                              in which they should be acquired.\n\n    Examples:\n\n        Create a LinearSkillSet with a list of skills specified as BaseSkill instances:\n        &gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill, AnalysisSkill, ClassificationSkill\n        &gt;&gt;&gt; skillset = LinearSkillSet(skills=[TransformSkill(), ClassificationSkill(), AnalysisSkill()])\n    \"\"\"\n\n    skill_sequence: List[str] = None\n\n    @model_validator(mode=\"after\")\n    def skill_sequence_validator(self) -&gt; \"LinearSkillSet\":\n        \"\"\"\n        Validates and sets the default order for the skill sequence if not provided.\n\n        Returns:\n            LinearSkillSet: The current instance with updated skill_sequence attribute.\n        \"\"\"\n        if self.skill_sequence is None:\n            # use default skill sequence defined by lexicographical order\n            self.skill_sequence = list(self.skills.keys())\n        if len(self.skill_sequence) != len(self.skills):\n            raise ValueError(\n                f\"skill_sequence must contain all skill names - \"\n                f\"length of skill_sequence is {len(self.skill_sequence)} \"\n                f\"while length of skills is {len(self.skills)}\"\n            )\n        return self\n\n    def apply(\n        self,\n        input: Union[Record, InternalDataFrame],\n        runtime: Runtime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Sequentially applies each skill on the dataset.\n\n        Args:\n            input (InternalDataFrame): Input dataset.\n            runtime (Runtime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n        Returns:\n            InternalDataFrame: Skill predictions.\n        \"\"\"\n        if improved_skill:\n            # start from the specified skill, assuming previous skills have already been applied\n            skill_sequence = self.skill_sequence[\n                self.skill_sequence.index(improved_skill) :\n            ]\n        else:\n            skill_sequence = self.skill_sequence\n        skill_input = input\n        for i, skill_name in enumerate(skill_sequence):\n            skill = self.skills[skill_name]\n            # use input dataset for the first node in the pipeline\n            logger.info(\"Applying skill: %s\", skill_name)\n            skill_output = skill.apply(skill_input, runtime)\n\n            # Commented out to not log customer data. Can be used when debugging if needed\n            # print_dataframe(skill_output)\n\n            if isinstance(skill, TransformSkill):\n                # Columns to drop from skill_input because they are also in skill_output\n                cols_to_drop = set(skill_output.columns) &amp; set(skill_input.columns)\n                skill_input_reduced = skill_input.drop(columns=cols_to_drop)\n\n                skill_input = skill_input_reduced.merge(\n                    skill_output, left_index=True, right_index=True, how=\"inner\"\n                )\n            elif isinstance(skill, (AnalysisSkill, SynthesisSkill)):\n                skill_input = skill_output\n            else:\n                raise ValueError(f\"Unsupported skill type: {type(skill)}\")\n        if isinstance(skill_input, InternalSeries):\n            skill_input = skill_input.to_frame().T\n        return skill_input\n\n    async def aapply(\n        self,\n        input: Union[Record, InternalDataFrame],\n        runtime: AsyncRuntime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Sequentially and asynchronously applies each skill on the dataset.\n\n        Args:\n            input (InternalDataFrame): Input dataset.\n            runtime (AsyncRuntime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n        Returns:\n            InternalDataFrame: Skill predictions.\n        \"\"\"\n        if improved_skill:\n            # start from the specified skill, assuming previous skills have already been applied\n            skill_sequence = self.skill_sequence[\n                self.skill_sequence.index(improved_skill) :\n            ]\n        else:\n            skill_sequence = self.skill_sequence\n        skill_input = input\n        for i, skill_name in enumerate(skill_sequence):\n            skill = self.skills[skill_name]\n            # use input dataset for the first node in the pipeline\n            logger.info(\"Applying skill: %s\", skill_name)\n            skill_output = await skill.aapply(skill_input, runtime)\n\n            # Commented out to not log customer data. Can be used when debugging if needed\n            # print_dataframe(skill_output)\n\n            if isinstance(skill, TransformSkill):\n                # Columns to drop from skill_input because they are also in skill_output\n                cols_to_drop = set(skill_output.columns) &amp; set(skill_input.columns)\n                skill_input_reduced = skill_input.drop(columns=cols_to_drop)\n\n                skill_input = skill_input_reduced.merge(\n                    skill_output, left_index=True, right_index=True, how=\"inner\"\n                )\n            elif isinstance(skill, (AnalysisSkill, SynthesisSkill)):\n                skill_input = skill_output\n            else:\n                raise ValueError(f\"Unsupported skill type: {type(skill)}\")\n        if isinstance(skill_input, InternalSeries):\n            skill_input = skill_input.to_frame().T\n        return skill_input\n\n    def __rich__(self):\n        \"\"\"Returns a rich representation of the skill.\"\"\"\n        # TODO: move it to a base class and use repr derived from Skills\n        text = f\"[bold blue]Total Agent Skills: {len(self.skills)}[/bold blue]\\n\\n\"\n        for skill in self.skills.values():\n            text += (\n                f\"[bold underline green]{skill.name}[/bold underline green]\\n\"\n                f\"[green]{skill.instructions}[green]\\n\"\n            )\n        return text\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.__rich__","title":"<code>__rich__()</code>","text":"<p>Returns a rich representation of the skill.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def __rich__(self):\n    \"\"\"Returns a rich representation of the skill.\"\"\"\n    # TODO: move it to a base class and use repr derived from Skills\n    text = f\"[bold blue]Total Agent Skills: {len(self.skills)}[/bold blue]\\n\\n\"\n    for skill in self.skills.values():\n        text += (\n            f\"[bold underline green]{skill.name}[/bold underline green]\\n\"\n            f\"[green]{skill.instructions}[green]\\n\"\n        )\n    return text\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.aapply","title":"<code>aapply(input, runtime, improved_skill=None)</code>  <code>async</code>","text":"<p>Sequentially and asynchronously applies each skill on the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>Input dataset.</p> required <code>runtime</code> <code>AsyncRuntime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Name of the skill to improve. Defaults to None.</p> <code>None</code> <p>Returns:     InternalDataFrame: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>async def aapply(\n    self,\n    input: Union[Record, InternalDataFrame],\n    runtime: AsyncRuntime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Sequentially and asynchronously applies each skill on the dataset.\n\n    Args:\n        input (InternalDataFrame): Input dataset.\n        runtime (AsyncRuntime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n    Returns:\n        InternalDataFrame: Skill predictions.\n    \"\"\"\n    if improved_skill:\n        # start from the specified skill, assuming previous skills have already been applied\n        skill_sequence = self.skill_sequence[\n            self.skill_sequence.index(improved_skill) :\n        ]\n    else:\n        skill_sequence = self.skill_sequence\n    skill_input = input\n    for i, skill_name in enumerate(skill_sequence):\n        skill = self.skills[skill_name]\n        # use input dataset for the first node in the pipeline\n        logger.info(\"Applying skill: %s\", skill_name)\n        skill_output = await skill.aapply(skill_input, runtime)\n\n        # Commented out to not log customer data. Can be used when debugging if needed\n        # print_dataframe(skill_output)\n\n        if isinstance(skill, TransformSkill):\n            # Columns to drop from skill_input because they are also in skill_output\n            cols_to_drop = set(skill_output.columns) &amp; set(skill_input.columns)\n            skill_input_reduced = skill_input.drop(columns=cols_to_drop)\n\n            skill_input = skill_input_reduced.merge(\n                skill_output, left_index=True, right_index=True, how=\"inner\"\n            )\n        elif isinstance(skill, (AnalysisSkill, SynthesisSkill)):\n            skill_input = skill_output\n        else:\n            raise ValueError(f\"Unsupported skill type: {type(skill)}\")\n    if isinstance(skill_input, InternalSeries):\n        skill_input = skill_input.to_frame().T\n    return skill_input\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.apply","title":"<code>apply(input, runtime, improved_skill=None)</code>","text":"<p>Sequentially applies each skill on the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>Input dataset.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Name of the skill to improve. Defaults to None.</p> <code>None</code> <p>Returns:     InternalDataFrame: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def apply(\n    self,\n    input: Union[Record, InternalDataFrame],\n    runtime: Runtime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Sequentially applies each skill on the dataset.\n\n    Args:\n        input (InternalDataFrame): Input dataset.\n        runtime (Runtime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n    Returns:\n        InternalDataFrame: Skill predictions.\n    \"\"\"\n    if improved_skill:\n        # start from the specified skill, assuming previous skills have already been applied\n        skill_sequence = self.skill_sequence[\n            self.skill_sequence.index(improved_skill) :\n        ]\n    else:\n        skill_sequence = self.skill_sequence\n    skill_input = input\n    for i, skill_name in enumerate(skill_sequence):\n        skill = self.skills[skill_name]\n        # use input dataset for the first node in the pipeline\n        logger.info(\"Applying skill: %s\", skill_name)\n        skill_output = skill.apply(skill_input, runtime)\n\n        # Commented out to not log customer data. Can be used when debugging if needed\n        # print_dataframe(skill_output)\n\n        if isinstance(skill, TransformSkill):\n            # Columns to drop from skill_input because they are also in skill_output\n            cols_to_drop = set(skill_output.columns) &amp; set(skill_input.columns)\n            skill_input_reduced = skill_input.drop(columns=cols_to_drop)\n\n            skill_input = skill_input_reduced.merge(\n                skill_output, left_index=True, right_index=True, how=\"inner\"\n            )\n        elif isinstance(skill, (AnalysisSkill, SynthesisSkill)):\n            skill_input = skill_output\n        else:\n            raise ValueError(f\"Unsupported skill type: {type(skill)}\")\n    if isinstance(skill_input, InternalSeries):\n        skill_input = skill_input.to_frame().T\n    return skill_input\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.skill_sequence_validator","title":"<code>skill_sequence_validator()</code>","text":"<p>Validates and sets the default order for the skill sequence if not provided.</p> <p>Returns:</p> Name Type Description <code>LinearSkillSet</code> <code>LinearSkillSet</code> <p>The current instance with updated skill_sequence attribute.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@model_validator(mode=\"after\")\ndef skill_sequence_validator(self) -&gt; \"LinearSkillSet\":\n    \"\"\"\n    Validates and sets the default order for the skill sequence if not provided.\n\n    Returns:\n        LinearSkillSet: The current instance with updated skill_sequence attribute.\n    \"\"\"\n    if self.skill_sequence is None:\n        # use default skill sequence defined by lexicographical order\n        self.skill_sequence = list(self.skills.keys())\n    if len(self.skill_sequence) != len(self.skills):\n        raise ValueError(\n            f\"skill_sequence must contain all skill names - \"\n            f\"length of skill_sequence is {len(self.skill_sequence)} \"\n            f\"while length of skills is {len(self.skills)}\"\n        )\n    return self\n</code></pre>"},{"location":"skills/#adala.skills.skillset.ParallelSkillSet","title":"<code>ParallelSkillSet</code>","text":"<p>               Bases: <code>SkillSet</code></p> <p>Represents a set of skills that are acquired simultaneously to reach a goal.</p> <p>In a ParallelSkillSet, each skill can be developed independently of the others. This is useful for agents that require multiple, diverse capabilities, or tasks where each skill contributes a piece of the overall solution.</p> <p>Examples:</p> <p>Create a ParallelSkillSet with a list of skills specified as BaseSkill instances</p> <pre><code>&gt;&gt;&gt; from adala.skills import ParallelSkillSet, ClassificationSkill, TransformSkill\n&gt;&gt;&gt; skillset = ParallelSkillSet(skills=[ClassificationSkill(), TransformSkill()])\n</code></pre> Source code in <code>adala/skills/skillset.py</code> <pre><code>class ParallelSkillSet(SkillSet):\n    \"\"\"\n    Represents a set of skills that are acquired simultaneously to reach a goal.\n\n    In a ParallelSkillSet, each skill can be developed independently of the others. This is useful\n    for agents that require multiple, diverse capabilities, or tasks where each skill contributes a piece of\n    the overall solution.\n\n    Examples:\n        Create a ParallelSkillSet with a list of skills specified as BaseSkill instances\n        &gt;&gt;&gt; from adala.skills import ParallelSkillSet, ClassificationSkill, TransformSkill\n        &gt;&gt;&gt; skillset = ParallelSkillSet(skills=[ClassificationSkill(), TransformSkill()])\n    \"\"\"\n\n    def apply(\n        self,\n        input: Union[InternalSeries, InternalDataFrame],\n        runtime: Runtime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies each skill on the dataset, enhancing the agent's experience.\n\n        Args:\n            input (Union[Record, InternalDataFrame]): Input data\n            runtime (Runtime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Unused in ParallelSkillSet. Defaults to None.\n        Returns:\n            Union[Record, InternalDataFrame]: Skill predictions.\n        \"\"\"\n        if improved_skill:\n            # start from the specified skill, assuming previous skills have already been applied\n            skill_sequence = [improved_skill]\n        else:\n            skill_sequence = list(self.skills.keys())\n\n        skill_outputs = []\n        for i, skill_name in enumerate(skill_sequence):\n            skill = self.skills[skill_name]\n            # use input dataset for the first node in the pipeline\n            logger.info(\"Applying skill: %s\", skill_name)\n            skill_output = skill.apply(input, runtime)\n            skill_outputs.append(skill_output)\n        if not skill_outputs:\n            return InternalDataFrame()\n        else:\n            if isinstance(skill_outputs[0], InternalDataFrame):\n                skill_outputs = InternalDataFrameConcat(skill_outputs, axis=1)\n                cols_to_drop = set(input.columns) &amp; set(skill_outputs.columns)\n                skill_input_reduced = input.drop(columns=cols_to_drop)\n\n                return skill_input_reduced.merge(\n                    skill_outputs, left_index=True, right_index=True, how=\"inner\"\n                )\n            elif isinstance(skill_outputs[0], (dict, InternalSeries)):\n                # concatenate output to each row of input\n                output = skill_outputs[0]\n                return InternalDataFrameConcat(\n                    [\n                        input,\n                        InternalDataFrame(\n                            [output] * len(input),\n                            columns=output.index,\n                            index=input.index,\n                        ),\n                    ],\n                    axis=1,\n                )\n            else:\n                raise ValueError(f\"Unsupported output type: {type(skill_outputs[0])}\")\n</code></pre>"},{"location":"skills/#adala.skills.skillset.ParallelSkillSet.apply","title":"<code>apply(input, runtime, improved_skill=None)</code>","text":"<p>Applies each skill on the dataset, enhancing the agent's experience.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[Record, InternalDataFrame]</code> <p>Input data</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Unused in ParallelSkillSet. Defaults to None.</p> <code>None</code> <p>Returns:     Union[Record, InternalDataFrame]: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def apply(\n    self,\n    input: Union[InternalSeries, InternalDataFrame],\n    runtime: Runtime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies each skill on the dataset, enhancing the agent's experience.\n\n    Args:\n        input (Union[Record, InternalDataFrame]): Input data\n        runtime (Runtime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Unused in ParallelSkillSet. Defaults to None.\n    Returns:\n        Union[Record, InternalDataFrame]: Skill predictions.\n    \"\"\"\n    if improved_skill:\n        # start from the specified skill, assuming previous skills have already been applied\n        skill_sequence = [improved_skill]\n    else:\n        skill_sequence = list(self.skills.keys())\n\n    skill_outputs = []\n    for i, skill_name in enumerate(skill_sequence):\n        skill = self.skills[skill_name]\n        # use input dataset for the first node in the pipeline\n        logger.info(\"Applying skill: %s\", skill_name)\n        skill_output = skill.apply(input, runtime)\n        skill_outputs.append(skill_output)\n    if not skill_outputs:\n        return InternalDataFrame()\n    else:\n        if isinstance(skill_outputs[0], InternalDataFrame):\n            skill_outputs = InternalDataFrameConcat(skill_outputs, axis=1)\n            cols_to_drop = set(input.columns) &amp; set(skill_outputs.columns)\n            skill_input_reduced = input.drop(columns=cols_to_drop)\n\n            return skill_input_reduced.merge(\n                skill_outputs, left_index=True, right_index=True, how=\"inner\"\n            )\n        elif isinstance(skill_outputs[0], (dict, InternalSeries)):\n            # concatenate output to each row of input\n            output = skill_outputs[0]\n            return InternalDataFrameConcat(\n                [\n                    input,\n                    InternalDataFrame(\n                        [output] * len(input),\n                        columns=output.index,\n                        index=input.index,\n                    ),\n                ],\n                axis=1,\n            )\n        else:\n            raise ValueError(f\"Unsupported output type: {type(skill_outputs[0])}\")\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet","title":"<code>SkillSet</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Represents a collection of interdependent skills aiming to achieve a specific goal.</p> <p>A skill set breaks down the path to achieve a goal into necessary precursor skills. Agents can evolve these skills either in parallel for tasks like self-consistency or sequentially for complex problem decompositions and causal reasoning. In the most generic cases, task decomposition can involve a graph-based approach.</p> <p>Attributes:</p> Name Type Description <code>skills</code> <code>Dict[str, Skill]</code> <p>A dictionary of skills in the skill set.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>class SkillSet(BaseModel, ABC):\n    \"\"\"\n    Represents a collection of interdependent skills aiming to achieve a specific goal.\n\n    A skill set breaks down the path to achieve a goal into necessary precursor skills.\n    Agents can evolve these skills either in parallel for tasks like self-consistency or\n    sequentially for complex problem decompositions and causal reasoning. In the most generic\n    cases, task decomposition can involve a graph-based approach.\n\n    Attributes:\n        skills (Dict[str, Skill]): A dictionary of skills in the skill set.\n    \"\"\"\n\n    skills: Union[List, Dict[str, Skill]]\n\n    @field_validator(\"skills\", mode=\"before\")\n    def skills_validator(cls, v: Union[List, Dict]) -&gt; Dict[str, Skill]:\n        \"\"\"\n        Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.\n\n        Args:\n            v (Union[List[Skill], Dict[str, Skill]]): The skills attribute to validate and convert.\n\n        Returns:\n            Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.\n        \"\"\"\n        skills = OrderedDict()\n        if not v:\n            return skills\n\n        elif isinstance(v, list):\n            if isinstance(v[0], Skill):\n                # convert list of skill names to dictionary\n                for skill in v:\n                    skills[skill.name] = skill\n            elif isinstance(v[0], dict):\n                # convert list of skill dictionaries to dictionary\n                for skill in v:\n                    if \"type\" not in skill:\n                        raise ValueError(\"Skill dictionary must contain a 'type' key\")\n                    skills[skill[\"name\"]] = Skill.create_from_registry(\n                        skill.pop(\"type\"), **skill\n                    )\n        elif isinstance(v, dict):\n            skills = v\n        else:\n            raise ValueError(\n                f\"skills must be a list or dictionary, but received type {type(v)}\"\n            )\n        return skills\n\n    @abstractmethod\n    def apply(\n        self,\n        input: Union[Record, InternalDataFrame],\n        runtime: Runtime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Apply the skill set to a dataset using a specified runtime.\n\n        Args:\n            input (Union[Record, InternalDataFrame]): Input data to apply the skill set to.\n            runtime (Runtime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Name of the skill to start from (to optimize calculations). Defaults to None.\n        Returns:\n            InternalDataFrame: Skill predictions.\n        \"\"\"\n\n    def __getitem__(self, skill_name) -&gt; Skill:\n        \"\"\"\n        Select skill by name.\n\n        Args:\n            skill_name (str): Name of the skill to select.\n\n        Returns:\n            BaseSkill: Skill\n        \"\"\"\n        return self.skills[skill_name]\n\n    def __setitem__(self, skill_name, skill: Skill):\n        \"\"\"\n        Set skill by name.\n\n        Args:\n            skill_name (str): Name of the skill to set.\n            skill (BaseSkill): Skill to set.\n        \"\"\"\n        self.skills[skill_name] = skill\n\n    def get_skill_names(self) -&gt; List[str]:\n        \"\"\"\n        Get list of skill names.\n\n        Returns:\n            List[str]: List of skill names.\n        \"\"\"\n        return list(self.skills.keys())\n\n    def get_skill_outputs(self) -&gt; Dict[str, str]:\n        \"\"\"\n        Get dictionary of skill outputs.\n\n        Returns:\n            Dict[str, str]: Dictionary of skill outputs. Keys are output names and values are skill names\n        \"\"\"\n        return {\n            field: skill.name\n            for skill in self.skills.values()\n            for field in skill.get_output_fields()\n        }\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.__getitem__","title":"<code>__getitem__(skill_name)</code>","text":"<p>Select skill by name.</p> <p>Parameters:</p> Name Type Description Default <code>skill_name</code> <code>str</code> <p>Name of the skill to select.</p> required <p>Returns:</p> Name Type Description <code>BaseSkill</code> <code>Skill</code> <p>Skill</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def __getitem__(self, skill_name) -&gt; Skill:\n    \"\"\"\n    Select skill by name.\n\n    Args:\n        skill_name (str): Name of the skill to select.\n\n    Returns:\n        BaseSkill: Skill\n    \"\"\"\n    return self.skills[skill_name]\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.__setitem__","title":"<code>__setitem__(skill_name, skill)</code>","text":"<p>Set skill by name.</p> <p>Parameters:</p> Name Type Description Default <code>skill_name</code> <code>str</code> <p>Name of the skill to set.</p> required <code>skill</code> <code>BaseSkill</code> <p>Skill to set.</p> required Source code in <code>adala/skills/skillset.py</code> <pre><code>def __setitem__(self, skill_name, skill: Skill):\n    \"\"\"\n    Set skill by name.\n\n    Args:\n        skill_name (str): Name of the skill to set.\n        skill (BaseSkill): Skill to set.\n    \"\"\"\n    self.skills[skill_name] = skill\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.apply","title":"<code>apply(input, runtime, improved_skill=None)</code>  <code>abstractmethod</code>","text":"<p>Apply the skill set to a dataset using a specified runtime.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[Record, InternalDataFrame]</code> <p>Input data to apply the skill set to.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Name of the skill to start from (to optimize calculations). Defaults to None.</p> <code>None</code> <p>Returns:     InternalDataFrame: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@abstractmethod\ndef apply(\n    self,\n    input: Union[Record, InternalDataFrame],\n    runtime: Runtime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Apply the skill set to a dataset using a specified runtime.\n\n    Args:\n        input (Union[Record, InternalDataFrame]): Input data to apply the skill set to.\n        runtime (Runtime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Name of the skill to start from (to optimize calculations). Defaults to None.\n    Returns:\n        InternalDataFrame: Skill predictions.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.get_skill_names","title":"<code>get_skill_names()</code>","text":"<p>Get list of skill names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of skill names.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def get_skill_names(self) -&gt; List[str]:\n    \"\"\"\n    Get list of skill names.\n\n    Returns:\n        List[str]: List of skill names.\n    \"\"\"\n    return list(self.skills.keys())\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.get_skill_outputs","title":"<code>get_skill_outputs()</code>","text":"<p>Get dictionary of skill outputs.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Dictionary of skill outputs. Keys are output names and values are skill names</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def get_skill_outputs(self) -&gt; Dict[str, str]:\n    \"\"\"\n    Get dictionary of skill outputs.\n\n    Returns:\n        Dict[str, str]: Dictionary of skill outputs. Keys are output names and values are skill names\n    \"\"\"\n    return {\n        field: skill.name\n        for skill in self.skills.values()\n        for field in skill.get_output_fields()\n    }\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.skills_validator","title":"<code>skills_validator(v)</code>","text":"<p>Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Union[List[Skill], Dict[str, Skill]]</code> <p>The skills attribute to validate and convert.</p> required <p>Returns:</p> Type Description <code>Dict[str, Skill]</code> <p>Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@field_validator(\"skills\", mode=\"before\")\ndef skills_validator(cls, v: Union[List, Dict]) -&gt; Dict[str, Skill]:\n    \"\"\"\n    Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.\n\n    Args:\n        v (Union[List[Skill], Dict[str, Skill]]): The skills attribute to validate and convert.\n\n    Returns:\n        Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.\n    \"\"\"\n    skills = OrderedDict()\n    if not v:\n        return skills\n\n    elif isinstance(v, list):\n        if isinstance(v[0], Skill):\n            # convert list of skill names to dictionary\n            for skill in v:\n                skills[skill.name] = skill\n        elif isinstance(v[0], dict):\n            # convert list of skill dictionaries to dictionary\n            for skill in v:\n                if \"type\" not in skill:\n                    raise ValueError(\"Skill dictionary must contain a 'type' key\")\n                skills[skill[\"name\"]] = Skill.create_from_registry(\n                    skill.pop(\"type\"), **skill\n                )\n    elif isinstance(v, dict):\n        skills = v\n    else:\n        raise ValueError(\n            f\"skills must be a list or dictionary, but received type {type(v)}\"\n        )\n    return skills\n</code></pre>"},{"location":"utils/","title":"Utils","text":""},{"location":"utils/#adala.utils.internal_data.InternalDataFrameConcat","title":"<code>InternalDataFrameConcat(dfs, **kwargs)</code>","text":"<p>Concatenate dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>Iterable[InternalDataFrame]</code> <p>The dataframes to concatenate.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The concatenated dataframe.</p> Source code in <code>adala/utils/internal_data.py</code> <pre><code>def InternalDataFrameConcat(\n    dfs: Iterable[InternalDataFrame], **kwargs\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Concatenate dataframes.\n\n    Args:\n        dfs (Iterable[InternalDataFrame]): The dataframes to concatenate.\n\n    Returns:\n        InternalDataFrame: The concatenated dataframe.\n    \"\"\"\n    return pd.concat(dfs, **kwargs)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_dataframe","title":"<code>print_dataframe(dataframe)</code>","text":"<p>Print dataframe to console.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_dataframe(dataframe: InternalDataFrame):\n    \"\"\"\n    Print dataframe to console.\n    \"\"\"\n    num_rows = 5\n    table = Table(show_header=True, header_style=\"bold magenta\")\n    # index_name = dataframe.index.name or 'index'\n    # table.add_column(index_name)\n\n    for column in dataframe.columns:\n        table.add_column(str(column))\n\n    for index, value_list in enumerate(dataframe.iloc[:num_rows].values.tolist()):\n        # row = [str(index)]\n        row = []\n        row += [str(x) for x in value_list]\n        table.add_row(*row)\n\n    # Update the style of the table\n    table.row_styles = [\"none\", \"dim\"]\n    table.box = box.SIMPLE_HEAD\n\n    console.print(table)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_error","title":"<code>print_error(text)</code>","text":"<p>Print error message to console.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_error(text: str):\n    \"\"\"\n    Print error message to console.\n    \"\"\"\n    error_console.print(text)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_series","title":"<code>print_series(data)</code>","text":"<p>Print series to console.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_series(data: InternalSeries):\n    \"\"\"\n    Print series to console.\n    \"\"\"\n\n    # Create a Rich Table with a column for each series value\n    table = Table(show_header=True, header_style=\"bold magenta\")\n\n    # Add a column for each value in the series with the index as the header\n    for index in data.index:\n        table.add_column(str(index))\n\n    # Add a single row with all the values from the series\n    table.add_row(*[str(value) for value in data])\n\n    # Print the table with the Rich console\n    console.print(table)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_text","title":"<code>print_text(text, style=None, streaming_style=False)</code>","text":"<p>Print text to console with optional style and streaming style.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_text(text: str, style=None, streaming_style=False):\n    \"\"\"\n    Print text to console with optional style and streaming style.\n    \"\"\"\n    if streaming_style:\n        for char in text:\n            console.print(char, sep=\"\", end=\"\", style=style)\n            time.sleep(0.01)\n        console.print()\n    else:\n        console.print(text, style=style)\n</code></pre>"},{"location":"utils/#adala.utils.matching.fuzzy_match","title":"<code>fuzzy_match(x, y, threshold=0.8)</code>","text":"<p>Fuzzy match string values in two series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InternalSeries</code> <p>The first series.</p> required <code>y</code> <code>InternalSeries</code> <p>The second series.</p> required <code>threshold</code> <code>float</code> <p>The threshold to use for fuzzy matching. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Name Type Description <code>InternalSeries</code> <code>InternalSeries</code> <p>The series with fuzzy match results.</p> Source code in <code>adala/utils/matching.py</code> <pre><code>def fuzzy_match(x: InternalSeries, y: InternalSeries, threshold=0.8) -&gt; InternalSeries:\n    \"\"\"\n    Fuzzy match string values in two series.\n\n    Args:\n        x (InternalSeries): The first series.\n        y (InternalSeries): The second series.\n        threshold (float): The threshold to use for fuzzy matching. Defaults to 0.8.\n\n    Returns:\n        InternalSeries: The series with fuzzy match results.\n    \"\"\"\n    result = x.combine(y, lambda x, y: _fuzzy_match(x, y, threshold))\n    return result\n</code></pre>"}]}