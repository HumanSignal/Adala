interactions:
- request:
    body: '{"contents": [{"role": "user", "parts": [{"text": "Hello, how are you?"}]}],
      "generationConfig": {}}'
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '100'
      content-type:
      - application/json
      host:
      - generativelanguage.googleapis.com
      user-agent:
      - litellm/1.65.6
    method: POST
    uri: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAC/61SwW7TQBC9+yuGvXBJIpOmScsFVRSpQVSNIEIgQGgaj+1V1jvW7prWRPl3Zp06
        deCKLa1WM2/mvZm3uwRAbdBmOsNAXr2GbxIB2HVnzLENZIMk+pAEa3ThGXv4doO7QAI9xiK1BKwg
        Y20LeCBjRhBKtFtouYGcHaDfSuoFXHlAMOgKktMWDcql4oykYCnl9mUAeqzJabIbAqo4aLYepENO
        ZKSFB6O3BGVTocQzHsF9E6Bjzxu7ifCogeugKzSmBZkZHGHWQmCR4bUPUdUEbvgBZCNSW5KpO6WB
        M2zffLdqMOP+eP8xet6MY0Nx7E57D9/3AJVrq335kdCzjbBP67uVOmbxV/GBi9rxfVzuOJ2ks4X8
        6dlsenGZXs6n83nSU3ekqvGyqFsKKO7h0SMlLao6rHlL9i03nXvzA8vA65P0bPGUDxzQnKTOz0b/
        dPXXwqnN8A0MnocMj0aHNk64fvdlrQYLCiei+gUlgz3+LfE/cc0Wp2TJky8Hqz6T8/rgSUGVuDSe
        TtJxbtCXXUflyNfy4miZdZivzStc0c+b8996cXuxcnfX75srVsk++QNPjAV+UgMAAA==
    headers:
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json; charset=UTF-8
      Date:
      - Mon, 01 Sep 2025 12:03:14 GMT
      Server:
      - scaffolding on HTTPServer2
      Server-Timing:
      - gfet4t7; dur=812
      Transfer-Encoding:
      - chunked
      Vary:
      - Origin
      - X-Origin
      - Referer
      X-Content-Type-Options:
      - nosniff
      X-Frame-Options:
      - SAMEORIGIN
      X-XSS-Protection:
      - '0'
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "user", "content": "Hello, how are you?"}], "model":
      "gemini/gemini-2.0-flash"}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '102'
      content-type:
      - application/json
      host:
      - localhost:30001
      user-agent:
      - AsyncOpenAI/Python 1.76.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - async:asyncio
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.76.0
      x-stainless-read-timeout:
      - '600'
      x-stainless-retry-count:
      - '0'
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.12
    method: POST
    uri: http://localhost:30001/chat/completions
  response:
    body:
      string: '{"id":"chatcmpl-d6480e6a-098d-4bcf-9eee-c575ff43b37e","object":"chat.completion","created":1756728193,"model":"gemini-2.0-flash","choices":[{"finish_reason":"stop","index":0,"message":{"content":"I
        am doing well, thank you for asking! As a large language model, I don''t experience
        emotions or feelings like humans do, but I am functioning optimally and ready
        to assist you. How can I help you today?\n","role":"assistant","tool_calls":null,"function_call":null},"logprobs":-0.047474034289096266}],"usage":{"completion_tokens":47,"prompt_tokens":6,"total_tokens":53,"completion_tokens_details":null,"prompt_tokens_details":{"audio_tokens":null,"cached_tokens":null}},"service_tier":"default"}'
    headers:
      content-length:
      - '691'
      content-type:
      - application/json
    status:
      code: 200
      message: OK
version: 1
