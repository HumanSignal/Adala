interactions:
- request:
    body: '{"messages": [{"role": "user", "content": "Hey, how''s it going?"}], "model":
      "gpt-4-turbo-preview", "max_tokens": 10}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '117'
      content-type:
      - application/json
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SQzW7CMBCE73mK7V56IRVBQEmeoKiHSlXVVqoqZMySGPwneykgxLtXTgK0Fx9m
        PONvfMoAUK2wApSNYGm8zku9nQlfvG2eZ0uxLt+HgT4Or+XhZXLcf+IgJdxyQ5IvqQfpjNfEytnO
        loEEU2otHkdFMZ08ltPWMG5FOsVqz/k4HxajSe4D/Sja98nGKUkRK/jKAABO7ZkY7YoOWMFwcFEM
        xShqwup6CQCD00lBEaOKLCzj4GZKZ5lsiz2/N9BQIGAHDWl/B09uD1JYmEOXxT53vj6oXe2DWyY4
        u9P6qq+VVbFZBBLR2VSuydbcdAXnDOC7nbb7R4s+OON5wW5LNlUW464Qb5/5x+xnIzsW+qaPxllP
        iPEYmcxirWxNwQfV7kyc2Tn7BQAA//8DANqNJLHmAQAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a4520f5de811031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:36:37 GMT
      Server:
      - cloudflare
      Set-Cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        path=/; expires=Tue, 16-Jul-24 22:06:37 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '397'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '500'
      x-ratelimit-limit-tokens:
      - '30000'
      x-ratelimit-remaining-requests:
      - '499'
      x-ratelimit-remaining-tokens:
      - '29984'
      x-ratelimit-reset-requests:
      - 120ms
      x-ratelimit-reset-tokens:
      - 32ms
      x-request-id:
      - req_a992346f2005624ada5d6049a6e38eb8
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "user", "content": "Hey, how''s it going?"}], "model":
      "gpt-3.5-turbo", "max_tokens": 10}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '111'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SQMW/CMBSE9/yKVy9dEkQoNCEzqtqhY4eqqpATHonB9rPsF9EW8d8rhwDt4uHO
        d/7OxwRAqI2oQDSd5MY4nS31vqzNe3nAVfdTPm2Lt65fmXJ6MPb1INKYoHqHDV9Sk4aM08iK7Nlu
        PErG2JoXszx/XBTLYjAMbVDHWOs4e5gsMu59Tdk0ny3GZEeqwSAq+EgAAI7DGRntBr9EBdP0ohgM
        QbYoquslAOFJR0XIEFRgaVmkN7Mhy2gH7GfUmu7g5d7Arg8MEuKGntGD89R6aVIIJMbs6fqoptZ5
        qiOg7bW+6ltlVejWHmUgGx/QaFvuzgWnBOBzmNf/IxbOk3G8ZtqjjZX5/Fwobh/6xxynCyaW+qbP
        5slIKMJ3YDTrrbIteufVsDVyJqfkFwAA//8DAA9Yq+rqAQAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a4520fa9e281031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:36:38 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '427'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9998'
      x-ratelimit-remaining-tokens:
      - '199983'
      x-ratelimit-reset-requests:
      - 13.141s
      x-ratelimit-reset-tokens:
      - 4ms
      x-request-id:
      - req_4609a33177545c6c05bdf84e2b9159d9
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content":
      "Text: 0 5 0\nOutput: "}], "model": "gpt-3.5-turbo", "max_tokens": 1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '152'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SRQWvjMBCF7/4Vg85NiJO6TXMppQvLnnLZhe1uS1Dkia1W0ghpFBJK/nuR7CT0
        Ysy8+R5vnj4rAKFbsQKhesnKejN5MB9L9b7+E/7an/WP/b+wXiZePLn1S3PL4iYTtH1HxWdqqsh6
        g6zJDbIKKBmza30/r+u75v5hWQRLLZqMdZ4ni2kz4RS2NJnV82Yke9IKo1jB/woA4LN8c0bX4kGs
        YHZznliMUXYoVpclABHI5ImQMerI0g15R1GRY3Ql9u8egRL7xLCjANwjMB4YXsUMGpi9ClCUTAtb
        BOmO3GvXQYseXZv/yBWi+B14Cs9l90gJvEEZEXygvW4RLAUE7XYUrMz1AIUzBZHgFyjpoNN7LLAc
        9qVSKUhGCBg9uYiPYrzhdDneUOcDbXNRLhlzme+007HfBJSRXD40MvkBP1UAb6Xk9K034QNZzxum
        D3TZcH472Inrs17FxXIUmVia6/xuXo35RDxGRrvZaddh8EGXxnPK6lR9AQAA//8DAK4zP3xwAgAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a4520ff1ead1031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:36:38 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '450'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9997'
      x-ratelimit-remaining-tokens:
      - '198992'
      x-ratelimit-reset-requests:
      - 21.08s
      x-ratelimit-reset-tokens:
      - 302ms
      x-request-id:
      - req_f82f19efdff44dda1b9d2968767beb6b
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content":
      "Text: 0 0 0\nOutput: "}], "model": "gpt-3.5-turbo", "max_tokens": 1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '152'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1RRwW4aMRS871eMfOkF0EIgJFxyaKS2qqpWjdoemgiZ3be7Ll4/y34bQBH/HnmB
        Rb34MPNmPG/eWwYoU6oVVNFoKVpvx/d2e1d8+1X9vv30ffn0+PXHn/rx8/7n042ed40aJQVv/lEh
        F9Wk4NZbEsPuRBeBtFBynS5n0+ntYnl/1xMtl2STrPYyvpksxtKFDY/z6WxxVjZsCopqhb8ZALz1
        b8roStqrFfLRBWkpRl2TWg1DgApsE6J0jCaKdqJGV7JgJ+T62F8EkaiNsGZLOHD34ZXgA7+akkpo
        RBJwBde1GwoRzypHjvxZYdMJSlPCsSB6Kkx1wK7RAu7Ed5KsoAPBMm+Nq1FxmOAjd7bsKW9Jx+En
        tBwIfay9gAMKq8PgmOZ3vbIP2RKEUTJ2RhpIQ5Eu+R7Uecnj0I7l2gfepCZdZ+2AV8aZ2KwD6cgu
        NRGF/Ul+zICX/grdf8UqH7j1shbekkuGs/nJTl3vfiXnyzMpLNpe8eU0O+dT8RCF2nVlXE3BB9Of
        JKXMjtk7AAAA//8DACvTm6yRAgAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a452102bc5d1031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:36:39 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '626'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9996'
      x-ratelimit-remaining-tokens:
      - '198992'
      x-ratelimit-reset-requests:
      - 29.112s
      x-ratelimit-reset-tokens:
      - 302ms
      x-request-id:
      - req_204b87202e813a42f48f7ed9a8221094
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content":
      "Text: The output for the text \"0 5 0\" could be anything depending on the
      context. Could you please provide more information or context so I can give
      you a more accurate response?\nOutput: "}], "model": "gpt-3.5-turbo", "max_tokens":
      1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '321'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1RSy27bMBC86ysGPMuGnESN60tQoD0U6KFAeugjhUFRa4kJyWXJVeIg8L8XlF0b
        vRDEzM5gdsi3ClC2VxsoM2oxPrrFe/e0pnv55m6+NN1+TKn9/unH48fu5/r2Q6vqouDukYz8Uy0N
        ++hILIcjbRJpoeK6ur1ard6166aZCc89uSIboiyul+1CptTxolldnYzNyNZQVhv8qgDgbT5LxtDT
        Xm0w+8yIp5z1QGpzHgJUYlcQpXO2WXQQVV9Iw0EozLHvp0Q1bMDLqAUzsRfYDBkJ8/1BNWjRPCh0
        ZMOAKVN/h88ZVpDIlf0gDI0cydidNYiJh6S9L9NOh2HSA9XwWkbyWqzRDvRn0qWmGpyQ2RNYRkrI
        hoJOlu/wNfGz7YuF50ToSbR1GS/WOYzkIjxhsM+EV56gj0PamClpISTKkUOmpTotfTi35XiIibvS
        bJicO+M7G2wet4l05lCaycLxKD9UwO/5Vab/ilYxsY+yFX6iUAzb9minLv/gQt6sT6SwaHfBV811
        dQqo8msW8tudDQOlmOz8RiVmdaj+AgAA//8DAKr/uzOiAgAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a4521081c801031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:36:40 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '819'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9995'
      x-ratelimit-remaining-tokens:
      - '198950'
      x-ratelimit-reset-requests:
      - 36.891s
      x-ratelimit-reset-tokens:
      - 315ms
      x-request-id:
      - req_c8400ea5379ee23060e2610da28322da
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content":
      "Text: It seems like you''ve provided a set of numbers \"0 0 0\" but did not
      specify what output you are looking for. Could you please provide more context
      or clarify what you would like me to do with these numbers?\nOutput: "}], "model":
      "gpt-3.5-turbo", "max_tokens": 1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '354'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1RSu27bMBTd9RWnnG1Dcmsn9RKgGdoC6ZIOGZrAoOkrizXFy5BXVdTA/15QfqGL
        hvPC4bl6LwBlt2oFZRotpg1u+tntb+ud+/blVezr41Oomx9L9l9/PjzeNG9qkh28+U1Gzq6Z4TY4
        Esv+SJtIWiinVjfzqloubstqJFreksu2XZDpx9liKl3c8LSs5ouTs2FrKKkVfhUA8D5+c0e/pTe1
        Qjk5Iy2lpHekVhcRoCK7jCidkk2ivajJlTTshfxY+56iaOvd8AH33LktBu4QHOlEME5HWw/oGy0j
        3o8KZ/eEliCMLaO30kAaQiIB1/Bdu6GY8KxKlCif1R2eLsGjVRiBYs2xhYbRznRO58kmMOz/UJQc
        18J6YWjP0lBEVk/AEdprN/ylsyQnHF+AXg93eCDJ1faeezTc4zuM9jiOMDaou5gDZ+q0xuEyo+Nd
        iLzJk/vOuQteW29Ts46kE/s8WRIOR/uhAF7Gc3X/XUCFyG2QtfCefA5cfjrGqesPciUXNydSWLS7
        4tW8Kk4FVRqSULuurd9RDNGOx8s1i0PxDwAA//8DAELyICa7AgAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a45210ebe391031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:36:42 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '1196'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9994'
      x-ratelimit-remaining-tokens:
      - '198942'
      x-ratelimit-reset-requests:
      - 44.467s
      x-ratelimit-reset-tokens:
      - 317ms
      x-request-id:
      - req_0bacef91e4a2a401bb4233145a8ff609
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "\nA prompt is a text paragraph that outlines the
      expected actions and instructs the large language model (LLM) to generate a
      specific output. This prompt is concatenated with the input text, and the model
      then creates the required output.\nThis describes the full template how the
      prompt is concatenated with the input to produce the output:\n\n```\n\n{prompt}\nText:
      {input}\nOutput: {0->1}\n```\n\nHere:\n- \"Text: {input}\" is input template,\n-
      \"{prompt}\" is the LLM prompt,\n- \"Output: {0->1}\" is the output template.\n\nModel
      can produce erroneous output if a prompt is not well defined. In our collaboration,
      we\u2019ll work together to refine a prompt. The process consists of two main
      steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction
      examples. Each example contains the input text, the final prediction produced
      by the model, and the user feedback. User feedback indicates whether the model
      prediction is correct or not. Your task is to analyze the examples and user
      feedback, determining whether the existing instruction is describing the task
      reflected by these examples precisely, and suggests changes to the prompt to
      address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review
      your reasoning in step 1, integrate the insights to refine the prompt, and provide
      me with the new prompt that improves the model\u2019s performance."}, {"role":
      "assistant", "content": "Sure, I\u2019d be happy to help you with this prompt
      engineering problem. Please provide me with the current prompt and the examples
      with user feedback."}, {"role": "user", "content": "\n## Current prompt\n...\n\n##
      Examples\n### Example #0\n\nText: 0 5 0\n\nOutput: The output for the text \"0
      5 0\" could be anything depending on the context. Could you please provide more
      information or context so I can give you a more accurate response?\n\nUser feedback:
      Prediction is incorrect. Correct answer: \"1 5 1\"\n\n\n### Example #1\n\nText:
      0 0 0\n\nOutput: It seems like you''ve provided a set of numbers \"0 0 0\" but
      did not specify what output you are looking for. Could you please provide more
      context or clarify what you would like me to do with these numbers?\n\nUser
      feedback: Prediction is incorrect. Correct answer: \"1 1 1\"\n\n\n\nSummarize
      your analysis about incorrect predictions and suggest changes to the prompt."}],
      "model": "gpt-4-turbo-preview", "max_tokens": 4096}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '2529'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA4xWTW/cNhC9+1cM5IMBYy14N0nj7KFA2nxt0SIB4l5SFQmXnJUYU6TCofYDhv97
        MSQl7To+9LLAiuRw5r03j3N/BlBoVSyhkI0Isu3M1Stzd1P/sVg/+/J+9d3j7evV25dv9LP9+8Vf
        P74UMz7h1t9RhuFUKV3bGQza2bQsPYqAHHX+cjGf//Li5noRF1qn0PCxugtXz6+u54sXV53HrcZd
        Ptk4LZGKJfxzBgBwH385R6twXyzhejZ8aZFI1Fgsx00AhXeGvxSCSFMQNhSzaVE6G9DGtM/P4bUV
        5kCawG1gZaXzHmWATx6VllwKVbay77xrITQInXdbrVAB7gUXSyCsgp7QwwZRrYW8m4EOILoOhad4
        Jgi6g1bXTQBtt85sEci1CBvnW761EyGgt+BRutpqvnQGwQtLvEOk/86DAOpQ6o2W4Dr0cYEvMhoV
        BMfr+KNHK5Gj2r5do6cSbhsE2XuPNnD6bReAEFviI2uEVhNpW/MFvVW40RbVDASBDhcEHjuPhDag
        gvUBqqIsy6rgoJqgFXfI+0A2whi0NcfhRIiQCBq3gx0aE0HQloLvE6JQ91ph/ByVANrGuz1TpTgI
        g9qh5/pjzAFFEaARVpXwwe1wi34Ga0GowNm4x4rQ+1g9/xsIidH4A+47lFzJwLKwtENPkbGMyZQT
        AfVd5yiBO/LBQMU/RgQcEacjyKecBEj0QWgLvjfIJwkD7+S/BNYFkAaFNwdwfTAMPWMRjhg7xq1k
        KZ6fn8PHNaHfRgHQsrLzEi4v/+RK3QY+DxpZHZ28vFxGHcTSIq3UOUvMnlVaciW9zbmGA4i16wPs
        GhG4dOVgp0OTaez6MBQ6A+rrGikkjnj3z1ozQt7RpNwTHTgPsRn3IV85EF1WdsFF5TZEBVsq4e3A
        38c+dH04KYr5GvkNjvHkTMVEHKqn+gN2DXoEFLKBqriuCtgMvZ5q1XQSQtvYaVUxr4oZHDCwdnTi
        UjkGgritpOgJeUm5TPQk8FMtiohFWdlnXPDf7CTvsnC5wFXmhyaABxJzidS43ihuZQFKR12fmsfQ
        D4+42zhj3C5220jPDNY9p6oV2qA3mq2AtToq73MiHBX83ghbY3QRDv4psr2s7K0DoZRnA0iXDs3W
        TZYaO1IYXdunChrFNmL0yF+zEbO6LKIi7jNuI/QQDUwfVx3lBCsbvFO95HJx3xkt9WlvJXhzF8aD
        k8cOHT8heqy0NQ5WNRnRCdSwGwiSXgcthSnhA4tOc+Y0IppLap1iKuJFS8b927dvlX3nfBLpUxqu
        9RZt9Gy+9+jxSEeUrvlJktJ5lS06+uMoAKZ4yV5vhEQQxsDF9QUlIi7mFzQDg2IbpWIMuNAw0hyU
        2DaiEFTJEtgOvv5k14kkiURzlNQt7sMS7mPaD5VNfb2E++urX+cPufLKxseGxwMaUVpW9gouL7PZ
        YQp8Kyj1zGSr6UGjR4T+H9bKdMOb2FEpwBP+89sBKIgwPlKnMmFcZ0dvinU7aAQdv+TxKUyac5mR
        GewazUynxvaoegavXeu61+HALwQjP3p4zvQ191Mm7ScbyT45CgtaEWSTgXmU9fAkxBef1x+132PT
        P/LO8WEkmGwyMr3iaallV0lQEYLMDpLr1C0PV0k+QsreC3kYmnhwiGMPWR/yNMYRdbYNwVukpmFe
        sGrqVdcH6VosizwKPowzpHF1592a503bGzN+Zy+h5qtHQc7yvEjBden4wxnAv3FW7U/GzyLp82tw
        d2g54Iv5IsUrpvF4Wn1+kyfZIrggzLTw6tXiLKdY0IECtl832tboO6/j7MqJnj2c/QcAAP//AwAZ
        iVFKugsAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a452117eca61031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:07 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '24689'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '500'
      x-ratelimit-limit-tokens:
      - '30000'
      x-ratelimit-remaining-requests:
      - '499'
      x-ratelimit-remaining-tokens:
      - '25336'
      x-ratelimit-reset-requests:
      - 120ms
      x-ratelimit-reset-tokens:
      - 9.328s
      x-request-id:
      - req_ce7a98fd88eba27a2131c92ec8a96147
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "\nA prompt is a text paragraph that outlines the
      expected actions and instructs the large language model (LLM) to generate a
      specific output. This prompt is concatenated with the input text, and the model
      then creates the required output.\nThis describes the full template how the
      prompt is concatenated with the input to produce the output:\n\n```\n\n{prompt}\nText:
      {input}\nOutput: {0->1}\n```\n\nHere:\n- \"Text: {input}\" is input template,\n-
      \"{prompt}\" is the LLM prompt,\n- \"Output: {0->1}\" is the output template.\n\nModel
      can produce erroneous output if a prompt is not well defined. In our collaboration,
      we\u2019ll work together to refine a prompt. The process consists of two main
      steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction
      examples. Each example contains the input text, the final prediction produced
      by the model, and the user feedback. User feedback indicates whether the model
      prediction is correct or not. Your task is to analyze the examples and user
      feedback, determining whether the existing instruction is describing the task
      reflected by these examples precisely, and suggests changes to the prompt to
      address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review
      your reasoning in step 1, integrate the insights to refine the prompt, and provide
      me with the new prompt that improves the model\u2019s performance."}, {"role":
      "assistant", "content": "Sure, I\u2019d be happy to help you with this prompt
      engineering problem. Please provide me with the current prompt and the examples
      with user feedback."}, {"role": "user", "content": "\n## Current prompt\n...\n\n##
      Examples\n### Example #0\n\nText: 0 5 0\n\nOutput: The output for the text \"0
      5 0\" could be anything depending on the context. Could you please provide more
      information or context so I can give you a more accurate response?\n\nUser feedback:
      Prediction is incorrect. Correct answer: \"1 5 1\"\n\n\n### Example #1\n\nText:
      0 0 0\n\nOutput: It seems like you''ve provided a set of numbers \"0 0 0\" but
      did not specify what output you are looking for. Could you please provide more
      context or clarify what you would like me to do with these numbers?\n\nUser
      feedback: Prediction is incorrect. Correct answer: \"1 1 1\"\n\n\n\nSummarize
      your analysis about incorrect predictions and suggest changes to the prompt."},
      {"role": "assistant", "content": "## Analysis of Incorrect Predictions\n\nFrom
      the provided examples and user feedback, it appears the task might involve some
      form of pattern recognition, transformation, or a specific operation applied
      to a sequence of numbers. The current prompt seems to be missing or undefined,
      as it''s represented by \"...\". This makes it challenging to assess how well
      the instructions guide the model in understanding and performing the task at
      hand. However, based on the nature of the feedback and the expected correct
      answers, it seems the model is supposed to transform or translate sequences
      of numbers based on a certain rule or set of rules not clearly outlined in the
      current instructions.\n\n### Observations:\n1. **Lack of Specific Instructions**:
      The model''s responses indicate uncertainty about what to do with the input
      numbers, suggesting that the current prompt lacks specific instructions or context
      about the task.\n2. **Predicted vs. Expected Output**: The model is expected
      to output a transformed sequence of numbers where each \"0\" from the input
      is transformed into a \"1\", yet it is not doing so because it does not understand
      the expected action.\n3. **User Feedback**: Indicates that the model''s output
      should be a direct transformation of the input numbers, following a specific,
      but unidentified, rule.\n\n### Suggested Changes to the Prompt:\nTo address
      the incorrect predictions and align the model''s output with the expected user
      feedback, the prompt needs a clearer definition of the task. Introducing explicit
      instructions that outline the operation or transformation expected to be performed
      on the input numbers would be critical. Here is a suggested prompt modification:\n\n```\nFor
      each sequence of numbers given as input, transform each digit according to the
      following rule: replace all ''0''s with ''1''s, leaving all other digits unchanged.
      Provide the transformed sequence as the output.\n\nText: {input}\nOutput: {0->1}\n```\n\nThis
      revised prompt:\n- **Specifies the Task**: It clearly defines the operation
      to be performed on the input numbers.\n- **Directs the Expected Output**: By
      stating the transformation rule, the model now has a specific guideline to follow,
      which should reduce ambiguity in its responses.\n- **Aligns with User Feedback**:
      The modification matches the transformation indicated by the user feedback,
      suggesting that each \"0\" translates to a \"1\".\n\nImplementing these changes
      should improve the accuracy of the model''s predictions by providing it with
      a precise task and expected outcome."}, {"role": "user", "content": "\nNow please
      carefully review your reasoning in Step 1 and help with Step 2: refining the
      prompt.\n\n## Current prompt\n...\n\n## Follow this guidance to refine the prompt:\n\n1.
      The new prompt should should describe the task precisely, and address the points
      raised in the user feedback.\n\n2. The new prompt should be similar to the current
      prompt, and only differ in the parts that address the issues you identified
      in Step 1.\n    Example:\n    - Current prompt: \"Generate a summary of the
      input text.\"\n    - New prompt: \"Generate a summary of the input text. Pay
      attention to the original style.\"\n\n3. Reply only with the new prompt. Do
      not include input and output templates in the prompt.\n"}], "model": "gpt-4-turbo-preview",
      "max_tokens": 4096}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '5852'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SRzW7bMBCE73qKBS++2IblxnbtNwiQQwrkVgQGRa0kOuQuQy6dJoHfvaD81154
        2OEMZz9+VwDKtmoHygxajA9utnVv29XXVmf7a203q8ZzJ+3yRfLz40GraXFwc0AjV9fcsA8OxTKd
        ZRNRC5bUerOs6/Xq53IzCp5bdMXWB5k9zBb1cjULEY8WPy7Oga3BpHbwuwIA+B7P0pFa/KN2sJhe
        Jx5T0j2q3e0SgIrsykTplGwSTaKmd9EwCdJY+yVqSh1HD6jNAAnfM5JB4A4o+wZjghD5aFtsQSew
        FLJA8wkRg9PGUg/aOZgsJgk+rAwwqSdpDk+ojzgqLAPGW1QmM2jqsQVhOMMBGRA4S8m9vj5Xl66n
        25KO+xC5KUAoO3ebd5ZsGvYRdWIqCyXhcLafKoDXEWb+j48KkX2QvfAbUgms64f1OVDdP/Au/7ig
        VsKi3b+2zbq6dFTpMwn6fWepxxiiHemWptWp+gsAAP//AwAGqYpIXAIAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a4521b3cdaa1031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:08 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '1577'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '500'
      x-ratelimit-limit-tokens:
      - '30000'
      x-ratelimit-remaining-requests:
      - '499'
      x-ratelimit-remaining-tokens:
      - '24534'
      x-ratelimit-reset-requests:
      - 120ms
      x-ratelimit-reset-tokens:
      - 10.932s
      x-request-id:
      - req_56dd49d914158c16f1edc14a66ce673f
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "Transform each sequence of
      numbers provided as input by replacing all ''0''s with ''1''s. Leave all other
      numbers unchanged to create the output sequence."}, {"role": "user", "content":
      "Text: 0 5 0\nOutput: "}], "model": "gpt-3.5-turbo", "max_tokens": 1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '299'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SQQW/CMAyF7/0Vls8UEVBh9LbLLjuMA5omTRMKrSmBNA6JkZgQ/31KKVS7+PCe
        39NnXzMANDWWgNVeS9V6my/tcbk4xa/D+yp+qt1qfnl7dXpSr4tT+MBRSvD2QJU8UuOKW29JDLu7
        XQXSQqlVLaZKzYuX6bIzWq7JpljjJZ+Ni1zOYcv5RE2LPrlnU1HEEr4zAIBrNxOjq+mCJUxGD6Wl
        GHVDWD6XADCwTQrqGE0U7QRHg1mxE3Id9pouUoKCAhT2C7dns+XGB94mCne29qnvjDNxvwmkI7vU
        EoX9PX7LAH66C87/oNAHbr1shI/kUmExu9fh8LPBfOk9YdF2kOcq6/Ew/kahdrMzrqHgg+muSZDZ
        LfsDAAD//wMAET0i+cwBAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a4521c29ae51031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:10 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '530'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9997'
      x-ratelimit-remaining-tokens:
      - '198955'
      x-ratelimit-reset-requests:
      - 24.327s
      x-ratelimit-reset-tokens:
      - 313ms
      x-request-id:
      - req_d9a3aa9faf4f5dbe9bb7897f378e889b
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "Transform each sequence of
      numbers provided as input by replacing all ''0''s with ''1''s. Leave all other
      numbers unchanged to create the output sequence."}, {"role": "user", "content":
      "Text: 0 0 0\nOutput: "}], "model": "gpt-3.5-turbo", "max_tokens": 1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '299'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SQUWvCMBSF3/srLnm2YnR1tX9gw7m3bQzGkDZe28w0NyRXUMT/PhI7ywjk4Zx7
        Dt+9lwxA6J2oQKiuZtU7k6/MYVW6zfm9PD+8rhv98rwpPp86vfxYH6SYxAQ1P6j4LzVV1DuDrMne
        bOWxZoyt8nEu5bIoF7Nk9LRDE2Ot43wxLXI++obymZwXQ7IjrTCICr4yAIBL+iOj3eFJVJB6ktJj
        CHWLoroPAQhPJiqiDkEHri2LyWgqsow2Yb/hiSuQ8Ylh4HpvNtQ6T02ksEdj7vpeWx26rcc6kI0t
        gcnd4tcM4DttcPwHJZyn3vGW6YA2FhaLW50Ybzaa5eAxcW1GeSmzAU+Ec2Dst3ttW/TO67RNhMyu
        2S8AAAD//wMAjidE58wBAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a4521c7699c1031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:10 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '453'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9996'
      x-ratelimit-remaining-tokens:
      - '198955'
      x-ratelimit-reset-requests:
      - 32.223s
      x-ratelimit-reset-tokens:
      - 313ms
      x-request-id:
      - req_3a1d40b2d46375ff1b40e1eef29fefbd
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content":
      "Text: Text: 1 5 1\nOutput: "}], "model": "gpt-3.5-turbo", "max_tokens": 1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '158'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SRvW7jMBCEez3FgLVtWP67s5srEiRIkepwRZAcDEpaS4wpLkGukjiB3/1A2rFx
        DYud/XZnh18FoEyjNlB1p6XuvR2v7X69/mPvHh9+L578520b7ht5faruuVms1SgRXL1SLd/UpObe
        WxLD7iTXgbRQmlr+mJXlavlzXmah54Zswlov4/lkOZYhVDyelrPlmezY1BTVBs8FAHzlN3l0DX2o
        Daaj70pPMeqW1ObSBKjANlWUjtFE0U7U6CrW7IRctv0giER9hDV7woEHdPqN4AO/mYYaaESdboJx
        fpARXlSJJcoXNcEND7bJiLek4wVCz4GQd3wIOCB6qs3ugPdOS+5/z2TeKB2BB/GDQBgV/VJnm8fL
        fZZbH7hKWbjB2kt9Z5yJ3TaQjuzSLVHYn/BjAfzNOQ7/RaN84N7LVnhPLg2crU7j1PXnruJ8cRaF
        RdtrfTUtzv5UPEShfrszrqXgg8mhJpfFsfgHAAD//wMApeqvuFMCAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a4521cbef891031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:11 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '556'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9995'
      x-ratelimit-remaining-tokens:
      - '198990'
      x-ratelimit-reset-requests:
      - 40.101s
      x-ratelimit-reset-tokens:
      - 303ms
      x-request-id:
      - req_1d7b9c51d0ef1e51044df5afc7420588
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content":
      "Text: Text: 1 1 1\nOutput: "}], "model": "gpt-3.5-turbo", "max_tokens": 1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '158'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1RRTWsbMRS8768YdLZNdo3dZi8llBwChV5SCm2KkXefvYolPSG9jR2C/3uR/EUQ
        6DDzZjRv9FEByvSqheoGLZ0Ldnpvd/cPP1737tn9qg8LEx/i75+7fv34RzypSVbw+pU6uahmHbtg
        SQz7E91F0kLZtf7S1PVy8XXeFMJxTzbLtkGm89liKmNc8/SubhZn5cCmo6Ra/K0A4KPcOaPv6aBa
        3E0uiKOU9JZUex0CVGSbEaVTMkm0FzW5kR17IV9iPwkSkUuwZkd45xGDfiOQF4rU40U900Fa1Pm8
        qBm+82j7Mhcs6UQIkd9MT3AcCcX4IOAI4zccnc5dgD32g5Yi2xeD8pojCKNn7I0MkMEkZPU3dU56
        vK5oeRsir3MdfrT2im+MN2lYRdKJfV4nCYeT/FgB/0qV46d2VIjsgqyEd+SzYbM82anb593I+YUU
        Fm1v+LKpzvlUek9CbrUxfksxRFN6zSmrY/UfAAD//wMAz1UBQ1YCAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a4521d06e0e1031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:12 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '659'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9994'
      x-ratelimit-remaining-tokens:
      - '198990'
      x-ratelimit-reset-requests:
      - 48.037s
      x-ratelimit-reset-tokens:
      - 303ms
      x-request-id:
      - req_f60dc8cf20d873f921a589e59a3bd509
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "\nA prompt is a text paragraph that outlines the
      expected actions and instructs the large language model (LLM) to generate a
      specific output. This prompt is concatenated with the input text, and the model
      then creates the required output.\nThis describes the full template how the
      prompt is concatenated with the input to produce the output:\n\n```\n\n{prompt}\nText:
      {input}\nOutput: {0->1}\n```\n\nHere:\n- \"Text: {input}\" is input template,\n-
      \"{prompt}\" is the LLM prompt,\n- \"Output: {0->1}\" is the output template.\n\nModel
      can produce erroneous output if a prompt is not well defined. In our collaboration,
      we\u2019ll work together to refine a prompt. The process consists of two main
      steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction
      examples. Each example contains the input text, the final prediction produced
      by the model, and the user feedback. User feedback indicates whether the model
      prediction is correct or not. Your task is to analyze the examples and user
      feedback, determining whether the existing instruction is describing the task
      reflected by these examples precisely, and suggests changes to the prompt to
      address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review
      your reasoning in step 1, integrate the insights to refine the prompt, and provide
      me with the new prompt that improves the model\u2019s performance."}, {"role":
      "assistant", "content": "Sure, I\u2019d be happy to help you with this prompt
      engineering problem. Please provide me with the current prompt and the examples
      with user feedback."}, {"role": "user", "content": "\n## Current prompt\nTransform
      each sequence of numbers provided as input by replacing all ''0''s with ''1''s.
      Leave all other numbers unchanged to create the output sequence.\n\n## Examples\n###
      Example #0\n\nText: 0 5 0\n\nOutput: Text: 1 5 1\n\nUser feedback: Prediction
      is incorrect. Correct answer: \"1 5 1\"\n\n\n### Example #1\n\nText: 0 0 0\n\nOutput:
      Text: 1 1 1\n\nUser feedback: Prediction is incorrect. Correct answer: \"1 1
      1\"\n\n\n\nSummarize your analysis about incorrect predictions and suggest changes
      to the prompt."}], "model": "gpt-4-turbo-preview", "max_tokens": 4096}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '2317'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA6xWTW8bNxC9+1cM1ge3xlqwHNup3VPiNm1QFDVS91BURUCRs9qJueSWw5WsBPnv
        xZC7q5XcIj30IB2WnK83b97w0xFAQaa4hULXKuqmtWc39vHm9XX88dXNdw8/Pf58+fGvd/fd3L8L
        Nx87VZRi4ZcfUMfBaqZ901qM5F0+1gFVRPE6f3kxn19fffPiRTpovEErZqs2nl2enc8vrs7agGvC
        TW9Ze9LIxS38cQQA8Cn9S47O4FNxC+fl8KVBZrXC4na8BFAEb+VLoZiJo3KxKHeH2ruILqX9WjEa
        8A5ijdAGvyaDBvBJSSEMypl0YpB1oFYqA1+lT1HxYwkUgREbhlirmL6n0k4Y2oCGtFgwqICgdOyU
        tVtQllaO3Aq0DwF1tFvYUKxHp32wJRqgMa+mjTN4GG4QAz61ljSJtVr6LkIMynHlQyOuUekaGP/q
        0GmUjF3XLDEwLLcQsLVKyy1lLZycn3COfzI/yQVbVOvh2Mcaw2jdOV0rt0ID0UNubkrQd7Ht4hgw
        ZzqCOMIasLKoBSbiXIjSugsqot3eLtzCncEbH+D7bAjH52XyTm7qHBbFOVzB+aIQGHYYjvXn7BbF
        HK5gvihKaFTUtRR0gHBq5+xZ1HkJTA1ZFex2moDEPf9Pcec57qYmXYOy7HPPeb/PJwzkOIYuk2Qm
        AAhu2ruqY2GaCsTIUAXfJKuOMUCFaJZKPwI5Q1rFXFhPvkPSkesT/RZqv8E1hhKWU8ofwlH+o5uh
        TdJYYjDEOmCrnN6C9p01sEQwHUr9ChrizhkMMnZGsusHpieJQKUi+JCvkosY2oBRTYdrCoykWvvN
        HtHqIWobkNFFNAm+4+Nj+LVbrZCzZeUD3CXGsuQmHu7TLMnlH2iNGYQd4q1npqXM1AT6g+DEu6jD
        hI5cDyoNTKzV6Fqk5gADioy2KqFGAVd+UOEGeJJ6amkGt0bbgrYqULWd6EGamPkMTk/v+rNfsvf7
        nF0C9PT0Ft5GaGhVx+xIJre/j08qUfjL6JaA3KKmJGCUagmY5GIL6aAiPXR2JGPvziGaBH/lrfWb
        Gbyt9jHbCAoNKhflFks23tlc6nS6RgWQMUqSV4saPOBTvF0UkmxFTyJvaekMrclZlFl0dqVN9JOj
        Ggh0IXDeZ70CBXcW1U4Z3qT6BNFX5kPHcVrFUPyeYks5O81LMVFLaj0yjQ8IWmLY7SyJkPBeOY1l
        D/JBS5yXCNp2JutuWwfFEwyeVanWPinvIU1Xwv1U8Qup+JUxJGxRFt4eTF4uWlQmUana7bh+C+WK
        UqVRkUuUqEYjmfPcmKmQZUH9ytLjJPmvn2XfQzM0CO68W2NgFF3eU4gpY/tdQxVwp2tAiw26mHVs
        mu4SIWDj19KOIBVRRYesGUXlt9YkTmX1mGiMXFgUDwNL/33xjokp7sv/wiou/489/LvvwgGHDPWL
        a8rM6ZjlIM+nTRqrdkSJ+BT3uts5i8yjHCSpEOjJO5muIu83Yuh6MPshUTFi00bOHJKO5FDNklYd
        RUl0pYIZFvj+GpH3ykATgxU5UfrdgmXtW0yXcuuTJnIJrRdVznIW0HSpB+Oy3O3YcVdOGH2433hW
        9E/Lz+Ob1PpVG/xS3q+us3b8XpEjrt8HVOydvD85+jabfz4C+DO9fbu952yRYXof/SM6cXj58jL7
        K3bP7cnpzUV/Gn1Udndwc3191KdY8JYjNu8rcitZvpTewpLo0eejvwEAAP//AwCP91xnCgwAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a4521d64dbc1031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:30 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '18030'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '500'
      x-ratelimit-limit-tokens:
      - '30000'
      x-ratelimit-remaining-requests:
      - '499'
      x-ratelimit-remaining-tokens:
      - '25388'
      x-ratelimit-reset-requests:
      - 120ms
      x-ratelimit-reset-tokens:
      - 9.224s
      x-request-id:
      - req_af2fe465dfc27241822247898323cf86
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "\nA prompt is a text paragraph that outlines the
      expected actions and instructs the large language model (LLM) to generate a
      specific output. This prompt is concatenated with the input text, and the model
      then creates the required output.\nThis describes the full template how the
      prompt is concatenated with the input to produce the output:\n\n```\n\n{prompt}\nText:
      {input}\nOutput: {0->1}\n```\n\nHere:\n- \"Text: {input}\" is input template,\n-
      \"{prompt}\" is the LLM prompt,\n- \"Output: {0->1}\" is the output template.\n\nModel
      can produce erroneous output if a prompt is not well defined. In our collaboration,
      we\u2019ll work together to refine a prompt. The process consists of two main
      steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction
      examples. Each example contains the input text, the final prediction produced
      by the model, and the user feedback. User feedback indicates whether the model
      prediction is correct or not. Your task is to analyze the examples and user
      feedback, determining whether the existing instruction is describing the task
      reflected by these examples precisely, and suggests changes to the prompt to
      address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review
      your reasoning in step 1, integrate the insights to refine the prompt, and provide
      me with the new prompt that improves the model\u2019s performance."}, {"role":
      "assistant", "content": "Sure, I\u2019d be happy to help you with this prompt
      engineering problem. Please provide me with the current prompt and the examples
      with user feedback."}, {"role": "user", "content": "\n## Current prompt\nTransform
      each sequence of numbers provided as input by replacing all ''0''s with ''1''s.
      Leave all other numbers unchanged to create the output sequence.\n\n## Examples\n###
      Example #0\n\nText: 0 5 0\n\nOutput: Text: 1 5 1\n\nUser feedback: Prediction
      is incorrect. Correct answer: \"1 5 1\"\n\n\n### Example #1\n\nText: 0 0 0\n\nOutput:
      Text: 1 1 1\n\nUser feedback: Prediction is incorrect. Correct answer: \"1 1
      1\"\n\n\n\nSummarize your analysis about incorrect predictions and suggest changes
      to the prompt."}, {"role": "assistant", "content": "Based on the provided examples
      and the description of the task, it seems that the model''s predictions are
      actually aligning correctly with the task described in the prompt. The task
      is explicitly about transforming each sequence of numbers by replacing all ''0''s
      with ''1''s and leaving all other numbers unchanged to create the output sequence.
      The examples provided reflect this task accurately:\n\n- For Example #0, the
      input sequence \"0 5 0\" is correctly transformed to \"1 5 1\", matching the
      task description.\n- For Example #1, similarly, the input \"0 0 0\" is correctly
      transformed to \"1 1 1\", which also aligns with the task''s instructions.\n\nThe
      confusion arises from the user feedback indicating that the predictions are
      incorrect; however, based on the task description, the predictions are accurate.
      This discrepancy could be due to a misunderstanding of the output format or
      a misinterpretation of the instructions on how the output should be presented.\n\n###
      Suggestions for Changes to the Prompt\n\nGiven the confusion possibly arises
      from how the output is presented in the examples rather than the content of
      the output itself, here are a few suggestions that could help clarify the prompt:\n\n1.
      **Clarify Output Presentation**: It might help to clarify exactly how the output
      should be presented, especially if there''s any specific format that the output
      needs to follow. If the examples were meant to show only the transformed sequence
      without the \"Text:\" prefix repeated in the output, this should be explicitly
      stated.\n\n2. **Provide a Clear Example Format**: Adjust the example format
      in the prompt to reflect the expected output more clearly. For instance, if
      the output should not include the phrase \"Text:\", this should be avoided in
      the examples given.\n\n3. **Additional Instructions on Formatting**: If the
      model is expected to retain any formatting or prefixes from the input (like
      \"Text:\"), this should be clearly stated. Conversely, instructions should be
      provided if such elements are expected to be removed or modified in the output.\n\n###
      Updated Prompt Suggestion\n\n\"Transform each sequence of numbers provided as
      input by replacing all ''0''s with ''1''s, leaving all other numbers unchanged
      to create the output sequence. Your output should directly reflect the transformed
      number sequence without any additional text or prefixes unless specifically
      mentioned.\"\n\nThis updated prompt attempts to remove any ambiguity regarding
      the output format and clearly defines the task''s scope and expectations, potentially
      reducing incorrect feedback based on formatting misunderstandings."}, {"role":
      "user", "content": "\nNow please carefully review your reasoning in Step 1 and
      help with Step 2: refining the prompt.\n\n## Current prompt\nTransform each
      sequence of numbers provided as input by replacing all ''0''s with ''1''s. Leave
      all other numbers unchanged to create the output sequence.\n\n## Follow this
      guidance to refine the prompt:\n\n1. The new prompt should should describe the
      task precisely, and address the points raised in the user feedback.\n\n2. The
      new prompt should be similar to the current prompt, and only differ in the parts
      that address the issues you identified in Step 1.\n    Example:\n    - Current
      prompt: \"Generate a summary of the input text.\"\n    - New prompt: \"Generate
      a summary of the input text. Pay attention to the original style.\"\n\n3. Reply
      only with the new prompt. Do not include input and output templates in the prompt.\n"}],
      "model": "gpt-4-turbo-preview", "max_tokens": 4096}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '5867'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA2ySO4/bMBCEe/2KBRs3tmE59iV27+KuSFLkUQSBQZMriTlylyZXvjMO/u8B5dcF
        SKNihzP6uMO3CkA5q9agTKfFhOgnK/+8+v55kx73+PTkXP7yc7E/PP74ugkPm4UaFwfv/qCRq2tq
        OESP4pjOskmoBUtq/XFe1w/LT8v5IAS26IutjTJZTGb1fDmJCQ8OXy7Ojp3BrNbwqwIAeBu+hZEs
        vqo1zMbXScCcdYtqfTsEoBL7MlE6Z5dFk6jxXTRMgjRgf0uacsMpAGrTQcZ9j2QQuAHqww5Thpj4
        4Cxa0BkcxV5gd4SE0WvjqAXtPYxmowwvTjoY1aM8Bk0WPOrDVWfpMN0CezKdphYtCEOLhEkLgnQI
        ga1rHNobxxQ2lPt0VrmX8veYMCNJBiZ/HIT/UVuX0Ig/jgcu7qUgo5YBiY6grXWlKO1B8FWAE5Q1
        aIEmcRhih8tO1WVvp9vCPbcx8a6UQ733t3njyOVum1BnprLcLBzP9lMF8Hsotv+nKxUThyhb4Wek
        EljXi9U5UN0f011efriIwqL9O9t8Nq8ujCofs2DYNo5aTDG5oelCWp2qvwAAAP//AwBlM5+/6AIA
        AA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a452248abb91031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:34 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '1931'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '500'
      x-ratelimit-limit-tokens:
      - '30000'
      x-ratelimit-remaining-requests:
      - '499'
      x-ratelimit-remaining-tokens:
      - '24530'
      x-ratelimit-reset-requests:
      - 120ms
      x-ratelimit-reset-tokens:
      - 10.94s
      x-request-id:
      - req_8af437ed47b74d010c8e94fac4f04b08
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "Transform each sequence of
      numbers provided as input by replacing all ''0''s with ''1''s, and leaving all
      other numbers unchanged to generate the modified sequence. Ensure the output
      presents only the sequence of numbers directly, without repeating any additional
      text or format from the input."}, {"role": "user", "content": "Text: 0 5 0\nOutput:
      "}], "model": "gpt-3.5-turbo", "max_tokens": 1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '439'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SQy2rDMBBF9/6KQes4RGmdh5dpKV0VCoUUSgmyMrHVyBohjSEh5N+LHDemGy3u
        9bk68iUDEGYvShC6Uaxbb/O1Pa63nVFv55eGZ9vV++umea5Om/nnR/UkJomg6gc1/1FTTa23yIbc
        rdYBFWNalcu5lItiVTz2RUt7tAmrPecP0yLnLlSUz+S8GMiGjMYoSvjKAAAu/Zkc3R5PooTZ5C9p
        MUZVoyjvHwGIQDYlQsVoIivHYjKWmhyj67UlFCDFUF3vm5ZqH6hK97vO2nt+MM7EZhdQRXKJj0z+
        hl8zgO/evfunI3yg1vOO6YguDS4Xtzkx/q2xLIaOiZUd45XMBj0Rz5Gx3R2MqzH4YPp3JMnsmv0C
        AAD//wMAvsNCbMYBAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a45225caa431031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:34 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '415'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9996'
      x-ratelimit-remaining-tokens:
      - '198920'
      x-ratelimit-reset-requests:
      - 34.25s
      x-ratelimit-reset-tokens:
      - 324ms
      x-request-id:
      - req_f3ad3e75ee9b7b2f0783c628a11ccc63
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "Transform each sequence of
      numbers provided as input by replacing all ''0''s with ''1''s, and leaving all
      other numbers unchanged to generate the modified sequence. Ensure the output
      presents only the sequence of numbers directly, without repeating any additional
      text or format from the input."}, {"role": "user", "content": "Text: 0 0 0\nOutput:
      "}], "model": "gpt-3.5-turbo", "max_tokens": 1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '439'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SQy27CMBBF9/mKkdcEYUp4ZFukik1XlVqpqpBxhsTE8Vj2ILVF/HvlEIi68eLe
        nOvjXDIAYSpRgtCNYt15m29su/l43r7/7nanrdy8rhdv7aI5YuVOL62YJIIOJ9R8p6aaOm+RDblb
        rQMqxrQqV3Mpl8W6KPqiowptwmrP+dO0yPkcDpTP5LwYyIaMxihK+MwAAC79mRxdhd+ihNnknnQY
        o6pRlI+PAEQgmxKhYjSRlWMxGUtNjtH12hIkSDFU18empdoHOqT73dnaR340zsRmH1BFcomPTP6G
        XzOAr979/E9H+ECd5z1Tiy4Nrpa3OTH+rbEsho6JlR3jtcwGPRF/ImO3PxpXY/DB9O9Iktk1+wMA
        AP//AwBwDIkSxgEAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a45226139671031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:35 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '408'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9995'
      x-ratelimit-remaining-tokens:
      - '198920'
      x-ratelimit-reset-requests:
      - 42.156s
      x-ratelimit-reset-tokens:
      - 324ms
      x-request-id:
      - req_859498438f47226f78018d3053f715d8
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content":
      "Text: 1 5 1\nOutput: "}], "model": "gpt-3.5-turbo", "max_tokens": 1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '152'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA4SST4/aMBDF7/kUTz4DIrRhC9c9ceih1bbqqlTIOEPi4niCPeGPVnz3KiEBrVS1
        l0iZN+/nZ8+8JYCyuVpCmVKLqWo3Xrj94vXHtzJ7eT48r14PX/RxN9fm++Hr7HOhRq2Dt7/JyOCa
        GK5qR2LZ32QTSAu11PRplqbz7FM274SKc3Ktrahl/GGSjaUJWx5P01nWO0u2hqJa4mcCAG/dt83o
        czqrJaajoVJRjLogtbw3ASqwaytKx2ijaC9q9BANeyHfxX4pCdxI3Qh2HCAlQegsWKsUGdK1guHG
        5dgStL/AN9WWAngHKa0vInKqyefWF2DfuTv2WcABp1ILLtzg1CGc3ROEkTNOVsq2O1JPjBOs/dqv
        dl2/DgTHvG+xQ6rYVLdj75ZR99OHPw0pnyb/AfVu6BC0LyiH9dDRDLcIOYW/k1OkyP5Fj1xR9ywg
        F2mE2pGOhDrw0eaEisPjeSJjBaM9Cnsk6JuojWmCluHoiepHdr3P2nFRB962e+Eb5+71nfU2lptA
        OrJv5xqF65v9mgC/up1q3q2JqgNXtWyE9+Rb4OzjDaceW/wQF/NeFBbtHvV0Nk36gCpeolC12Vlf
        UKiD7TasjZlckz8AAAD//wMAUaoYfmADAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a45226619df1031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:37 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '1430'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9994'
      x-ratelimit-remaining-tokens:
      - '198992'
      x-ratelimit-reset-requests:
      - 50.001s
      x-ratelimit-reset-tokens:
      - 302ms
      x-request-id:
      - req_413a38d34606d9795d192436d3700194
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content":
      "Text: 1 1 1\nOutput: "}], "model": "gpt-3.5-turbo", "max_tokens": 1000}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '152'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1RRTU8CMRC976+Y9AzEXV0RrkZvCge8+BFStrO00u007VQkhv9uuqwQLz28N+/N
        m9efAkAYJeYgGi256bwdz+xu9jp9wvblAZfPj9XtbHmIj6tFtV/cazHKCtp8YsN/qklDnbfIhtyJ
        bgJKxuxaTquyvK3v6mlPdKTQZtnW8/h6Uo85hQ2Nr8qqHpSaTINRzOGtAAD46d+c0Sn8FnO4Gv0h
        HcYotyjm5yEAEchmRMgYTWTpWIwuZEOO0fWxVxqBEvvEsKdkFSj06BSQg72WDOQxyHwQOEQVgQk2
        CB5DS6FDBXvDGlgjMH4zvIsSSign7wKWFmVE8IG+jEJoU2CNAYyLHFKTHeN5yYHSsN2aHfZ2Q6Z+
        3UQM0Y/nmy1tfaBN7scla894a5yJeh1QRnL5vsjkT/JjAfDRd5v+1SV8oM7zmmmHLhtWNyc7cfnN
        C3ldDyQTS3vB61kx5BPxEBm7dWvcFoMPpi86pyyOxS8AAAD//wMA/xEDkmcCAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a452270a9de1031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:37 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '381'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '200000'
      x-ratelimit-remaining-requests:
      - '9993'
      x-ratelimit-remaining-tokens:
      - '198992'
      x-ratelimit-reset-requests:
      - 56.969s
      x-ratelimit-reset-tokens:
      - 302ms
      x-request-id:
      - req_8d8c015257c7ca2b394fdd46fbbcc8e8
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "\nA prompt is a text paragraph that outlines the
      expected actions and instructs the large language model (LLM) to generate a
      specific output. This prompt is concatenated with the input text, and the model
      then creates the required output.\nThis describes the full template how the
      prompt is concatenated with the input to produce the output:\n\n```\n\n{prompt}\nText:
      {0->1}\nOutput: {1->2}\n```\n\nHere:\n- \"Text: {0->1}\" is input template,\n-
      \"{prompt}\" is the LLM prompt,\n- \"Output: {1->2}\" is the output template.\n\nModel
      can produce erroneous output if a prompt is not well defined. In our collaboration,
      we\u2019ll work together to refine a prompt. The process consists of two main
      steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction
      examples. Each example contains the input text, the final prediction produced
      by the model, and the user feedback. User feedback indicates whether the model
      prediction is correct or not. Your task is to analyze the examples and user
      feedback, determining whether the existing instruction is describing the task
      reflected by these examples precisely, and suggests changes to the prompt to
      address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review
      your reasoning in step 1, integrate the insights to refine the prompt, and provide
      me with the new prompt that improves the model\u2019s performance."}, {"role":
      "assistant", "content": "Sure, I\u2019d be happy to help you with this prompt
      engineering problem. Please provide me with the current prompt and the examples
      with user feedback."}, {"role": "user", "content": "\n## Current prompt\n...\n\n##
      Examples\n### Example #0\n\nText: 1 5 1\n\nOutput: The output for the text \"1
      5 1\" could be any number of things depending on the context or what you would
      like to do with these numbers. \n\nIf you are looking for the sum of the numbers,
      the output would be 7.\n\nIf you are looking for the numbers arranged in ascending
      order, the output would be 1 1 5.\n\nIf you are looking for something else,
      please provide more context so I can give a more accurate output.\n\nUser feedback:
      Prediction is incorrect. Correct answer: \"2 5 2\"\n\n\n### Example #1\n\nText:
      1 1 1\n\nOutput: The output would depend on what operation needs to be performed
      with the text \"1 1 1.\" Please provide further instructions on what you would
      like the output to be.\n\nUser feedback: Prediction is incorrect. Correct answer:
      \"2 2 2\"\n\n\n\nSummarize your analysis about incorrect predictions and suggest
      changes to the prompt."}], "model": "gpt-4-turbo-preview", "max_tokens": 4096}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '2724'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA3RWwW4bNxC9+ysG64NbQRIix05j3VonbVLkkDQxiqIuAi452p2IS244pGQhyL8X
        Q+5qldS92NCSM3zz3pshv5wBVGSqNVS6VVF3vV3c2O2Nsoebv/DJ5e+fXt+9e3f7It79ubp98ebq
        oZpLhK8/oY5j1FL7rrcYybuyrAOqiJJ19dPlavXs+vn187zQeYNWwpo+Lq4WT1aX14s+4I5wP0S2
        njRytYa/zwAAvuS/gtEZfKjW8GQ+fumQWTVYrY+bAKrgrXypFDNxVC5W82lRexfRZdi/KEYD3kFs
        EfBBCX6GPvgdGTSgnMkrG0RTK72FgBpph2YOFC8YtEUVILYq5m06hYAuSnzXRyAG5yMog5+TimgP
        0CQy5Jq82SrXJNUgZDIgegkzSWNeNcgU0EA89Ah+Az7FPsUlfGjHgIDce8fIwKlpkGPBQflcv9lg
        kJN6z0y1RUmgfYcM9ViyIdkkeH2PQYlsXHJon6yBGqHHsPGhQwN7im0G1tAOHbjU1Rh4Dpa2CMqM
        VXWQevABVAjKNceP5ECBxhAVOfDBYFjCK7/HHYZ5zpoYw8TyxgeofWwnRcgZ0ioiT2TjQ486Sikj
        etgrFh7J6YCdFIZKtwNWqA+wWt67e3d+fg4/O2UPnImC1077EFBHeBvQkM5EyMbVEmazNwLIb+B9
        j5o2pCkeZrN11uE7uY1HdhcROO88wF6QTuBE4ELn6DdyfYrA+Dmh01nlgdYTmS8YGnQYSD8iePZg
        DIfMs4cmIfOQOaIzp9zMYd+SbsGiMiNJQ9X9VHXm51LKvnMGg3ROVvZO5HmZCS8+EQp+y04Y1ZNu
        GBIqx/tsDlmLirfAiN1w6s7bHU4SSfZTlahkPJKSVRM6Al5IO41OEEaHvSNPEz9jI6RShPfSxcTF
        H49aJxf+VAp/W8S8tSoMUr8nNzTlt309TolhFqD8cEMXivbibiDHMaTCLuwxjA0U/YQ9E3UAq/QW
        y7zR5XRwiKKi9ANFiUlHWfJswgfUKRZwPBgUAn5OZXYo3h4d/754Bg3ctso1yCOEUrBs++Clk8Ng
        IsbHPTI/pUIA5kx1roU2JOcOP8MJKFX79F3fqpxvCa+EFWJQo69FWym59fvTs45TKeA+UIzo1oJ6
        sVjIv9nsD9yRjLZS0Ho2k8/31a8+FIc90minGv7v0MhMl/mb4QTkZLNxx4xLeOk4haLDmHpw5xDY
        KXJ5+mWpVIdSYadiSS4TERQ/MhWW99VJkR/ExWGoc6RF7iB7ODqNJ2PJoOkDamK5fPI8OhXMeIdl
        sOeSvz04yxywPgBa6sipOF5cqqupSWLPYnXFeaoMQ2jyTKmcl/B6nImEBdw0FH+YaBeqf5wfiSo7
        C0nzYvbMMZ+SfDFQR6IkMoZ8NStLjRM4x0sLT0YXMObD/nPtLKvhifD1+LawvumDr+Ud4pK1x+8b
        csTtx4CKvZN3BEffl/CvZwD/5DdM+uZZUhW5Pka/RScJr59dlnzV9GyaVp8+vxpWo4/KTgs3V8/O
        BogVHzhi93FDrsHQB8pvGgF69vXsXwAAAP//AwCMGpyr0gkAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a452274dfa31031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:51 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '12659'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '500'
      x-ratelimit-limit-tokens:
      - '30000'
      x-ratelimit-remaining-requests:
      - '499'
      x-ratelimit-remaining-tokens:
      - '25289'
      x-ratelimit-reset-requests:
      - 120ms
      x-ratelimit-reset-tokens:
      - 9.422s
      x-request-id:
      - req_d2e3ae6f6c66e6111dd3b0667f3481f9
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "\nA prompt is a text paragraph that outlines the
      expected actions and instructs the large language model (LLM) to generate a
      specific output. This prompt is concatenated with the input text, and the model
      then creates the required output.\nThis describes the full template how the
      prompt is concatenated with the input to produce the output:\n\n```\n\n{prompt}\nText:
      {0->1}\nOutput: {1->2}\n```\n\nHere:\n- \"Text: {0->1}\" is input template,\n-
      \"{prompt}\" is the LLM prompt,\n- \"Output: {1->2}\" is the output template.\n\nModel
      can produce erroneous output if a prompt is not well defined. In our collaboration,
      we\u2019ll work together to refine a prompt. The process consists of two main
      steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction
      examples. Each example contains the input text, the final prediction produced
      by the model, and the user feedback. User feedback indicates whether the model
      prediction is correct or not. Your task is to analyze the examples and user
      feedback, determining whether the existing instruction is describing the task
      reflected by these examples precisely, and suggests changes to the prompt to
      address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review
      your reasoning in step 1, integrate the insights to refine the prompt, and provide
      me with the new prompt that improves the model\u2019s performance."}, {"role":
      "assistant", "content": "Sure, I\u2019d be happy to help you with this prompt
      engineering problem. Please provide me with the current prompt and the examples
      with user feedback."}, {"role": "user", "content": "\n## Current prompt\n...\n\n##
      Examples\n### Example #0\n\nText: 1 5 1\n\nOutput: The output for the text \"1
      5 1\" could be any number of things depending on the context or what you would
      like to do with these numbers. \n\nIf you are looking for the sum of the numbers,
      the output would be 7.\n\nIf you are looking for the numbers arranged in ascending
      order, the output would be 1 1 5.\n\nIf you are looking for something else,
      please provide more context so I can give a more accurate output.\n\nUser feedback:
      Prediction is incorrect. Correct answer: \"2 5 2\"\n\n\n### Example #1\n\nText:
      1 1 1\n\nOutput: The output would depend on what operation needs to be performed
      with the text \"1 1 1.\" Please provide further instructions on what you would
      like the output to be.\n\nUser feedback: Prediction is incorrect. Correct answer:
      \"2 2 2\"\n\n\n\nSummarize your analysis about incorrect predictions and suggest
      changes to the prompt."}, {"role": "assistant", "content": "Based on the examples
      provided and the feedback received, it''s clear that the current prompt is not
      adequately guiding the language model to produce the desired type of output.
      The model responses suggest that it is offering possible outcomes based on different
      operations that could be performed with the given numbers, like adding them
      up or arranging them in a certain order. However, the user feedback for both
      examples indicates that the expected operation was to increment each number
      by 1.\n\n### Analysis of Incorrect Predictions\n\n1. **Lack of Specificity**:
      The current prompt doesn''t specify what operation to perform on the input sequence
      of numbers. The model''s generic responses suggest it''s trying to guess the
      intended operation, which leads to incorrect predictions.\n\n2. **Understanding
      User Expectations**: Given the user''s correct answers, the task seems to involve
      incrementing each number in the sequence by 1. There''s no indication in the
      model''s responses that it understood this was the expected operation.\n\n3.
      **Prompt Clarity**: Since the prompt is not provided, it''s evident that whatever
      instructions were given to the model, they lacked the clarity needed for it
      to understand and execute the specific required task.\n\n### Suggested Changes
      to the Prompt\n\nTo address these incorrect predictions, the prompt needs to
      be modified to be more specific about the expected action. Here is a suggestion
      for how the prompt could be rewritten:\n\n---\n\n**Revised Prompt:**\n\n\"For
      each sequence of numbers provided, increment each number by 1 and output the
      resulting sequence. Ensure the numbers in the output maintain the same format
      and order as the input sequence.\"\n\n---\n\nThis revised prompt clearly instructs
      the model on precisely what needs to be done with each input sequence, thereby
      eliminating the ambiguity that was leading to incorrect outputs. It specifies
      the operation (increment by 1), maintains the format, and ensures the numbers''
      order is preserved, aligning with the expectations set by the user feedback."},
      {"role": "user", "content": "\nNow please carefully review your reasoning in
      Step 1 and help with Step 2: refining the prompt.\n\n## Current prompt\n...\n\n##
      Follow this guidance to refine the prompt:\n\n1. The new prompt should should
      describe the task precisely, and address the points raised in the user feedback.\n\n2.
      The new prompt should be similar to the current prompt, and only differ in the
      parts that address the issues you identified in Step 1.\n    Example:\n    -
      Current prompt: \"Generate a summary of the input text.\"\n    - New prompt:
      \"Generate a summary of the input text. Pay attention to the original style.\"\n\n3.
      Reply only with the new prompt. Do not include input and output templates in
      the prompt.\n"}], "model": "gpt-4-turbo-preview", "max_tokens": 4096}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '5559'
      content-type:
      - application/json
      cookie:
      - __cf_bm=WixDbOUXaN7esFuxd4ctnTmPJrStGonegliuJvEGpNg-1721165797-1.0.1.1-SDQuBu5xF9dXNmylynnHo52RU3X6v9tmg9acWBD0f8X3zxVqaZQa5n.0.28Qof8S47Pn9Fc2wVGaxoSpQ91OTQ;
        _cfuvid=2vMndz0fnu4RB1YFa7WnY_ZqpSAeP7xxSIwpVoyyP6k-1721165797470-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - x64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.10.5
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SRQW/bMAyF7/4VhM5OEaft0ua4AcOAokB32Q7DECgybbOVSFWis2VF/vsgx022
        iw78+KjHx7cKwFBrNmDcYNWF6Bf3/uWew/7m45/rh6/523fUx5Tunl4/fRmeO1MXheye0em76spJ
        iB6VhE/YJbSKZWqzXjXNh9u7dTOBIC36IuujLm4Wy2Z1u4gJ94S/ZuUg5DCbDfyoAADeprd45BZ/
        mw0s6/dKwJxtj2ZzbgIwSXypGJszZbWspr5AJ6zIk+3PkgCtGyDj64jsEKQDHsMOU4ae9sg1ELuE
        AVlPnScKxKADAnEcFXYHaGoIllgtMXFfGCWQ1GICyy3IqKWxSIK01BG2lz+tc5Ja4t4frszs83he
        0Esfk+xKGDx6f653xJSHbUKbhcsyWSWe5McK4OcU5PhfNiYmCVG3Ki/IZWDTLOckzeV4F7xaz1BF
        rf9XtlpXs0eTD1kxbDviHlNMNCVbnFbH6i8AAAD//wMAZw60AVgCAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a4522c77a961031-LAX
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Tue, 16 Jul 2024 21:37:53 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - user-jt0khdjhkecks0yodhgozy8e
      openai-processing-ms:
      - '1249'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '500'
      x-ratelimit-limit-tokens:
      - '30000'
      x-ratelimit-remaining-requests:
      - '499'
      x-ratelimit-remaining-tokens:
      - '24607'
      x-ratelimit-reset-requests:
      - 120ms
      x-ratelimit-reset-tokens:
      - 10.786s
      x-request-id:
      - req_589b8ae89be3e39bf40dc9051191f411
    status:
      code: 200
      message: OK
version: 1
