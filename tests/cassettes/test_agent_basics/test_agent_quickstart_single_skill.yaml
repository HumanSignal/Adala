interactions:
- request:
    body: '{"messages": [{"role": "user", "content": "Hey, how''s it going?"}], "model":
      "gpt-3.5-turbo", "max_tokens": 10}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '111'
      content-type:
      - application/json
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.9
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SQQU8CMRSE7/srnr14YQlF0LA3E2PcuwfQGFKWR7fY9jXt26gh/HfTZQG99DDT
        mX7TQwEgzFZUIJpWceOCLRcU7W6pqdX1a9euaPmGj08Pz/dfq9pYMcoJ2uyx4XNq3JALFtmQP9lN
        RMWYW+XDVC7kRErZG462aHNMBy7vxvOSu7ihciKn8yHZkmkwiQreCwCAQ39mRr/Fb1HBZHRWHKak
        NIrqcglARLJZESolk1h5FqOr2ZBn9D32C1pLN1DfOth3iUFB3tAxRgiRdFQOEkEthvDx8qolHSJt
        MqHvrL3oO+NNatcRVSKfX7DoNbengmMB8NHv6/4hixDJBV4zfaLPlXJ2KhTXH/1jDtsFEyt71aez
        YiAU6ScxuvXOeI0xRNOPzZzFsfgFAAD//wMAjFxEB+sBAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a8c1cb46daf5bd6-LIS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Thu, 25 Jul 2024 12:21:52 GMT
      Server:
      - cloudflare
      Set-Cookie:
      - __cf_bm=vLI38TDLkgpcmdejfa5eMzWlBPDfbNF_mzbrxJyCA7A-1721910112-1.0.1.1-d6RTE.rn8VVpKqg2raFWXGllC7_R3AGWn63.grRo6Vr0pkZowuny.zRGn7rUWzZJyLUb3wbOfyop9Ma7OytSVg;
        path=/; expires=Thu, 25-Jul-24 12:51:52 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=LKqIIYlSB2lAvo8LSiqmx0qQ1UHgHopqmpsTjXR0EM0-1721910112020-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - heartex
      openai-processing-ms:
      - '160'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '50000000'
      x-ratelimit-remaining-requests:
      - '9999'
      x-ratelimit-remaining-tokens:
      - '49999983'
      x-ratelimit-reset-requests:
      - 6ms
      x-ratelimit-reset-tokens:
      - 0s
      x-request-id:
      - req_ab6a910a49ca0682175fb37ad3141403
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "user", "content": "Hey, how''s it going?"}], "model":
      "gpt-3.5-turbo", "max_tokens": 10}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '111'
      content-type:
      - application/json
      cookie:
      - __cf_bm=vLI38TDLkgpcmdejfa5eMzWlBPDfbNF_mzbrxJyCA7A-1721910112-1.0.1.1-d6RTE.rn8VVpKqg2raFWXGllC7_R3AGWn63.grRo6Vr0pkZowuny.zRGn7rUWzZJyLUb3wbOfyop9Ma7OytSVg;
        _cfuvid=LKqIIYlSB2lAvo8LSiqmx0qQ1UHgHopqmpsTjXR0EM0-1721910112020-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.9
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SQS2/CMBCE7/kVW196SRAJIESOPVTtpRyqvlUhY5bE1PZG9qYPIf575RBCe/Fh
        Zmf2W+8TAKE3ogShasnKNiZbkDfV3WM7f/16knI5uX+pt2QfzDNeLa9FGhO03qHiU2qkyDYGWZM7
        2sqjZIyt+bzIF/k4z4vOsLRBE2NVw9lkNMu49WvKxnkx65M1aYVBlPCWAADsuzcyug1+ixLG6Umx
        GIKsUJTDEIDwZKIiZAg6sHQs0rOpyDG6DvsGjaELuL20sGsDg4RP7bmVBoZkCoFEHz4MWw1Vjad1
        JHStMYO+1U6HeuVRBnJxg0FXcX0sOCQA79197T9k0XiyDa+YPtDFynx6LBTnH/1j9rcLJpbmrBfT
        pCcU4Scw2tVWuwp943V3bORMDskvAAAA//8DAIkDYSbrAQAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a8c1cb92d965bd6-LIS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Thu, 25 Jul 2024 12:21:52 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - heartex
      openai-processing-ms:
      - '225'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '50000000'
      x-ratelimit-remaining-requests:
      - '9999'
      x-ratelimit-remaining-tokens:
      - '49999983'
      x-ratelimit-reset-requests:
      - 6ms
      x-ratelimit-reset-tokens:
      - 0s
      x-request-id:
      - req_fac3895ac4b5a730a5049179afee79d6
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content":
      "Input: 0 5 0"}], "model": "gpt-3.5-turbo", "max_tokens": 1000, "temperature":
      0.0, "tool_choice": {"type": "function", "function": {"name": "Output"}}, "tools":
      [{"type": "function", "function": {"name": "Output", "description": "Correctly
      extracted `Output` with all the required parameters with correct types", "parameters":
      {"properties": {"0_to_1": {"description": "Output:", "title": "0 To 1", "type":
      "string"}}, "required": ["0_to_1"], "type": "object"}}}]}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '545'
      content-type:
      - application/json
      cookie:
      - __cf_bm=vLI38TDLkgpcmdejfa5eMzWlBPDfbNF_mzbrxJyCA7A-1721910112-1.0.1.1-d6RTE.rn8VVpKqg2raFWXGllC7_R3AGWn63.grRo6Vr0pkZowuny.zRGn7rUWzZJyLUb3wbOfyop9Ma7OytSVg;
        _cfuvid=LKqIIYlSB2lAvo8LSiqmx0qQ1UHgHopqmpsTjXR0EM0-1721910112020-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.9
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA2xS30vDMBB+718R7nkbzVbR9VEQhYGKOFSclCzL2s40F5MrTsb+d0m7tXOYh3Dc
        l+8Hd9lFjEG5gpSBLATJyurhFJ0uXu+2/l7NHpPkhSfJ0/z263l7I17vYBAYuNwoSUfWSGJltaIS
        TQtLpwSpoMovx3zKY84nDVDhSulAyy0NJ6OLIdVuicOYjy8OzAJLqTyk7D1ijLFdc4eMZqW2kLJ4
        cOxUynuRK0i7R4yBQx06ILwvPQlDMOhBiYaUCbFNrfUJQIg6k0Lr3rg9u5O6H5TQOhtf38yXKzFL
        Etp8zovv57f8YfJk8xO/VvrHNoHWtZHdgE7wrp+emTEGRlQN96EmW9MZkzEQLq8rZSikht0C4oww
        4wtIFxCP4gXs4Q9jH/1XfxyqfTdYjbl1uPRnc4J1aUpfZE4J3+QFT2hbiyD30Syw/rMTsA4rSxnh
        pzJB8Cpp5aD/Mj3ID8sFQhK670+T6JAP/I8nVWXr0uTKWVd224z20S8AAAD//wMAdJPfG8wCAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a8c1cbd8d885bd6-LIS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Thu, 25 Jul 2024 12:21:53 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - heartex
      openai-processing-ms:
      - '207'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '50000000'
      x-ratelimit-remaining-requests:
      - '9999'
      x-ratelimit-remaining-tokens:
      - '49998993'
      x-ratelimit-reset-requests:
      - 6ms
      x-ratelimit-reset-tokens:
      - 1ms
      x-request-id:
      - req_63decb912348d5d264167981c648e940
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content":
      "Input: 0 0 0"}], "model": "gpt-3.5-turbo", "max_tokens": 1000, "temperature":
      0.0, "tool_choice": {"type": "function", "function": {"name": "Output"}}, "tools":
      [{"type": "function", "function": {"name": "Output", "description": "Correctly
      extracted `Output` with all the required parameters with correct types", "parameters":
      {"properties": {"0_to_1": {"description": "Output:", "title": "0 To 1", "type":
      "string"}}, "required": ["0_to_1"], "type": "object"}}}]}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '545'
      content-type:
      - application/json
      cookie:
      - __cf_bm=vLI38TDLkgpcmdejfa5eMzWlBPDfbNF_mzbrxJyCA7A-1721910112-1.0.1.1-d6RTE.rn8VVpKqg2raFWXGllC7_R3AGWn63.grRo6Vr0pkZowuny.zRGn7rUWzZJyLUb3wbOfyop9Ma7OytSVg;
        _cfuvid=LKqIIYlSB2lAvo8LSiqmx0qQ1UHgHopqmpsTjXR0EM0-1721910112020-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.9
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA2xSTU/jMBC951dYc6YoKVQluaEeChKI1Qp62C2KXNd1ArbH2BMJVPW/IydpGip8
        sEbz/D40433CGNRbKBiIipMwTk9y9LrKb14Wz4/L2ylfrdTiJROVWKl8RnARGbh5k4KOrEuBxmlJ
        NdoOFl5yklE1m0+zPEuz7KoFDG6ljjTlaHJ1OZtQ4zc4SbPprGdWWAsZoGD/E8YY27d3zGi38hMK
        ll4cO0aGwJWEYnjEGHjUsQM8hDoQt13eHhRoSdoY2zZajwBC1KXgWp+Mu7Mf1adBca1Ls1x8VP/U
        w93y8Vndmfnf97c/9/Pr1civk/5ybaBdY8UwoBE+9IszM8bActNynxpyDZ0xGQPuVWOkpZga9mtI
        S8IyW0OxhnQNB/jx/pD8Vr/21WEYq0blPG7C2ZRgV9s6VKWXPLRpIRC6ziLKvbbra35sBJxH46gk
        fJc2Ct5cd3Jw+jAjsMcIietTO58mfTwIX4GkKXe1VdI7Xw+rTA7JNwAAAP//AwCxJkv+yQIAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a8c1cc16c5d5bd6-LIS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Thu, 25 Jul 2024 12:21:53 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - heartex
      openai-processing-ms:
      - '198'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '50000000'
      x-ratelimit-remaining-requests:
      - '9999'
      x-ratelimit-remaining-tokens:
      - '49998993'
      x-ratelimit-reset-requests:
      - 6ms
      x-ratelimit-reset-tokens:
      - 1ms
      x-request-id:
      - req_2532e7b2b4e330a3fbeb17e304716bc4
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "\nA prompt is a text paragraph that outlines the
      expected actions and instructs the large language model (LLM) to generate a
      specific output. This prompt is concatenated with the input text, and the model
      then creates the required output.\nThis describes the full template how the
      prompt is concatenated with the input to produce the output:\n\n```\n\n{prompt}\nInput:
      {input}\nOutput: {0_to_1}\n```\n\nHere:\n- \"Input: {input}\" is input template,\n-
      \"{prompt}\" is the LLM prompt,\n- \"Output: {0_to_1}\" is the output template.\n\nModel
      can produce erroneous output if a prompt is not well defined. In our collaboration,
      we\u2019ll work together to refine a prompt. The process consists of two main
      steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction
      examples. Each example contains the input text, the final prediction produced
      by the model, and the user feedback. User feedback indicates whether the model
      prediction is correct or not. Your task is to analyze the examples and user
      feedback, determining whether the existing instruction is describing the task
      reflected by these examples precisely, and suggests changes to the prompt to
      address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review
      your reasoning in step 1, integrate the insights to refine the prompt, and provide
      me with the new prompt that improves the model\u2019s performance."}, {"role":
      "assistant", "content": "Sure, I\u2019d be happy to help you with this prompt
      engineering problem. Please provide me with the current prompt and the examples
      with user feedback."}, {"role": "user", "content": "\n## Current prompt\n...\n\n##
      Examples\n### Example #0\n\nInput: 0 5 0\n\nOutput: 0.0\n\nUser feedback: Prediction
      is incorrect. Correct answer: \"1 5 1\"\n\n\n### Example #1\n\nInput: 0 0 0\n\nOutput:
      0\n\nUser feedback: Prediction is incorrect. Correct answer: \"1 1 1\"\n\n\n\nSummarize
      your analysis about incorrect predictions and suggest changes to the prompt."}],
      "model": "gpt-3.5-turbo", "max_tokens": 1000, "temperature": 0.0}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '2174'
      content-type:
      - application/json
      cookie:
      - __cf_bm=vLI38TDLkgpcmdejfa5eMzWlBPDfbNF_mzbrxJyCA7A-1721910112-1.0.1.1-d6RTE.rn8VVpKqg2raFWXGllC7_R3AGWn63.grRo6Vr0pkZowuny.zRGn7rUWzZJyLUb3wbOfyop9Ma7OytSVg;
        _cfuvid=LKqIIYlSB2lAvo8LSiqmx0qQ1UHgHopqmpsTjXR0EM0-1721910112020-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.9
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAAwAAAP//fJNPbxoxEMXvfIrR5gqITSAh3NpLW1VVooieSkWMPey68Y4tzzgBIb57
        5WU3NFHbC4f5896Pt57DAKCwplhAoWslugludOujs9P5fbp/3pT0ga4flvtvd2mHD7uv18Uwb/jN
        L9TSb421b4JDsZ5ObR1RCWbV8uayvC0nZTltG4036PJaFWR0NZ6NJMWNH03Ky1m3WXurkYsF/BgA
        ABza38xIBnfFAibDvtIgs6qwWLwOARTRu1wpFLNlUSTF8NzUngSpxf6oGA14AqkRcKcyP0OI/tka
        NEOwAozYMEitpB1q0cEy4C6gFjQgHgLGrY8NsG8Q2EcBvwWJijiXVU6kN7EUkgClZoORx7CsEbSP
        EbWAIn7ByGDJWK0E35ty7ZMzsEFQxliqoMzeqHT9RjUXKySMJ4mzvk8SkoxhRSta+iwSkbmj6odC
        RGN1JuYhvGBv2nhjt/t2NkTfBMku2qGKbg+WWGLSf7JKq/8PwjF8xog5RAWcqgo556hrRRXmhbPL
        IrNeXMD3YPJLgvu2uqLHx8cVfbLPSFkCT3nXEbFPdvjOvotGkeliaE0icnIyziZfMt8CDi3ncUV3
        7dQCDpO1+HV57DxXtKwtQ+p4uixwF5zVVtweWJTgKVRR/JTB/vu1hvBSW133OdfoAlTJGnwbZoje
        JP3X71l0L/v4ehLOVyH6TT4fSs691reWLNfriIo95efP4sNp/TgA+NmeXnpzTcXpH67FPyFlwenV
        zUmvOF/7uVtO511XvCh3bszms0GHWPCeBZv11lKFMUTbnmIGHRwHvwEAAP//AwDCg4RmiQQAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a8c1cc53a7f5bd6-LIS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Thu, 25 Jul 2024 12:21:55 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - heartex
      openai-processing-ms:
      - '1473'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '50000000'
      x-ratelimit-remaining-requests:
      - '9999'
      x-ratelimit-remaining-tokens:
      - '49998522'
      x-ratelimit-reset-requests:
      - 6ms
      x-ratelimit-reset-tokens:
      - 1ms
      x-request-id:
      - req_6258938613eb19da63ebde0130d6e7b7
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "\nA prompt is a text paragraph that outlines the
      expected actions and instructs the large language model (LLM) to generate a
      specific output. This prompt is concatenated with the input text, and the model
      then creates the required output.\nThis describes the full template how the
      prompt is concatenated with the input to produce the output:\n\n```\n\n{prompt}\nInput:
      {input}\nOutput: {0_to_1}\n```\n\nHere:\n- \"Input: {input}\" is input template,\n-
      \"{prompt}\" is the LLM prompt,\n- \"Output: {0_to_1}\" is the output template.\n\nModel
      can produce erroneous output if a prompt is not well defined. In our collaboration,
      we\u2019ll work together to refine a prompt. The process consists of two main
      steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction
      examples. Each example contains the input text, the final prediction produced
      by the model, and the user feedback. User feedback indicates whether the model
      prediction is correct or not. Your task is to analyze the examples and user
      feedback, determining whether the existing instruction is describing the task
      reflected by these examples precisely, and suggests changes to the prompt to
      address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review
      your reasoning in step 1, integrate the insights to refine the prompt, and provide
      me with the new prompt that improves the model\u2019s performance."}, {"role":
      "assistant", "content": "Sure, I\u2019d be happy to help you with this prompt
      engineering problem. Please provide me with the current prompt and the examples
      with user feedback."}, {"role": "user", "content": "\n## Current prompt\n...\n\n##
      Examples\n### Example #0\n\nInput: 0 5 0\n\nOutput: 0.0\n\nUser feedback: Prediction
      is incorrect. Correct answer: \"1 5 1\"\n\n\n### Example #1\n\nInput: 0 0 0\n\nOutput:
      0\n\nUser feedback: Prediction is incorrect. Correct answer: \"1 1 1\"\n\n\n\nSummarize
      your analysis about incorrect predictions and suggest changes to the prompt."},
      {"role": "assistant", "content": "Based on the examples provided, it seems that
      the model is expected to perform some sort of transformation on the input numbers.
      The correct answers indicate that the model should be adding 1 to each input
      number to generate the correct output. \n\nTo address the incorrect predictions,
      we should modify the prompt to clearly instruct the model to add 1 to each input
      number. Here is a suggested change to the prompt:\n\n## Updated Prompt\n```\nGiven
      a set of three numbers, add 1 to each number and output the result.\n\nInput:
      {input}\nOutput: {0_to_1}\n```\n\nThis updated prompt explicitly states the
      task of adding 1 to each input number, which should help guide the model to
      produce the correct output."}, {"role": "user", "content": "\nNow please carefully
      review your reasoning in Step 1 and help with Step 2: refining the prompt.\n\n##
      Current prompt\n...\n\n## Follow this guidance to refine the prompt:\n\n1. The
      new prompt should should describe the task precisely, and address the points
      raised in the user feedback.\n\n2. The new prompt should be similar to the current
      prompt, and only differ in the parts that address the issues you identified
      in Step 1.\n    Example:\n    - Current prompt: \"Generate a summary of the
      input text.\"\n    - New prompt: \"Generate a summary of the input text. Pay
      attention to the original style.\"\n\n3. Reply only with the new prompt. Do
      not include input and output templates in the prompt.\n"}], "model": "gpt-3.5-turbo",
      "max_tokens": 1000, "temperature": 0.0}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '3656'
      content-type:
      - application/json
      cookie:
      - __cf_bm=vLI38TDLkgpcmdejfa5eMzWlBPDfbNF_mzbrxJyCA7A-1721910112-1.0.1.1-d6RTE.rn8VVpKqg2raFWXGllC7_R3AGWn63.grRo6Vr0pkZowuny.zRGn7rUWzZJyLUb3wbOfyop9Ma7OytSVg;
        _cfuvid=LKqIIYlSB2lAvo8LSiqmx0qQ1UHgHopqmpsTjXR0EM0-1721910112020-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.9
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1RR0W7bMAx891cQ6msSxO0yJ/6BYQMKNB2GFViGWLZoW61MChLdbSjy74PsNNle
        BOhOR96d3jIAZY0qQTW9lmbwbrnj4F726/p++2HztP9sd8Wv1+/bvXkqvmhWi6Tg+hkbeVetGh68
        Q7FMM90E1IJpal7c5rt8necfJ2Jggy7JOi/Lu9VmKWOoebnObzdnZc+2wahK+JEBALxNZ/JIBn+r
        EtaLd2TAGHWHqrw8AlCBXUKUjtFG0SRqcSUbJkGabD9iawlBegQfePACwqCNCRjjhNoYR4xgDZLY
        1qIBS/BV0ENeHuhANzfwzZuUEh6mAQeqqupAn+wrEmiIKMAtSB8QgcahxhAXaQPkaRXqpj/D6doh
        YdAyG+JR/CirtKWqKnX2f7oEd9z5wHUqiUbnLnhrycb+GFBHphQyCvtZfsoAfk4Fj/91pubwR+EX
        pDSwuNvN89T1T6/shRQW7f5RFdvs7FDFP1FwOLaWOgw+2Knv5DM7ZX8BAAD//wMA0GN7gW4CAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a8c1cd0ae415bd6-LIS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Thu, 25 Jul 2024 12:21:56 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - heartex
      openai-processing-ms:
      - '672'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '50000000'
      x-ratelimit-remaining-requests:
      - '9999'
      x-ratelimit-remaining-tokens:
      - '49998176'
      x-ratelimit-reset-requests:
      - 6ms
      x-ratelimit-reset-tokens:
      - 2ms
      x-request-id:
      - req_794739e560cc24904203bf7a892f0536
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "Refine the prompt to address
      the issues identified in Step 1:\n\n## Updated Prompt\n```\nGiven a set of three
      numbers, add 1 to each number to generate the output.\n\n```"}, {"role": "user",
      "content": "Input: 0 5 0"}], "model": "gpt-3.5-turbo", "max_tokens": 1000, "temperature":
      0.0, "tool_choice": {"type": "function", "function": {"name": "Output"}}, "tools":
      [{"type": "function", "function": {"name": "Output", "description": "Correctly
      extracted `Output` with all the required parameters with correct types", "parameters":
      {"properties": {"0_to_1": {"description": "Output:", "title": "0 To 1", "type":
      "string"}}, "required": ["0_to_1"], "type": "object"}}}]}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '712'
      content-type:
      - application/json
      cookie:
      - __cf_bm=vLI38TDLkgpcmdejfa5eMzWlBPDfbNF_mzbrxJyCA7A-1721910112-1.0.1.1-d6RTE.rn8VVpKqg2raFWXGllC7_R3AGWn63.grRo6Vr0pkZowuny.zRGn7rUWzZJyLUb3wbOfyop9Ma7OytSVg;
        _cfuvid=LKqIIYlSB2lAvo8LSiqmx0qQ1UHgHopqmpsTjXR0EM0-1721910112020-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.9
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAAwAAAP//bFJbSysxEH7fXxHmuZVNLxb37Yh68EHEGyhWljRNt/FkMzGZgFr63yW7
        7XYtJw9hmC/fhZlsMsZAL6FgINeCZO3M8Ay9McsPffUc7fnFpZhM1OyKx7+P5s6+wCAxcPGuJO1Z
        JxJrZxRptC0svRKkkiqfjfgZzzmfNUCNS2USrXI0HJ9MhxT9Aoc5H013zDVqqQIU7DVjjLFNc6eM
        dqk+oWD5YN+pVQiiUlB0jxgDjyZ1QISgAwlLMDiAEi0pm2LbaEwPIERTSmHMwbg9m159GJQwpswv
        b2ff6nryLj6moz835/enzxcP10/fPb9W+ss1gVbRym5APbzrF0dmjIEVdcO9jeQiHTEZA+GrWCtL
        KTVs5pCXhCWfQzEHzk4Zn8MWfnG22f/qt1217UZrsHIeF+FoUrDSVod16ZUITWIIhK61SHJvzQrj
        r62A81g7Kgn/KZsE+Wjc6sHh1/TRHUhIwvT642m2SwjhK5Cqy5W2lfLO626j2Tb7AQAA//8DACS7
        443QAgAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a8c1cd81ba95bd6-LIS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Thu, 25 Jul 2024 12:21:57 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - heartex
      openai-processing-ms:
      - '532'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '50000000'
      x-ratelimit-remaining-requests:
      - '9999'
      x-ratelimit-remaining-tokens:
      - '49998954'
      x-ratelimit-reset-requests:
      - 6ms
      x-ratelimit-reset-tokens:
      - 1ms
      x-request-id:
      - req_81e685c16afda4afca03ff1b04b0f470
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "Refine the prompt to address
      the issues identified in Step 1:\n\n## Updated Prompt\n```\nGiven a set of three
      numbers, add 1 to each number to generate the output.\n\n```"}, {"role": "user",
      "content": "Input: 0 0 0"}], "model": "gpt-3.5-turbo", "max_tokens": 1000, "temperature":
      0.0, "tool_choice": {"type": "function", "function": {"name": "Output"}}, "tools":
      [{"type": "function", "function": {"name": "Output", "description": "Correctly
      extracted `Output` with all the required parameters with correct types", "parameters":
      {"properties": {"0_to_1": {"description": "Output:", "title": "0 To 1", "type":
      "string"}}, "required": ["0_to_1"], "type": "object"}}}]}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '712'
      content-type:
      - application/json
      cookie:
      - __cf_bm=vLI38TDLkgpcmdejfa5eMzWlBPDfbNF_mzbrxJyCA7A-1721910112-1.0.1.1-d6RTE.rn8VVpKqg2raFWXGllC7_R3AGWn63.grRo6Vr0pkZowuny.zRGn7rUWzZJyLUb3wbOfyop9Ma7OytSVg;
        _cfuvid=LKqIIYlSB2lAvo8LSiqmx0qQ1UHgHopqmpsTjXR0EM0-1721910112020-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.9
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA2xSS2/bMAy++1cIPCdFZNfo6tuKXYahr8OwZkthyLLiqJVEQaKxdUH++yA7ddxg
        OggEP30PkNpnjIFuoWIgd4Kk9WZ5jcHYH+Wayq/8qhDF4+d1Tm3x8P1pd/sKi8TA5kVJemddSLTe
        KNLoRlgGJUglVX6V82u+4vzTAFhslUm0ztOyuCiX1IcGlyuel0fmDrVUESr2K2OMsf1wp4yuVX+g
        YqvFe8eqGEWnoJoeMQYBTeqAiFFHEo5gcQIlOlIuxXa9MTOAEE0thTEn4/HsZ/VpUMKYWpufT98e
        b8Rd65sm/3uz/nJ/99v0lzO/UfrND4G2vZPTgGb41K/OzBgDJ+zAve/J93TGZAxE6HqrHKXUsN/A
        qias+QaqDXDGGd/AAT5wDtn/6udjdZhGa7DzAZt4NinYaqfjrg5KxCExREI/WiS552GF/YetgA9o
        PdWEr8olQZ4Xox6cfs0cPYKEJMysX5TZMSHEt0jK1lvtOhV80NNGs0P2DwAA//8DALCZgRbQAgAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a8c1cdd8cf05bd6-LIS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Thu, 25 Jul 2024 12:21:58 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - heartex
      openai-processing-ms:
      - '479'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '50000000'
      x-ratelimit-remaining-requests:
      - '9999'
      x-ratelimit-remaining-tokens:
      - '49998954'
      x-ratelimit-reset-requests:
      - 6ms
      x-ratelimit-reset-tokens:
      - 1ms
      x-request-id:
      - req_c39ad38b2088aafd9f4274dbd8fcfcef
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "\nA prompt is a text paragraph that outlines the
      expected actions and instructs the large language model (LLM) to generate a
      specific output. This prompt is concatenated with the input text, and the model
      then creates the required output.\nThis describes the full template how the
      prompt is concatenated with the input to produce the output:\n\n```\n\n{prompt}\nInput:
      {input}\nOutput: {0_to_1}\n```\n\nHere:\n- \"Input: {input}\" is input template,\n-
      \"{prompt}\" is the LLM prompt,\n- \"Output: {0_to_1}\" is the output template.\n\nModel
      can produce erroneous output if a prompt is not well defined. In our collaboration,
      we\u2019ll work together to refine a prompt. The process consists of two main
      steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction
      examples. Each example contains the input text, the final prediction produced
      by the model, and the user feedback. User feedback indicates whether the model
      prediction is correct or not. Your task is to analyze the examples and user
      feedback, determining whether the existing instruction is describing the task
      reflected by these examples precisely, and suggests changes to the prompt to
      address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review
      your reasoning in step 1, integrate the insights to refine the prompt, and provide
      me with the new prompt that improves the model\u2019s performance."}, {"role":
      "assistant", "content": "Sure, I\u2019d be happy to help you with this prompt
      engineering problem. Please provide me with the current prompt and the examples
      with user feedback."}, {"role": "user", "content": "\n## Current prompt\nRefine
      the prompt to address the issues identified in Step 1:\n\n## Updated Prompt\n```\nGiven
      a set of three numbers, add 1 to each number to generate the output.\n\n```\n\n##
      Examples\n### Example #0\n\nInput: 0 5 0\n\nOutput: 1 6 1\n\nUser feedback:
      Prediction is incorrect. Correct answer: \"1 5 1\"\n\n\n### Example #1\n\nInput:
      0 0 0\n\nOutput: 1 1 1\n\nUser feedback: Prediction is correct.\n\n\n\nSummarize
      your analysis about incorrect predictions and suggest changes to the prompt."}],
      "model": "gpt-3.5-turbo", "max_tokens": 1000, "temperature": 0.0}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '2319'
      content-type:
      - application/json
      cookie:
      - __cf_bm=vLI38TDLkgpcmdejfa5eMzWlBPDfbNF_mzbrxJyCA7A-1721910112-1.0.1.1-d6RTE.rn8VVpKqg2raFWXGllC7_R3AGWn63.grRo6Vr0pkZowuny.zRGn7rUWzZJyLUb3wbOfyop9Ma7OytSVg;
        _cfuvid=LKqIIYlSB2lAvo8LSiqmx0qQ1UHgHopqmpsTjXR0EM0-1721910112020-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.9
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA4xU72vbQAz9nr9CuF+dEGftSvJtZT/oYLBBOzaW0Z7vZPuas86cdF1D6f8+znbj
        dmVjX4KjJz09PaS7nwFk1mQbyHSjRLedm699cPTx7Vqpr+9v1Pmn9TfnLk/adfv9zZfTLE8VvrxB
        LY9VC+3bzqFYTwOsAyrBxFqcrop1sSyKdQ+03qBLZXUn81eLk7nEUPr5slidjJWNtxo528CPGQDA
        ff+bNJLBu2wDy/wx0iKzqjHbHJIAsuBdimSK2bIokiyfQO1JkHrZZ4rRgCeQBgHvVNLP0AV/aw0a
        UGR6JDIGqBBNqfQuByvAiC2DNEr6BB1DQJJU2XYCloG8gNI6BiXo9hCwcqjFUt3ni+LdVN3bAYRo
        GMRDh6HyoV3AxQGzDHjXoRY0KUMZA0X6QKUboNiWGMAOU1jqYtInOZRRUvTdMBccLfMn7SxpHwJq
        cfvEhyNjSmDUnszEy4LKgK96sLKBZcQWW9rSRa8nICc/LINljpjDOXCsa2RJ/Wy1fxx9tEg8tGqH
        yUvtUAUMf/rBjY/O/N+sYMnYW2uicm6fjLMMv6xz0KDroI7W4BPm5HHwJuohOPoAPkqi057S0iCJ
        2/cDHh3BZWfSJsPnXvyWrq+vt/TB3iKB6vv35gTEUSDnf9M9yUxYjYRpQ3odQ//FyL6lsz1wh/rg
        nZJnXKM/ZbJBB2yRksKnHfKXbpYoggEiGQzpMMy0junPP2xRDAZbTyxBDY1engZUPkzrViyy8ege
        DtfqfN0FX6bLpujcIV5ZstxcBVTsKV0mi++G8ocZwM/+VYjPDj0bNulK/A4pER6frga+bHqIJrQ4
        Hd+MTLwoNwGvj1ezUWLGexZsrypLNYYu2P6VSEJnD7PfAAAA//8DAGjMmb4kBQAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a8c1ce3588f5bd6-LIS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Thu, 25 Jul 2024 12:22:01 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - heartex
      openai-processing-ms:
      - '1926'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '50000000'
      x-ratelimit-remaining-requests:
      - '9999'
      x-ratelimit-remaining-tokens:
      - '49998487'
      x-ratelimit-reset-requests:
      - 6ms
      x-ratelimit-reset-tokens:
      - 1ms
      x-request-id:
      - req_483e3fe558df857d3106e46e15107f24
    status:
      code: 200
      message: OK
- request:
    body: '{"messages": [{"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "\nA prompt is a text paragraph that outlines the
      expected actions and instructs the large language model (LLM) to generate a
      specific output. This prompt is concatenated with the input text, and the model
      then creates the required output.\nThis describes the full template how the
      prompt is concatenated with the input to produce the output:\n\n```\n\n{prompt}\nInput:
      {input}\nOutput: {0_to_1}\n```\n\nHere:\n- \"Input: {input}\" is input template,\n-
      \"{prompt}\" is the LLM prompt,\n- \"Output: {0_to_1}\" is the output template.\n\nModel
      can produce erroneous output if a prompt is not well defined. In our collaboration,
      we\u2019ll work together to refine a prompt. The process consists of two main
      steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction
      examples. Each example contains the input text, the final prediction produced
      by the model, and the user feedback. User feedback indicates whether the model
      prediction is correct or not. Your task is to analyze the examples and user
      feedback, determining whether the existing instruction is describing the task
      reflected by these examples precisely, and suggests changes to the prompt to
      address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review
      your reasoning in step 1, integrate the insights to refine the prompt, and provide
      me with the new prompt that improves the model\u2019s performance."}, {"role":
      "assistant", "content": "Sure, I\u2019d be happy to help you with this prompt
      engineering problem. Please provide me with the current prompt and the examples
      with user feedback."}, {"role": "user", "content": "\n## Current prompt\nRefine
      the prompt to address the issues identified in Step 1:\n\n## Updated Prompt\n```\nGiven
      a set of three numbers, add 1 to each number to generate the output.\n\n```\n\n##
      Examples\n### Example #0\n\nInput: 0 5 0\n\nOutput: 1 6 1\n\nUser feedback:
      Prediction is incorrect. Correct answer: \"1 5 1\"\n\n\n### Example #1\n\nInput:
      0 0 0\n\nOutput: 1 1 1\n\nUser feedback: Prediction is correct.\n\n\n\nSummarize
      your analysis about incorrect predictions and suggest changes to the prompt."},
      {"role": "assistant", "content": "Based on the examples provided and the user
      feedback, it seems that the current prompt is not accurately reflecting the
      task that the model needs to perform. The model is expected to add 1 to each
      number in the input set, but in Example #0, the model incorrectly added 1 to
      the second number instead of the first number.\n\nTo address this issue, I suggest
      modifying the prompt to make it clearer that the model should add 1 to each
      number in the input set individually. This will help guide the model to produce
      the correct output consistently.\n\n## Updated Prompt\n```\nGiven a set of three
      numbers, add 1 to each number individually to generate the output.\n```\n\nBy
      specifying that each number should be incremented individually, the model should
      better understand the task and produce the correct output as demonstrated in
      the user feedback for Example #1."}, {"role": "user", "content": "\nNow please
      carefully review your reasoning in Step 1 and help with Step 2: refining the
      prompt.\n\n## Current prompt\nRefine the prompt to address the issues identified
      in Step 1:\n\n## Updated Prompt\n```\nGiven a set of three numbers, add 1 to
      each number to generate the output.\n\n```\n\n## Follow this guidance to refine
      the prompt:\n\n1. The new prompt should should describe the task precisely,
      and address the points raised in the user feedback.\n\n2. The new prompt should
      be similar to the current prompt, and only differ in the parts that address
      the issues you identified in Step 1.\n    Example:\n    - Current prompt: \"Generate
      a summary of the input text.\"\n    - New prompt: \"Generate a summary of the
      input text. Pay attention to the original style.\"\n\n3. Reply only with the
      new prompt. Do not include input and output templates in the prompt.\n"}], "model":
      "gpt-3.5-turbo", "max_tokens": 1000, "temperature": 0.0}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '4123'
      content-type:
      - application/json
      cookie:
      - __cf_bm=vLI38TDLkgpcmdejfa5eMzWlBPDfbNF_mzbrxJyCA7A-1721910112-1.0.1.1-d6RTE.rn8VVpKqg2raFWXGllC7_R3AGWn63.grRo6Vr0pkZowuny.zRGn7rUWzZJyLUb3wbOfyop9Ma7OytSVg;
        _cfuvid=LKqIIYlSB2lAvo8LSiqmx0qQ1UHgHopqmpsTjXR0EM0-1721910112020-0.0.1.1-604800000
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.34.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.34.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.9
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SRwU7DMBBE7/mKlc9JVadEkFyRQALEDyBUOc42MThey95Urar+O3IaWrj4MOMZ
        v12fMgBhOtGA0INiPXpb1BSsr8tHe/CxNU+vm5epLfeHN2PfpRd5SlD7hZp/UytNo7fIhtzF1gEV
        Y2qV96Ws5VqWcjZG6tCmWO+52KyqgqfQUrGWZbUkBzIao2jgIwMAOM1nYnQdHkQD6/xXGTFG1aNo
        rpcARCCbFKFiNJGVY5HfTE2O0c3Yz2aPDhREZKAd8BAQwU1jiyHmYJwOOKJjQKWHRYf2CBKM68ze
        dJOy9ghM0KPDoBiBBwSa2E+8EsuT5yurpd4HatNcbrL2qu+MM3HYBlSRXOKKTP4SP2cAn/NOpn9j
        Ch9o9Lxl+kaXCh821aVP3L7h5sp6MZlY2T+p6i5bCEU8RsZxuzOux+CDmVeUOLNz9gMAAP//AwDK
        OKsRIQIAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8a8c1cf1fc5e5bd6-LIS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Thu, 25 Jul 2024 12:22:01 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
      openai-organization:
      - heartex
      openai-processing-ms:
      - '377'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=15552000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '50000000'
      x-ratelimit-remaining-requests:
      - '9999'
      x-ratelimit-remaining-tokens:
      - '49998061'
      x-ratelimit-reset-requests:
      - 6ms
      x-ratelimit-reset-tokens:
      - 2ms
      x-request-id:
      - req_ee74d5ae15c9193a53444f9fe25b1dd5
    status:
      code: 200
      message: OK
version: 1
